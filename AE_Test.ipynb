{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Test for Saddle-Free Optimizer\n",
    "\n",
    "> Copyright 2019 Dave Fernandes. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    ">\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    ">  \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This example trains an autoencoder on MNIST data using either the ADAM optimizer or the Saddle-Free (SF) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from SFOptimizer import SFOptimizer\n",
    "from SFOptimizer import SFDamping\n",
    "from mnist.dataset import train\n",
    "from mnist.dataset import test\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Create a layer with sigmoid activation. Weights have a sparse random initialization as per [Martens \\(2010\\)](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = []\n",
    "\n",
    "def logistic_layer(layer_name, input_layer, hidden_units, n_random):\n",
    "    initial_W = np.zeros((input_layer.shape[1], hidden_units))\n",
    "    for i in range(hidden_units):\n",
    "        column = np.zeros((input_layer.shape[1], 1))\n",
    "        column[0:n_random,:] += np.random.randn(n_random, 1)\n",
    "        np.random.shuffle(column)\n",
    "        initial_W[:, i:i+1] = column\n",
    "    \n",
    "    with tf.name_scope('layer_' + layer_name):\n",
    "        W = tf.get_variable('W_' + layer_name, initializer=tf.convert_to_tensor(initial_W, dtype=input_layer.dtype), use_resource=True)\n",
    "        b = tf.get_variable('b_' + layer_name, [hidden_units], initializer=tf.zeros_initializer(), dtype=input_layer.dtype, use_resource=True)\n",
    "        y = tf.sigmoid(tf.matmul(input_layer, W) + b)\n",
    "    \n",
    "    var_list.append(W)\n",
    "    var_list.append(b)\n",
    "    return W, b, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep autoencoder network from [Hinton & Salakhutdinov \\(2006\\)](https://www.cs.toronto.edu/~hinton/science.pdf). This example is used as a standard test in several optimization papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_model(x):\n",
    "    n_inputs = 28*28\n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500\n",
    "    n_hidden3 = 250\n",
    "    n_hidden4 = 30\n",
    "\n",
    "    with tf.name_scope('dnn'):\n",
    "        _, _, y1 = logistic_layer('1', x, n_hidden1, 15)\n",
    "        _, _, y2 = logistic_layer('2', y1, n_hidden2, 15)\n",
    "        _, _, y3 = logistic_layer('3', y2, n_hidden3, 15)\n",
    "        \n",
    "        W4, b4, _ = logistic_layer('4', y3, n_hidden4, 15)\n",
    "        y4 = tf.matmul(y3, W4) + b4\n",
    "        \n",
    "        _, _, y5 = logistic_layer('5', y4, n_hidden3, 15)\n",
    "        _, _, y6 = logistic_layer('6', y5, n_hidden2, 15)\n",
    "        _, _, y7 = logistic_layer('7', y6, n_hidden1, 15)\n",
    "        W8, b8, y_out = logistic_layer('8', y7, n_inputs, 15)\n",
    "        y_logits = tf.matmul(y7, W8) + b8\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=y_logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        error = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, y_out), axis=1))\n",
    "\n",
    "    return loss, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Saves weights to data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_AE_test(use_SF, start_from_previous_run):\n",
    "    # Loop hyper-parameters\n",
    "    if use_SF:\n",
    "        n_epochs = 30\n",
    "        batch_size = 2000\n",
    "        n_little_steps = 5\n",
    "        batch_repeats = 2 * (n_little_steps + 1)\n",
    "        print_interval = 1\n",
    "    else:\n",
    "        n_epochs = 3000\n",
    "        batch_size = 200\n",
    "        batch_repeats = 1\n",
    "        print_interval = 100\n",
    "\n",
    "    # Set up datasets and iterator\n",
    "    mnist_dir = os.path.join(os.getcwd(), 'mnist')\n",
    "    train_dataset = train(mnist_dir).batch(batch_size, drop_remainder=True)\n",
    "    # Replicate each batch batch_repeats times\n",
    "    train_dataset = train_dataset.flat_map(lambda x, y:\n",
    "        tf.data.Dataset.zip((tf.data.Dataset.from_tensors(x).repeat(batch_repeats), tf.data.Dataset.from_tensors(y).repeat(batch_repeats))))\n",
    "    train_dataset = train_dataset.repeat(1)\n",
    "    \n",
    "    test_dataset = test(mnist_dir).batch(100000)\n",
    "    \n",
    "    iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "    train_init_op = iter.make_initializer(train_dataset)\n",
    "    test_init_op = iter.make_initializer(test_dataset)\n",
    "    x, labels = iter.get_next()\n",
    "    \n",
    "    # Set up model\n",
    "    loss, error = AE_model(x)\n",
    "    \n",
    "    model_filepath = os.path.join(os.getcwd(), 'data', 'ae_weights')\n",
    "    saver = tf.train.Saver(var_list)\n",
    "    \n",
    "    # Construct optimizer\n",
    "    if use_SF:\n",
    "        # See SFOptimizer.py for options\n",
    "        optimizer = SFOptimizer(var_list, krylov_dimension=64, damping_type=SFDamping.marquardt, dtype=x.dtype)\n",
    "    else:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "    print('Initializing...')\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if start_from_previous_run:\n",
    "        saver.restore(sess, model_filepath)\n",
    "    \n",
    "    if use_SF:\n",
    "        print('Constructing graph...')\n",
    "        big_train_op = optimizer.minimize(loss)\n",
    "        little_train_op = optimizer.fixed_subspace_step()\n",
    "        update_op = optimizer.update()\n",
    "        reset_op = optimizer.reset_lambda()\n",
    "\n",
    "    history = []\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        iteration = 0\n",
    "        total_error = 0.0\n",
    "        sess.run(train_init_op)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                if use_SF:\n",
    "                    # Reset the damping parameter\n",
    "                    _ = sess.run(reset_op)\n",
    "\n",
    "                    # Compute Krylov subspace and take one training step\n",
    "                    initial_loss, initial_lambda, _ = sess.run([loss, optimizer.lambda_damp, big_train_op])\n",
    "                    final_loss, error_train, rho, _ = sess.run([loss, error, optimizer.rho, update_op])\n",
    "\n",
    "                    if iteration % print_interval == 0:\n",
    "                        print('-- Epoch:', epoch + 1, ' Batch:', iteration + 1, '--')\n",
    "                        print('    Loss_i:', initial_loss, 'Loss_f:', final_loss, 'rho', rho, 'lambda:', initial_lambda)\n",
    "\n",
    "                    # Take up to 5 more steps without recomputing the Krylov subspace\n",
    "                    for little_step in range(n_little_steps):\n",
    "                        initial_loss, initial_lambda, _ = sess.run([loss, optimizer.lambda_damp, little_train_op])\n",
    "                        final_loss, error_new, rho, _ = sess.run([loss, error, optimizer.rho, update_op])\n",
    "\n",
    "                        if error_new < error_train:\n",
    "                            error_train = error_new\n",
    "\n",
    "                        if iteration % print_interval == 0:\n",
    "                            print('    Loss_i:', initial_loss, 'Loss_f:', final_loss, 'rho', rho, 'lambda:', initial_lambda)\n",
    "                else:\n",
    "                    # Take a gradient descent step\n",
    "                    _, initial_loss, error_train = sess.run([train_op, loss, error])\n",
    "\n",
    "                    if iteration % print_interval == 0:\n",
    "                        print('-- Epoch:', epoch + 1, ' Batch:', iteration + 1, '--')\n",
    "                        print('    Loss:', initial_loss)\n",
    "\n",
    "                history += [error_train]\n",
    "                total_error += error_train\n",
    "\n",
    "                if iteration % print_interval == 0:\n",
    "                    print('    Train error:', error_train)\n",
    "                iteration += 1\n",
    "                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            \n",
    "        error_train = total_error / iteration\n",
    "        sess.run(test_init_op)\n",
    "        error_test = sess.run(error)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "\n",
    "        print('\\n*** Epoch:', epoch + 1, 'Train error:', error_train, ' Test error:', error_test, ' Time:', dt, 'sec\\n')\n",
    "        save_path = saver.save(sess, model_filepath)\n",
    "    \n",
    "    sess.close()\n",
    "    return history, optimizer.get_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train with `use_SF = False` to use the ADAM method, and with `use_SF = True` to use the Saddle-Free method.\n",
    "* Train with `start_from_previous_run = False` to start from random initialization, and with `start_from_previous_run = True` to start from where you previously left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Constructing graph...\n",
      "Training...\n",
      "-- Epoch: 1  Batch: 1 --\n",
      "    Loss_i: 1.1837842 Loss_f: 0.48032126 rho 0.5522539 lambda: 0.01\n",
      "    Loss_i: 0.48032126 Loss_f: 0.95031184 rho -0.3754485 lambda: 0.001\n",
      "    Loss_i: 0.48032126 Loss_f: 0.8683601 rho -0.36342376 lambda: 0.010000001\n",
      "    Loss_i: 0.48032126 Loss_f: 0.6522206 rho -0.22235817 lambda: 0.10000001\n",
      "    Loss_i: 0.48032126 Loss_f: 0.51418805 rho -0.001765631 lambda: 1.0000001\n",
      "    Loss_i: 0.48032126 Loss_f: 0.48377502 rho -1.6991708e-05 lambda: 10.0\n",
      "    Train error: 73.9782\n",
      "-- Epoch: 1  Batch: 2 --\n",
      "    Loss_i: 0.48446438 Loss_f: 1.7152041 rho -4.1843586 lambda: 0.01\n",
      "    Loss_i: 0.48446438 Loss_f: 0.43459043 rho 0.13331443 lambda: 0.099999994\n",
      "    Loss_i: 0.43459043 Loss_f: 0.78828263 rho -5.9407206 lambda: 0.099999994\n",
      "    Loss_i: 0.43459043 Loss_f: 0.5065517 rho -0.19927923 lambda: 0.99999994\n",
      "    Loss_i: 0.43459043 Loss_f: 0.44410688 rho -0.0020834892 lambda: 9.999999\n",
      "    Loss_i: 0.43459043 Loss_f: 0.44410688 rho -0.0020834892 lambda: 10.0\n",
      "    Train error: 65.419365\n",
      "-- Epoch: 1  Batch: 3 --\n",
      "    Loss_i: 0.44600308 Loss_f: 2.2281127 rho -4.762999 lambda: 0.01\n",
      "    Loss_i: 0.44600308 Loss_f: 0.4201556 rho 0.057237558 lambda: 0.099999994\n",
      "    Loss_i: 0.4201556 Loss_f: 0.7829878 rho -7.611311 lambda: 0.099999994\n",
      "    Loss_i: 0.4201556 Loss_f: 0.49406576 rho -0.11650316 lambda: 0.99999994\n",
      "    Loss_i: 0.4201556 Loss_f: 0.4310935 rho -0.0016823009 lambda: 9.999999\n",
      "    Loss_i: 0.4201556 Loss_f: 0.4310935 rho -0.0016823008 lambda: 10.0\n",
      "    Train error: 68.29714\n",
      "-- Epoch: 1  Batch: 4 --\n",
      "    Loss_i: 0.410392 Loss_f: 1.543134 rho -4.3441496 lambda: 0.01\n",
      "    Loss_i: 0.410392 Loss_f: 0.3771492 rho 0.10703832 lambda: 0.099999994\n",
      "    Loss_i: 0.3771492 Loss_f: 0.66654825 rho -10.363473 lambda: 0.099999994\n",
      "    Loss_i: 0.3771492 Loss_f: 0.43680903 rho -0.10507496 lambda: 0.99999994\n",
      "    Loss_i: 0.3771492 Loss_f: 0.38557932 rho -0.001291999 lambda: 9.999999\n",
      "    Loss_i: 0.3771492 Loss_f: 0.38557923 rho -0.001291985 lambda: 10.0\n",
      "    Train error: 62.075577\n",
      "-- Epoch: 1  Batch: 5 --\n",
      "    Loss_i: 0.38722298 Loss_f: 1.6768559 rho -5.026875 lambda: 0.01\n",
      "    Loss_i: 0.38722298 Loss_f: 0.39836407 rho -0.038125116 lambda: 0.099999994\n",
      "    Loss_i: 0.38722298 Loss_f: 0.32860163 rho 0.09032663 lambda: 0.99999994\n",
      "    Loss_i: 0.32860163 Loss_f: 0.31786698 rho 0.024495283 lambda: 0.99999994\n",
      "    Loss_i: 0.31786698 Loss_f: 0.34898347 rho -0.08548787 lambda: 0.99999994\n",
      "    Loss_i: 0.31786698 Loss_f: 0.32182333 rho -0.0013902991 lambda: 9.999999\n",
      "    Train error: 58.89289\n",
      "-- Epoch: 1  Batch: 6 --\n",
      "    Loss_i: 0.3161718 Loss_f: 0.66466445 rho -2.538802 lambda: 0.01\n",
      "    Loss_i: 0.3161718 Loss_f: 0.3152013 rho 0.006386042 lambda: 0.099999994\n",
      "    Loss_i: 0.3152013 Loss_f: 0.4750202 rho -12.0206175 lambda: 0.099999994\n",
      "    Loss_i: 0.3152013 Loss_f: 0.35557717 rho -0.15259697 lambda: 0.99999994\n",
      "    Loss_i: 0.3152013 Loss_f: 0.3205186 rho -0.0019143824 lambda: 9.999999\n",
      "    Loss_i: 0.3152013 Loss_f: 0.3205186 rho -0.0019143824 lambda: 10.0\n",
      "    Train error: 60.43853\n",
      "-- Epoch: 1  Batch: 7 --\n",
      "    Loss_i: 0.326848 Loss_f: 0.7912028 rho -3.7072766 lambda: 0.01\n",
      "    Loss_i: 0.326848 Loss_f: 0.351579 rho -0.17238905 lambda: 0.099999994\n",
      "    Loss_i: 0.326848 Loss_f: 0.28734103 rho 0.1213674 lambda: 0.99999994\n",
      "    Loss_i: 0.28734103 Loss_f: 0.28919873 rho -0.00821854 lambda: 0.99999994\n",
      "    Loss_i: 0.28734103 Loss_f: 0.28539982 rho 0.0010993481 lambda: 9.999999\n",
      "    Loss_i: 0.28539982 Loss_f: 0.284673 rho 0.00041342215 lambda: 9.999999\n",
      "    Train error: 56.56233\n",
      "-- Epoch: 1  Batch: 8 --\n",
      "    Loss_i: 0.28226772 Loss_f: 0.36393172 rho -1.3201065 lambda: 0.01\n",
      "    Loss_i: 0.28226772 Loss_f: 0.27577516 rho 0.08669338 lambda: 0.099999994\n",
      "    Loss_i: 0.27577516 Loss_f: 0.30474985 rho -0.7758064 lambda: 0.099999994\n",
      "    Loss_i: 0.27577516 Loss_f: 0.2841346 rho -0.037831113 lambda: 0.99999994\n",
      "    Loss_i: 0.27577516 Loss_f: 0.27685928 rho -0.0005269979 lambda: 9.999999\n",
      "    Loss_i: 0.27577516 Loss_f: 0.27685928 rho -0.00052699784 lambda: 10.0\n",
      "    Train error: 53.88458\n",
      "-- Epoch: 1  Batch: 9 --\n",
      "    Loss_i: 0.27066472 Loss_f: 0.37326294 rho -4.3688893 lambda: 0.01\n",
      "    Loss_i: 0.27066472 Loss_f: 0.2743757 rho -0.13388668 lambda: 0.099999994\n",
      "    Loss_i: 0.27066472 Loss_f: 0.2650383 rho 0.08031678 lambda: 0.99999994\n",
      "    Loss_i: 0.2650383 Loss_f: 0.264711 rho 0.004778028 lambda: 0.99999994\n",
      "    Loss_i: 0.264711 Loss_f: 0.26968825 rho -0.05740871 lambda: 0.99999994\n",
      "    Loss_i: 0.264711 Loss_f: 0.26542866 rho -0.0010163496 lambda: 9.999999\n",
      "    Train error: 51.830265\n",
      "-- Epoch: 1  Batch: 10 --\n",
      "    Loss_i: 0.27070007 Loss_f: 0.29141897 rho -0.29315034 lambda: 0.01\n",
      "    Loss_i: 0.27070007 Loss_f: 0.26913518 rho 0.015817415 lambda: 0.099999994\n",
      "    Loss_i: 0.26913518 Loss_f: 0.28459635 rho -0.17839879 lambda: 0.099999994\n",
      "    Loss_i: 0.26913518 Loss_f: 0.27583387 rho -0.015452246 lambda: 0.99999994\n",
      "    Loss_i: 0.26913518 Loss_f: 0.27016023 rho -0.00026270284 lambda: 9.999999\n",
      "    Loss_i: 0.26913518 Loss_f: 0.27016023 rho -0.00026270282 lambda: 10.0\n",
      "    Train error: 53.79646\n",
      "-- Epoch: 1  Batch: 11 --\n",
      "    Loss_i: 0.27057317 Loss_f: 0.27301705 rho -0.057099417 lambda: 0.01\n",
      "    Loss_i: 0.27057317 Loss_f: 0.2682763 rho 0.029429108 lambda: 0.099999994\n",
      "    Loss_i: 0.2682763 Loss_f: 0.2781254 rho -0.11128228 lambda: 0.099999994\n",
      "    Loss_i: 0.2682763 Loss_f: 0.27249083 rho -0.008057686 lambda: 0.99999994\n",
      "    Loss_i: 0.2682763 Loss_f: 0.2689195 rho -0.00013211518 lambda: 9.999999\n",
      "    Loss_i: 0.2682763 Loss_f: 0.26891953 rho -0.00013212129 lambda: 10.0\n",
      "    Train error: 53.5259\n",
      "-- Epoch: 1  Batch: 12 --\n",
      "    Loss_i: 0.27305424 Loss_f: 0.27194095 rho 0.017899869 lambda: 0.01\n",
      "    Loss_i: 0.27194095 Loss_f: 0.29857013 rho -0.52592415 lambda: 0.01\n",
      "    Loss_i: 0.27194095 Loss_f: 0.2832161 rho -0.11146071 lambda: 0.099999994\n",
      "    Loss_i: 0.27194095 Loss_f: 0.2760771 rho -0.0068207397 lambda: 0.99999994\n",
      "    Loss_i: 0.27194095 Loss_f: 0.2725676 rho -0.00011073778 lambda: 9.999999\n",
      "    Loss_i: 0.27194095 Loss_f: 0.27256763 rho -0.000110743036 lambda: 10.0\n",
      "    Train error: 54.68554\n",
      "-- Epoch: 1  Batch: 13 --\n",
      "    Loss_i: 0.26293525 Loss_f: 0.25998488 rho 0.04281475 lambda: 0.01\n",
      "    Loss_i: 0.25998488 Loss_f: 0.28384438 rho -0.35550767 lambda: 0.01\n",
      "    Loss_i: 0.25998488 Loss_f: 0.2724236 rho -0.05208867 lambda: 0.099999994\n",
      "    Loss_i: 0.25998488 Loss_f: 0.2669118 rho -0.0035420083 lambda: 0.99999994\n",
      "    Loss_i: 0.25998488 Loss_f: 0.26155046 rho -8.18638e-05 lambda: 9.999999\n",
      "    Loss_i: 0.25998488 Loss_f: 0.26155046 rho -8.186378e-05 lambda: 10.0\n",
      "    Train error: 52.142273\n",
      "-- Epoch: 1  Batch: 14 --\n",
      "    Loss_i: 0.2634501 Loss_f: 0.2768028 rho -0.07398555 lambda: 0.01\n",
      "    Loss_i: 0.2634501 Loss_f: 0.2699425 rho -0.021461438 lambda: 0.099999994\n",
      "    Loss_i: 0.2634501 Loss_f: 0.25913808 rho 0.0028314663 lambda: 0.99999994\n",
      "    Loss_i: 0.25913808 Loss_f: 0.26961917 rho -0.006694622 lambda: 0.99999994\n",
      "    Loss_i: 0.25913808 Loss_f: 0.26114732 rho -0.00014078681 lambda: 9.999999\n",
      "    Loss_i: 0.25913808 Loss_f: 0.26114732 rho -0.00014078681 lambda: 10.0\n",
      "    Train error: 51.114674\n",
      "-- Epoch: 1  Batch: 15 --\n",
      "    Loss_i: 0.263215 Loss_f: 0.26987702 rho -0.09459573 lambda: 0.01\n",
      "    Loss_i: 0.263215 Loss_f: 0.26911706 rho -0.027762039 lambda: 0.099999994\n",
      "    Loss_i: 0.263215 Loss_f: 0.2620778 rho 0.0006958414 lambda: 0.99999994\n",
      "    Loss_i: 0.2620778 Loss_f: 0.2710391 rho -0.0050185914 lambda: 0.99999994\n",
      "    Loss_i: 0.2620778 Loss_f: 0.26392704 rho -0.000108958375 lambda: 9.999999\n",
      "    Loss_i: 0.2620778 Loss_f: 0.26392704 rho -0.00010895837 lambda: 10.0\n",
      "    Train error: 52.241287\n",
      "-- Epoch: 1  Batch: 16 --\n",
      "    Loss_i: 0.25527018 Loss_f: 0.26215196 rho -0.053695474 lambda: 0.01\n",
      "    Loss_i: 0.25527018 Loss_f: 0.26006016 rho -0.016893027 lambda: 0.099999994\n",
      "    Loss_i: 0.25527018 Loss_f: 0.2550299 rho 0.00013078096 lambda: 0.99999994\n",
      "    Loss_i: 0.2550299 Loss_f: 0.2618442 rho -0.003635088 lambda: 0.99999994\n",
      "    Loss_i: 0.2550299 Loss_f: 0.2565487 rho -8.445188e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2550299 Loss_f: 0.2565487 rho -8.445188e-05 lambda: 10.0\n",
      "    Train error: 49.845837\n",
      "-- Epoch: 1  Batch: 17 --\n",
      "    Loss_i: 0.25938302 Loss_f: 0.27011308 rho -0.12447986 lambda: 0.01\n",
      "    Loss_i: 0.25938302 Loss_f: 0.26933578 rho -0.041149944 lambda: 0.099999994\n",
      "    Loss_i: 0.25938302 Loss_f: 0.26206285 rho -0.0014900095 lambda: 0.99999994\n",
      "    Loss_i: 0.25938302 Loss_f: 0.25759733 rho 0.00010283206 lambda: 9.999999\n",
      "    Loss_i: 0.25759733 Loss_f: 0.25731763 rho 1.6109425e-05 lambda: 9.999999\n",
      "    Loss_i: 0.25731763 Loss_f: 0.25844362 rho -6.465961e-05 lambda: 9.999999\n",
      "    Train error: 50.75768\n",
      "-- Epoch: 1  Batch: 18 --\n",
      "    Loss_i: 0.2581732 Loss_f: 0.25968257 rho -0.01768693 lambda: 0.01\n",
      "    Loss_i: 0.2581732 Loss_f: 0.26023936 rho -0.007773415 lambda: 0.099999994\n",
      "    Loss_i: 0.2581732 Loss_f: 0.25736332 rho 0.00039116843 lambda: 0.99999994\n",
      "    Loss_i: 0.25736332 Loss_f: 0.2620786 rho -0.0022823724 lambda: 0.99999994\n",
      "    Loss_i: 0.25736332 Loss_f: 0.25814223 rho -3.8432187e-05 lambda: 9.999999\n",
      "    Loss_i: 0.25736332 Loss_f: 0.25814223 rho -3.8432183e-05 lambda: 10.0\n",
      "    Train error: 50.48218\n",
      "-- Epoch: 1  Batch: 19 --\n",
      "    Loss_i: 0.26294202 Loss_f: 0.27265126 rho -0.09647699 lambda: 0.01\n",
      "    Loss_i: 0.26294202 Loss_f: 0.27299985 rho -0.02896024 lambda: 0.099999994\n",
      "    Loss_i: 0.26294202 Loss_f: 0.26770407 rho -0.0016923315 lambda: 0.99999994\n",
      "    Loss_i: 0.26294202 Loss_f: 0.26198244 rho 3.491914e-05 lambda: 9.999999\n",
      "    Loss_i: 0.26198244 Loss_f: 0.2628288 rho -3.0827865e-05 lambda: 9.999999\n",
      "    Loss_i: 0.26198244 Loss_f: 0.26282877 rho -3.0826774e-05 lambda: 10.0\n",
      "    Train error: 52.27148\n",
      "-- Epoch: 1  Batch: 20 --\n",
      "    Loss_i: 0.25607124 Loss_f: 0.25063366 rho 0.09102693 lambda: 0.01\n",
      "    Loss_i: 0.25063366 Loss_f: 0.25343245 rho -0.031228747 lambda: 0.01\n",
      "    Loss_i: 0.25063366 Loss_f: 0.25522384 rho -0.008749327 lambda: 0.099999994\n",
      "    Loss_i: 0.25063366 Loss_f: 0.25349697 rho -0.0005873785 lambda: 0.99999994\n",
      "    Loss_i: 0.25063366 Loss_f: 0.25120062 rho -1.1719898e-05 lambda: 9.999999\n",
      "    Loss_i: 0.25063366 Loss_f: 0.25120062 rho -1.1719896e-05 lambda: 10.0\n",
      "    Train error: 48.800335\n",
      "-- Epoch: 1  Batch: 21 --\n",
      "    Loss_i: 0.25258094 Loss_f: 0.25905567 rho -0.048073284 lambda: 0.01\n",
      "    Loss_i: 0.25258094 Loss_f: 0.2581823 rho -0.011070966 lambda: 0.099999994\n",
      "    Loss_i: 0.25258094 Loss_f: 0.25300053 rho -9.946124e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.25258094 Loss_f: 0.2517482 rho 2.0141133e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2517482 Loss_f: 0.25151834 rho 5.5709247e-06 lambda: 9.999999\n",
      "    Loss_i: 0.25151834 Loss_f: 0.2519132 rho -9.584289e-06 lambda: 9.999999\n",
      "    Train error: 48.64576\n",
      "-- Epoch: 1  Batch: 22 --\n",
      "    Loss_i: 0.249193 Loss_f: 0.25660846 rho -0.11554498 lambda: 0.01\n",
      "    Loss_i: 0.249193 Loss_f: 0.25787348 rho -0.015048603 lambda: 0.099999994\n",
      "    Loss_i: 0.249193 Loss_f: 0.25303346 rho -0.0006733702 lambda: 0.99999994\n",
      "    Loss_i: 0.249193 Loss_f: 0.2487861 rho 7.1423888e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2487861 Loss_f: 0.24934837 rho -9.844039e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2487861 Loss_f: 0.24934837 rho -9.844039e-06 lambda: 10.0\n",
      "    Train error: 48.295307\n",
      "-- Epoch: 1  Batch: 23 --\n",
      "    Loss_i: 0.25100845 Loss_f: 0.24961188 rho 0.0068839476 lambda: 0.01\n",
      "    Loss_i: 0.24961188 Loss_f: 0.25962293 rho -0.06325674 lambda: 0.01\n",
      "    Loss_i: 0.24961188 Loss_f: 0.26034856 rho -0.011193223 lambda: 0.099999994\n",
      "    Loss_i: 0.24961188 Loss_f: 0.2570953 rho -0.0008343913 lambda: 0.99999994\n",
      "    Loss_i: 0.24961188 Loss_f: 0.2513473 rho -1.9485124e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24961188 Loss_f: 0.2513473 rho -1.9485122e-05 lambda: 10.0\n",
      "    Train error: 47.788185\n",
      "-- Epoch: 1  Batch: 24 --\n",
      "    Loss_i: 0.25175542 Loss_f: 0.2825391 rho -0.49808547 lambda: 0.01\n",
      "    Loss_i: 0.25175542 Loss_f: 0.28058523 rho -0.061654884 lambda: 0.099999994\n",
      "    Loss_i: 0.25175542 Loss_f: 0.2734992 rho -0.004804662 lambda: 0.99999994\n",
      "    Loss_i: 0.25175542 Loss_f: 0.24735823 rho 9.748743e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24735823 Loss_f: 0.25141507 rho -8.933201e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24735823 Loss_f: 0.25141507 rho -8.933199e-05 lambda: 10.0\n",
      "    Train error: 47.56549\n",
      "-- Epoch: 1  Batch: 25 --\n",
      "    Loss_i: 0.25002328 Loss_f: 0.25657782 rho -0.042878155 lambda: 0.01\n",
      "    Loss_i: 0.25002328 Loss_f: 0.2558374 rho -0.007419225 lambda: 0.099999994\n",
      "    Loss_i: 0.25002328 Loss_f: 0.24965486 rho 5.1951138e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.24965486 Loss_f: 0.25993308 rho -0.0014397614 lambda: 0.99999994\n",
      "    Loss_i: 0.24965486 Loss_f: 0.25146332 rho -2.5571051e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24965486 Loss_f: 0.25146332 rho -2.5571047e-05 lambda: 10.0\n",
      "    Train error: 48.27457\n",
      "-- Epoch: 1  Batch: 26 --\n",
      "    Loss_i: 0.24389274 Loss_f: 0.25903985 rho -0.10426707 lambda: 0.01\n",
      "    Loss_i: 0.24389274 Loss_f: 0.2564699 rho -0.014305165 lambda: 0.099999994\n",
      "    Loss_i: 0.24389274 Loss_f: 0.24682938 rho -0.00035731867 lambda: 0.99999994\n",
      "    Loss_i: 0.24389274 Loss_f: 0.24232894 rho 1.9161489e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24232894 Loss_f: 0.24211033 rho 2.6808286e-06 lambda: 9.999999\n",
      "    Loss_i: 0.24211033 Loss_f: 0.24327596 rho -1.429849e-05 lambda: 9.999999\n",
      "    Train error: 46.31976\n",
      "-- Epoch: 1  Batch: 27 --\n",
      "    Loss_i: 0.23968957 Loss_f: 0.24328744 rho -0.024171552 lambda: 0.01\n",
      "    Loss_i: 0.23968957 Loss_f: 0.24575697 rho -0.005060394 lambda: 0.099999994\n",
      "    Loss_i: 0.23968957 Loss_f: 0.24249655 rho -0.00023990261 lambda: 0.99999994\n",
      "    Loss_i: 0.23968957 Loss_f: 0.23947273 rho 1.8578669e-06 lambda: 9.999999\n",
      "    Loss_i: 0.23947273 Loss_f: 0.23990262 rho -3.677905e-06 lambda: 9.999999\n",
      "    Loss_i: 0.23947273 Loss_f: 0.23990262 rho -3.6779045e-06 lambda: 10.0\n",
      "    Train error: 45.328312\n",
      "-- Epoch: 1  Batch: 28 --\n",
      "    Loss_i: 0.24260451 Loss_f: 0.24079691 rho 0.009791102 lambda: 0.01\n",
      "    Loss_i: 0.24079691 Loss_f: 0.2549621 rho -0.0813047 lambda: 0.01\n",
      "    Loss_i: 0.24079691 Loss_f: 0.25548434 rho -0.009280831 lambda: 0.099999994\n",
      "    Loss_i: 0.24079691 Loss_f: 0.25026459 rho -0.0006043498 lambda: 0.99999994\n",
      "    Loss_i: 0.24079691 Loss_f: 0.2428123 rho -1.28779975e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24079691 Loss_f: 0.24281228 rho -1.28778065e-05 lambda: 10.0\n",
      "    Train error: 45.9232\n",
      "-- Epoch: 1  Batch: 29 --\n",
      "    Loss_i: 0.24176122 Loss_f: 0.28015158 rho -0.14829604 lambda: 0.01\n",
      "    Loss_i: 0.24176122 Loss_f: 0.28355655 rho -0.042460185 lambda: 0.099999994\n",
      "    Loss_i: 0.24176122 Loss_f: 0.27922842 rho -0.0045475457 lambda: 0.99999994\n",
      "    Loss_i: 0.24176122 Loss_f: 0.23736255 rho 5.4448803e-05 lambda: 9.999999\n",
      "    Loss_i: 0.23736255 Loss_f: 0.24960816 rho -0.00015270147 lambda: 9.999999\n",
      "    Loss_i: 0.23736255 Loss_f: 0.24960817 rho -0.00015270163 lambda: 10.0\n",
      "    Train error: 44.687054\n",
      "-- Epoch: 1  Batch: 30 --\n",
      "    Loss_i: 0.24132662 Loss_f: 0.27701965 rho -0.15896487 lambda: 0.01\n",
      "    Loss_i: 0.24132662 Loss_f: 0.2709832 rho -0.022798013 lambda: 0.099999994\n",
      "    Loss_i: 0.24132662 Loss_f: 0.25331277 rho -0.0009935555 lambda: 0.99999994\n",
      "    Loss_i: 0.24132662 Loss_f: 0.24022892 rho 9.170797e-06 lambda: 9.999999\n",
      "    Loss_i: 0.24022892 Loss_f: 0.24164346 rho -1.1865523e-05 lambda: 9.999999\n",
      "    Loss_i: 0.24022892 Loss_f: 0.24164346 rho -1.1865523e-05 lambda: 10.0\n",
      "    Train error: 45.242195\n",
      "\n",
      "*** Epoch: 1 Train error: 52.970411936442055  Test error: 44.799732  Time: 718.9034851 sec\n",
      "\n",
      "-- Epoch: 2  Batch: 1 --\n",
      "    Loss_i: 0.2362566 Loss_f: 0.23598145 rho 0.0017680135 lambda: 0.01\n",
      "    Loss_i: 0.23598145 Loss_f: 0.24736814 rho -0.037495397 lambda: 0.01\n",
      "    Loss_i: 0.23598145 Loss_f: 0.2481142 rho -0.0059976145 lambda: 0.099999994\n",
      "    Loss_i: 0.23598145 Loss_f: 0.24422596 rho -0.00042905772 lambda: 0.99999994\n",
      "    Loss_i: 0.23598145 Loss_f: 0.23788187 rho -9.942548e-06 lambda: 9.999999\n",
      "    Loss_i: 0.23598145 Loss_f: 0.23788187 rho -9.942547e-06 lambda: 10.0\n",
      "    Train error: 44.791336\n",
      "-- Epoch: 2  Batch: 2 --\n",
      "    Loss_i: 0.23773442 Loss_f: 0.2764255 rho -0.10619306 lambda: 0.01\n",
      "    Loss_i: 0.23773442 Loss_f: 0.27996266 rho -0.03668911 lambda: 0.099999994\n",
      "    Loss_i: 0.23773442 Loss_f: 0.2787874 rho -0.0045527155 lambda: 0.99999994\n",
      "    Loss_i: 0.23773442 Loss_f: 0.23298275 rho 5.419325e-05 lambda: 9.999999\n",
      "    Loss_i: 0.23298275 Loss_f: 0.24204324 rho -0.000103792096 lambda: 9.999999\n",
      "    Loss_i: 0.23298275 Loss_f: 0.24204324 rho -0.00010379207 lambda: 10.0\n",
      "    Train error: 43.304756\n",
      "-- Epoch: 2  Batch: 3 --\n",
      "    Loss_i: 0.23794366 Loss_f: 0.256895 rho -0.065259516 lambda: 0.01\n",
      "    Loss_i: 0.23794366 Loss_f: 0.2523587 rho -0.008068188 lambda: 0.099999994\n",
      "    Loss_i: 0.23794366 Loss_f: 0.24146305 rho -0.00021012312 lambda: 0.99999994\n",
      "    Loss_i: 0.23794366 Loss_f: 0.23666203 rho 7.703316e-06 lambda: 9.999999\n",
      "    Loss_i: 0.23666203 Loss_f: 0.23664448 rho 1.055511e-07 lambda: 9.999999\n",
      "    Loss_i: 0.23664448 Loss_f: 0.2378301 rho -7.129298e-06 lambda: 9.999999\n",
      "    Train error: 44.35785\n",
      "-- Epoch: 2  Batch: 4 --\n",
      "    Loss_i: 0.22838728 Loss_f: 0.26150677 rho -0.15824887 lambda: 0.01\n",
      "    Loss_i: 0.22838728 Loss_f: 0.25709987 rho -0.014999003 lambda: 0.099999994\n",
      "    Loss_i: 0.22838728 Loss_f: 0.24073301 rho -0.00065099454 lambda: 0.99999994\n",
      "    Loss_i: 0.22838728 Loss_f: 0.22784777 rho 2.847543e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22784777 Loss_f: 0.22960168 rho -9.25895e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22784777 Loss_f: 0.2296017 rho -9.259028e-06 lambda: 10.0\n",
      "    Train error: 42.216583\n",
      "-- Epoch: 2  Batch: 5 --\n",
      "    Loss_i: 0.23404066 Loss_f: 0.23945364 rho -0.026220147 lambda: 0.01\n",
      "    Loss_i: 0.23404066 Loss_f: 0.24142945 rho -0.00345886 lambda: 0.099999994\n",
      "    Loss_i: 0.23404066 Loss_f: 0.23723641 rho -0.00014909949 lambda: 0.99999994\n",
      "    Loss_i: 0.23404066 Loss_f: 0.2338492 rho 8.929921e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2338492 Loss_f: 0.23431344 rho -2.1647043e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2338492 Loss_f: 0.23431344 rho -2.164704e-06 lambda: 10.0\n",
      "    Train error: 43.798492\n",
      "-- Epoch: 2  Batch: 6 --\n",
      "    Loss_i: 0.22914611 Loss_f: 0.22813542 rho 0.0036645061 lambda: 0.01\n",
      "    Loss_i: 0.22813542 Loss_f: 0.2391779 rho -0.033135194 lambda: 0.01\n",
      "    Loss_i: 0.22813542 Loss_f: 0.240737 rho -0.00428854 lambda: 0.099999994\n",
      "    Loss_i: 0.22813542 Loss_f: 0.23629776 rho -0.00028155488 lambda: 0.99999994\n",
      "    Loss_i: 0.22813542 Loss_f: 0.22980204 rho -5.756734e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22813542 Loss_f: 0.22980201 rho -5.7566303e-06 lambda: 10.0\n",
      "    Train error: 42.506977\n",
      "-- Epoch: 2  Batch: 7 --\n",
      "    Loss_i: 0.23848104 Loss_f: 0.29896253 rho -0.17259437 lambda: 0.01\n",
      "    Loss_i: 0.23848104 Loss_f: 0.30409914 rho -0.043269463 lambda: 0.099999994\n",
      "    Loss_i: 0.23848104 Loss_f: 0.29451624 rho -0.004252424 lambda: 0.99999994\n",
      "    Loss_i: 0.23848104 Loss_f: 0.23628543 rho 1.6917358e-05 lambda: 9.999999\n",
      "    Loss_i: 0.23628543 Loss_f: 0.25319564 rho -0.00012963233 lambda: 9.999999\n",
      "    Loss_i: 0.23628543 Loss_f: 0.25319564 rho -0.00012963232 lambda: 10.0\n",
      "    Train error: 44.357056\n",
      "-- Epoch: 2  Batch: 8 --\n",
      "    Loss_i: 0.2345234 Loss_f: 0.30667567 rho -0.35544384 lambda: 0.01\n",
      "    Loss_i: 0.2345234 Loss_f: 0.30225003 rho -0.043480027 lambda: 0.099999994\n",
      "    Loss_i: 0.2345234 Loss_f: 0.2756588 rho -0.0027234366 lambda: 0.99999994\n",
      "    Loss_i: 0.2345234 Loss_f: 0.23327883 rho 8.265759e-06 lambda: 9.999999\n",
      "    Loss_i: 0.23327883 Loss_f: 0.24100144 rho -5.098798e-05 lambda: 9.999999\n",
      "    Loss_i: 0.23327883 Loss_f: 0.24100144 rho -5.0987972e-05 lambda: 10.0\n",
      "    Train error: 43.27079\n",
      "-- Epoch: 2  Batch: 9 --\n",
      "    Loss_i: 0.22721201 Loss_f: 0.26124758 rho -0.105879106 lambda: 0.01\n",
      "    Loss_i: 0.22721201 Loss_f: 0.26034874 rho -0.01723609 lambda: 0.099999994\n",
      "    Loss_i: 0.22721201 Loss_f: 0.24533498 rho -0.0010105846 lambda: 0.99999994\n",
      "    Loss_i: 0.22721201 Loss_f: 0.22647305 rho 4.1505596e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22647305 Loss_f: 0.22966243 rho -1.7857126e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22647305 Loss_f: 0.22966243 rho -1.785712e-05 lambda: 10.0\n",
      "    Train error: 41.70833\n",
      "-- Epoch: 2  Batch: 10 --\n",
      "    Loss_i: 0.22961627 Loss_f: 0.26194653 rho -0.11798857 lambda: 0.01\n",
      "    Loss_i: 0.22961627 Loss_f: 0.25668678 rho -0.012199368 lambda: 0.099999994\n",
      "    Loss_i: 0.22961627 Loss_f: 0.24058323 rho -0.00050611334 lambda: 0.99999994\n",
      "    Loss_i: 0.22961627 Loss_f: 0.22925043 rho 1.6923746e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22925043 Loss_f: 0.23078874 rho -7.1029654e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22925043 Loss_f: 0.23078872 rho -7.1028953e-06 lambda: 10.0\n",
      "    Train error: 42.52855\n",
      "-- Epoch: 2  Batch: 11 --\n",
      "    Loss_i: 0.22956955 Loss_f: 0.23925658 rho -0.034565825 lambda: 0.01\n",
      "    Loss_i: 0.22956955 Loss_f: 0.24055906 rho -0.004504055 lambda: 0.099999994\n",
      "    Loss_i: 0.22956955 Loss_f: 0.23454025 rho -0.00020679738 lambda: 0.99999994\n",
      "    Loss_i: 0.22956955 Loss_f: 0.22935541 rho 8.9225773e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22935541 Loss_f: 0.23011309 rho -3.1554582e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22935541 Loss_f: 0.23011309 rho -3.1554578e-06 lambda: 10.0\n",
      "    Train error: 42.581512\n",
      "-- Epoch: 2  Batch: 12 --\n",
      "    Loss_i: 0.23399359 Loss_f: 0.24034695 rho -0.020748064 lambda: 0.01\n",
      "    Loss_i: 0.23399359 Loss_f: 0.24181083 rho -0.0029983846 lambda: 0.099999994\n",
      "    Loss_i: 0.23399359 Loss_f: 0.23721915 rho -0.00012591739 lambda: 0.99999994\n",
      "    Loss_i: 0.23399359 Loss_f: 0.23386379 rho 5.0762173e-07 lambda: 9.999999\n",
      "    Loss_i: 0.23386379 Loss_f: 0.2343101 rho -1.7457287e-06 lambda: 9.999999\n",
      "    Loss_i: 0.23386379 Loss_f: 0.2343101 rho -1.7457285e-06 lambda: 10.0\n",
      "    Train error: 43.47732\n",
      "-- Epoch: 2  Batch: 13 --\n",
      "    Loss_i: 0.2234642 Loss_f: 0.22585392 rho -0.0070468346 lambda: 0.01\n",
      "    Loss_i: 0.2234642 Loss_f: 0.22839609 rho -0.0017457305 lambda: 0.099999994\n",
      "    Loss_i: 0.2234642 Loss_f: 0.22579439 rho -8.416768e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.2234642 Loss_f: 0.22336659 rho 3.5332286e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22336659 Loss_f: 0.22371843 rho -1.2730668e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22336659 Loss_f: 0.22371843 rho -1.2730667e-06 lambda: 10.0\n",
      "    Train error: 41.31294\n",
      "-- Epoch: 2  Batch: 14 --\n",
      "    Loss_i: 0.22684209 Loss_f: 0.22721137 rho -0.0015085795 lambda: 0.01\n",
      "    Loss_i: 0.22684209 Loss_f: 0.23056419 rho -0.0014720982 lambda: 0.099999994\n",
      "    Loss_i: 0.22684209 Loss_f: 0.22876975 rho -7.5997144e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.22684209 Loss_f: 0.22675574 rho 3.4033184e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22675574 Loss_f: 0.22704932 rho -1.156868e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22675574 Loss_f: 0.22704932 rho -1.1568679e-06 lambda: 10.0\n",
      "    Train error: 41.568237\n",
      "-- Epoch: 2  Batch: 15 --\n",
      "    Loss_i: 0.22753318 Loss_f: 0.22715579 rho 0.001312804 lambda: 0.01\n",
      "    Loss_i: 0.22715579 Loss_f: 0.23669611 rho -0.024998669 lambda: 0.01\n",
      "    Loss_i: 0.22715579 Loss_f: 0.23801367 rho -0.0032584346 lambda: 0.099999994\n",
      "    Loss_i: 0.22715579 Loss_f: 0.23423171 rho -0.00021547782 lambda: 0.99999994\n",
      "    Loss_i: 0.22715579 Loss_f: 0.22862583 rho -4.4832263e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22715579 Loss_f: 0.22862583 rho -4.483226e-06 lambda: 10.0\n",
      "    Train error: 42.2821\n",
      "-- Epoch: 2  Batch: 16 --\n",
      "    Loss_i: 0.22493348 Loss_f: 0.30812123 rho -0.21263921 lambda: 0.01\n",
      "    Loss_i: 0.22493348 Loss_f: 0.31766194 rho -0.052727155 lambda: 0.099999994\n",
      "    Loss_i: 0.22493348 Loss_f: 0.30271536 rho -0.005039982 lambda: 0.99999994\n",
      "    Loss_i: 0.22493348 Loss_f: 0.2235978 rho 8.777197e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2235978 Loss_f: 0.24780978 rho -0.00015741901 lambda: 9.999999\n",
      "    Loss_i: 0.2235978 Loss_f: 0.24780978 rho -0.000157419 lambda: 10.0\n",
      "    Train error: 40.57031\n",
      "-- Epoch: 2  Batch: 17 --\n",
      "    Loss_i: 0.22492552 Loss_f: 0.29375944 rho -0.20895699 lambda: 0.01\n",
      "    Loss_i: 0.22492552 Loss_f: 0.29096052 rho -0.026173858 lambda: 0.099999994\n",
      "    Loss_i: 0.22492552 Loss_f: 0.27546418 rho -0.0020663328 lambda: 0.99999994\n",
      "    Loss_i: 0.22492552 Loss_f: 0.22416322 rho 3.126607e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22416322 Loss_f: 0.23889337 rho -6.014593e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22416322 Loss_f: 0.23889337 rho -6.0145914e-05 lambda: 10.0\n",
      "    Train error: 41.284935\n",
      "-- Epoch: 2  Batch: 18 --\n",
      "    Loss_i: 0.22560237 Loss_f: 0.3082487 rho -0.37860197 lambda: 0.01\n",
      "    Loss_i: 0.22560237 Loss_f: 0.30659205 rho -0.040160444 lambda: 0.099999994\n",
      "    Loss_i: 0.22560237 Loss_f: 0.27983323 rho -0.0027115096 lambda: 0.99999994\n",
      "    Loss_i: 0.22560237 Loss_f: 0.22456273 rho 5.202459e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22456273 Loss_f: 0.236221 rho -5.7919497e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22456273 Loss_f: 0.23622099 rho -5.791942e-05 lambda: 10.0\n",
      "    Train error: 40.822155\n",
      "-- Epoch: 2  Batch: 19 --\n",
      "    Loss_i: 0.22855926 Loss_f: 0.28627786 rho -0.14091003 lambda: 0.01\n",
      "    Loss_i: 0.22855926 Loss_f: 0.28141344 rho -0.021518592 lambda: 0.099999994\n",
      "    Loss_i: 0.22855926 Loss_f: 0.25915724 rho -0.0013348642 lambda: 0.99999994\n",
      "    Loss_i: 0.22855926 Loss_f: 0.22816315 rho 1.7404852e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22816315 Loss_f: 0.23422705 rho -2.648728e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22816315 Loss_f: 0.23422705 rho -2.6487278e-05 lambda: 10.0\n",
      "    Train error: 42.374077\n",
      "-- Epoch: 2  Batch: 20 --\n",
      "    Loss_i: 0.22236355 Loss_f: 0.27160418 rho -0.1413804 lambda: 0.01\n",
      "    Loss_i: 0.22236355 Loss_f: 0.2648022 rho -0.015743162 lambda: 0.099999994\n",
      "    Loss_i: 0.22236355 Loss_f: 0.24201752 rho -0.00075102015 lambda: 0.99999994\n",
      "    Loss_i: 0.22236355 Loss_f: 0.22203603 rho 1.2552714e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22203603 Loss_f: 0.22509287 rho -1.16853635e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22203603 Loss_f: 0.22509287 rho -1.1685361e-05 lambda: 10.0\n",
      "    Train error: 40.365932\n",
      "-- Epoch: 2  Batch: 21 --\n",
      "    Loss_i: 0.22476506 Loss_f: 0.24204575 rho -0.044861387 lambda: 0.01\n",
      "    Loss_i: 0.22476506 Loss_f: 0.24155127 rho -0.0058800727 lambda: 0.099999994\n",
      "    Loss_i: 0.22476506 Loss_f: 0.2324096 rho -0.00027747505 lambda: 0.99999994\n",
      "    Loss_i: 0.22476506 Loss_f: 0.22458121 rho 6.697496e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22458121 Loss_f: 0.22581314 rho -4.4834965e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22458121 Loss_f: 0.22581314 rho -4.483496e-06 lambda: 10.0\n",
      "    Train error: 41.07198\n",
      "-- Epoch: 2  Batch: 22 --\n",
      "    Loss_i: 0.2209707 Loss_f: 0.22719903 rho -0.018535035 lambda: 0.01\n",
      "    Loss_i: 0.2209707 Loss_f: 0.2287844 rho -0.0025638891 lambda: 0.099999994\n",
      "    Loss_i: 0.2209707 Loss_f: 0.22424853 rho -0.000108669556 lambda: 0.99999994\n",
      "    Loss_i: 0.2209707 Loss_f: 0.22090438 rho 2.2011552e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22090438 Loss_f: 0.22138743 rho -1.6016365e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22090438 Loss_f: 0.22138746 rho -1.6017352e-06 lambda: 10.0\n",
      "    Train error: 40.20492\n",
      "-- Epoch: 2  Batch: 23 --\n",
      "    Loss_i: 0.22452673 Loss_f: 0.22617127 rho -0.004754536 lambda: 0.01\n",
      "    Loss_i: 0.22452673 Loss_f: 0.22857279 rho -0.0013572515 lambda: 0.099999994\n",
      "    Loss_i: 0.22452673 Loss_f: 0.2264057 rho -6.405669e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.22452673 Loss_f: 0.22447406 rho 1.7987217e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22447406 Loss_f: 0.22476192 rho -9.828184e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22447406 Loss_f: 0.22476192 rho -9.828183e-07 lambda: 10.0\n",
      "    Train error: 40.594936\n",
      "-- Epoch: 2  Batch: 24 --\n",
      "    Loss_i: 0.22456542 Loss_f: 0.22503805 rho -0.0013168207 lambda: 0.01\n",
      "    Loss_i: 0.22456542 Loss_f: 0.22759058 rho -0.0009768385 lambda: 0.099999994\n",
      "    Loss_i: 0.22456542 Loss_f: 0.22588393 rho -4.326324e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.22456542 Loss_f: 0.22452694 rho 1.2644821e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22452694 Loss_f: 0.22471353 rho -6.128982e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22452694 Loss_f: 0.22471353 rho -6.1289813e-07 lambda: 10.0\n",
      "    Train error: 40.8818\n",
      "-- Epoch: 2  Batch: 25 --\n",
      "    Loss_i: 0.22700661 Loss_f: 0.22431411 rho 0.007302902 lambda: 0.01\n",
      "    Loss_i: 0.22431411 Loss_f: 0.22663349 rho -0.005662146 lambda: 0.01\n",
      "    Loss_i: 0.22431411 Loss_f: 0.22868785 rho -0.0011954907 lambda: 0.099999994\n",
      "    Loss_i: 0.22431411 Loss_f: 0.22717208 rho -7.906389e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.22431411 Loss_f: 0.22488016 rho -1.5678414e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22431411 Loss_f: 0.22488016 rho -1.5678414e-06 lambda: 10.0\n",
      "    Train error: 41.16654\n",
      "-- Epoch: 2  Batch: 26 --\n",
      "    Loss_i: 0.2195458 Loss_f: 0.30687746 rho -0.31007612 lambda: 0.01\n",
      "    Loss_i: 0.2195458 Loss_f: 0.30276105 rho -0.03140806 lambda: 0.099999994\n",
      "    Loss_i: 0.2195458 Loss_f: 0.2723027 rho -0.0020038409 lambda: 0.99999994\n",
      "    Loss_i: 0.2195458 Loss_f: 0.21941388 rho 5.013837e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21941388 Loss_f: 0.23034467 rho -4.140707e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21941388 Loss_f: 0.23034467 rho -4.140706e-05 lambda: 10.0\n",
      "    Train error: 39.67013\n",
      "-- Epoch: 2  Batch: 27 --\n",
      "    Loss_i: 0.21755591 Loss_f: 0.2799211 rho -0.16820857 lambda: 0.01\n",
      "    Loss_i: 0.21755591 Loss_f: 0.27425358 rho -0.01866712 lambda: 0.099999994\n",
      "    Loss_i: 0.21755591 Loss_f: 0.25154218 rho -0.0011442144 lambda: 0.99999994\n",
      "    Loss_i: 0.21755591 Loss_f: 0.21696027 rho 2.0098887e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21696027 Loss_f: 0.22397101 rho -2.3669016e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21696027 Loss_f: 0.22397101 rho -2.3669016e-05 lambda: 10.0\n",
      "    Train error: 39.122707\n",
      "-- Epoch: 2  Batch: 28 --\n",
      "    Loss_i: 0.21808012 Loss_f: 0.2885527 rho -0.21977916 lambda: 0.01\n",
      "    Loss_i: 0.21808012 Loss_f: 0.28072056 rho -0.019655757 lambda: 0.099999994\n",
      "    Loss_i: 0.21808012 Loss_f: 0.2507505 rho -0.0010257866 lambda: 0.99999994\n",
      "    Loss_i: 0.21808012 Loss_f: 0.21785311 rho 7.127929e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21785311 Loss_f: 0.22358379 rho -1.7964783e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21785311 Loss_f: 0.22358379 rho -1.7964781e-05 lambda: 10.0\n",
      "    Train error: 39.263515\n",
      "-- Epoch: 2  Batch: 29 --\n",
      "    Loss_i: 0.21902351 Loss_f: 0.2606264 rho -0.116443686 lambda: 0.01\n",
      "    Loss_i: 0.21902351 Loss_f: 0.25694606 rho -0.012385609 lambda: 0.099999994\n",
      "    Loss_i: 0.21902351 Loss_f: 0.23858602 rho -0.0006497607 lambda: 0.99999994\n",
      "    Loss_i: 0.21902351 Loss_f: 0.21874307 rho 9.330533e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21874307 Loss_f: 0.22220655 rho -1.1501017e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21874307 Loss_f: 0.22220655 rho -1.1501017e-05 lambda: 10.0\n",
      "    Train error: 39.44483\n",
      "-- Epoch: 2  Batch: 30 --\n",
      "    Loss_i: 0.22216944 Loss_f: 0.24682021 rho -0.07266985 lambda: 0.01\n",
      "    Loss_i: 0.22216944 Loss_f: 0.24477464 rho -0.00735645 lambda: 0.099999994\n",
      "    Loss_i: 0.22216944 Loss_f: 0.2318077 rho -0.00031695282 lambda: 0.99999994\n",
      "    Loss_i: 0.22216944 Loss_f: 0.22191362 rho 8.421558e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22191362 Loss_f: 0.22334912 rho -4.712446e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22191362 Loss_f: 0.22334912 rho -4.712445e-06 lambda: 10.0\n",
      "    Train error: 40.00218\n",
      "\n",
      "*** Epoch: 2 Train error: 41.69679260253906  Test error: 39.522507  Time: 690.8359988 sec\n",
      "\n",
      "-- Epoch: 3  Batch: 1 --\n",
      "    Loss_i: 0.21762444 Loss_f: 0.22128217 rho -0.008069443 lambda: 0.01\n",
      "    Loss_i: 0.21762444 Loss_f: 0.22317182 rho -0.001397237 lambda: 0.099999994\n",
      "    Loss_i: 0.21762444 Loss_f: 0.220125 rho -6.388787e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.21762444 Loss_f: 0.21756202 rho 1.5971133e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21756202 Loss_f: 0.21794139 rho -9.700341e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21756202 Loss_f: 0.21794139 rho -9.700341e-07 lambda: 10.0\n",
      "    Train error: 39.199646\n",
      "-- Epoch: 3  Batch: 2 --\n",
      "    Loss_i: 0.21851808 Loss_f: 0.21731764 rho 0.0027301852 lambda: 0.01\n",
      "    Loss_i: 0.21731764 Loss_f: 0.22273645 rho -0.010093447 lambda: 0.01\n",
      "    Loss_i: 0.21731764 Loss_f: 0.22423193 rho -0.001402175 lambda: 0.099999994\n",
      "    Loss_i: 0.21731764 Loss_f: 0.22157623 rho -8.7134715e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.21731764 Loss_f: 0.21813804 rho -1.6801165e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21731764 Loss_f: 0.21813804 rho -1.6801164e-06 lambda: 10.0\n",
      "    Train error: 38.806087\n",
      "-- Epoch: 3  Batch: 3 --\n",
      "    Loss_i: 0.22205529 Loss_f: 0.30514356 rho -0.14878866 lambda: 0.01\n",
      "    Loss_i: 0.22205529 Loss_f: 0.30215704 rho -0.029368548 lambda: 0.099999994\n",
      "    Loss_i: 0.22205529 Loss_f: 0.2761531 rho -0.0022155042 lambda: 0.99999994\n",
      "    Loss_i: 0.22205529 Loss_f: 0.22145894 rho 2.4711628e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22145894 Loss_f: 0.23406796 rho -5.1963758e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22145894 Loss_f: 0.23406796 rho -5.196375e-05 lambda: 10.0\n",
      "    Train error: 40.3553\n",
      "-- Epoch: 3  Batch: 4 --\n",
      "    Loss_i: 0.21395224 Loss_f: 0.3121601 rho -0.36351806 lambda: 0.01\n",
      "    Loss_i: 0.21395224 Loss_f: 0.3009642 rho -0.030951679 lambda: 0.099999994\n",
      "    Loss_i: 0.21395224 Loss_f: 0.2641475 rho -0.001778596 lambda: 0.99999994\n",
      "    Loss_i: 0.21395224 Loss_f: 0.2132733 rho 2.404797e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2132733 Loss_f: 0.2223481 rho -3.2120977e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2132733 Loss_f: 0.2223481 rho -3.2120974e-05 lambda: 10.0\n",
      "    Train error: 38.07595\n",
      "-- Epoch: 3  Batch: 5 --\n",
      "    Loss_i: 0.21865736 Loss_f: 0.27778307 rho -0.13767259 lambda: 0.01\n",
      "    Loss_i: 0.21865736 Loss_f: 0.27145997 rho -0.016929284 lambda: 0.099999994\n",
      "    Loss_i: 0.21865736 Loss_f: 0.24651791 rho -0.00092823786 lambda: 0.99999994\n",
      "    Loss_i: 0.21865736 Loss_f: 0.21830682 rho 1.17248e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21830682 Loss_f: 0.22330846 rho -1.6645627e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21830682 Loss_f: 0.22330847 rho -1.6645674e-05 lambda: 10.0\n",
      "    Train error: 39.624638\n",
      "-- Epoch: 3  Batch: 6 --\n",
      "    Loss_i: 0.21361648 Loss_f: 0.2629445 rho -0.11401793 lambda: 0.01\n",
      "    Loss_i: 0.21361648 Loss_f: 0.25532016 rho -0.010954511 lambda: 0.099999994\n",
      "    Loss_i: 0.21361648 Loss_f: 0.23186213 rho -0.0004858958 lambda: 0.99999994\n",
      "    Loss_i: 0.21361648 Loss_f: 0.213287 rho 8.786452e-07 lambda: 9.999999\n",
      "    Loss_i: 0.213287 Loss_f: 0.21602371 rho -7.2756834e-06 lambda: 9.999999\n",
      "    Loss_i: 0.213287 Loss_f: 0.21602371 rho -7.2756825e-06 lambda: 10.0\n",
      "    Train error: 38.431522\n",
      "-- Epoch: 3  Batch: 7 --\n",
      "    Loss_i: 0.22309275 Loss_f: 0.23743917 rho -0.03788369 lambda: 0.01\n",
      "    Loss_i: 0.22309275 Loss_f: 0.23780501 rho -0.004234673 lambda: 0.099999994\n",
      "    Loss_i: 0.22309275 Loss_f: 0.22954994 rho -0.00018754746 lambda: 0.99999994\n",
      "    Loss_i: 0.22309275 Loss_f: 0.22294544 rho 4.2825536e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22294544 Loss_f: 0.22394079 rho -2.890556e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22294544 Loss_f: 0.22394079 rho -2.8905558e-06 lambda: 10.0\n",
      "    Train error: 40.582386\n",
      "-- Epoch: 3  Batch: 8 --\n",
      "    Loss_i: 0.22089876 Loss_f: 0.22389415 rho -0.0075490847 lambda: 0.01\n",
      "    Loss_i: 0.22089876 Loss_f: 0.22572571 rho -0.0012518854 lambda: 0.099999994\n",
      "    Loss_i: 0.22089876 Loss_f: 0.22296488 rho -5.374184e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.22089876 Loss_f: 0.22084291 rho 1.4531263e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22084291 Loss_f: 0.22114255 rho -7.794832e-07 lambda: 9.999999\n",
      "    Loss_i: 0.22084291 Loss_f: 0.22114255 rho -7.794831e-07 lambda: 10.0\n",
      "    Train error: 39.94897\n",
      "-- Epoch: 3  Batch: 9 --\n",
      "    Loss_i: 0.2153517 Loss_f: 0.2157967 rho -0.00091831095 lambda: 0.01\n",
      "    Loss_i: 0.2153517 Loss_f: 0.21785566 rho -0.000570111 lambda: 0.099999994\n",
      "    Loss_i: 0.2153517 Loss_f: 0.21651241 rho -2.6703365e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.2153517 Loss_f: 0.21533309 rho 4.2862546e-08 lambda: 9.999999\n",
      "    Loss_i: 0.21533309 Loss_f: 0.21551184 rho -4.1144023e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21533309 Loss_f: 0.21551184 rho -4.1144017e-07 lambda: 10.0\n",
      "    Train error: 38.6501\n",
      "-- Epoch: 3  Batch: 10 --\n",
      "    Loss_i: 0.21799311 Loss_f: 0.21783674 rho 0.00032815977 lambda: 0.01\n",
      "    Loss_i: 0.21783674 Loss_f: 0.22738805 rho -0.01601515 lambda: 0.01\n",
      "    Loss_i: 0.21783674 Loss_f: 0.22821294 rho -0.002036386 lambda: 0.099999994\n",
      "    Loss_i: 0.21783674 Loss_f: 0.22427808 rho -0.0001286069 lambda: 0.99999994\n",
      "    Loss_i: 0.21783674 Loss_f: 0.21910842 rho -2.5434294e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21783674 Loss_f: 0.21910842 rho -2.5434292e-06 lambda: 10.0\n",
      "    Train error: 39.20106\n",
      "-- Epoch: 3  Batch: 11 --\n",
      "    Loss_i: 0.21824642 Loss_f: 0.30713686 rho -0.15196325 lambda: 0.01\n",
      "    Loss_i: 0.21824642 Loss_f: 0.306587 rho -0.031324867 lambda: 0.099999994\n",
      "    Loss_i: 0.21824642 Loss_f: 0.28616634 rho -0.0026982233 lambda: 0.99999994\n",
      "    Loss_i: 0.21824642 Loss_f: 0.21745029 rho 3.2012508e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21745029 Loss_f: 0.23665334 rho -7.749649e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21745029 Loss_f: 0.23665334 rho -7.749648e-05 lambda: 10.0\n",
      "    Train error: 39.655907\n",
      "-- Epoch: 3  Batch: 12 --\n",
      "    Loss_i: 0.22270301 Loss_f: 0.3332896 rho -0.3524541 lambda: 0.01\n",
      "    Loss_i: 0.22270301 Loss_f: 0.32566074 rho -0.032949723 lambda: 0.099999994\n",
      "    Loss_i: 0.22270301 Loss_f: 0.2928309 rho -0.0022452425 lambda: 0.99999994\n",
      "    Loss_i: 0.22270301 Loss_f: 0.22185126 rho 2.72711e-06 lambda: 9.999999\n",
      "    Loss_i: 0.22185126 Loss_f: 0.23794527 rho -5.1389376e-05 lambda: 9.999999\n",
      "    Loss_i: 0.22185126 Loss_f: 0.23794527 rho -5.1389365e-05 lambda: 10.0\n",
      "    Train error: 40.07454\n",
      "-- Epoch: 3  Batch: 13 --\n",
      "    Loss_i: 0.21130426 Loss_f: 0.28800386 rho -0.15573151 lambda: 0.01\n",
      "    Loss_i: 0.21130426 Loss_f: 0.28186405 rho -0.019345704 lambda: 0.099999994\n",
      "    Loss_i: 0.21130426 Loss_f: 0.2566899 rho -0.0012895378 lambda: 0.99999994\n",
      "    Loss_i: 0.21130426 Loss_f: 0.21040629 rho 2.560701e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21040629 Loss_f: 0.22032544 rho -2.8356255e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21040629 Loss_f: 0.22032544 rho -2.8356255e-05 lambda: 10.0\n",
      "    Train error: 37.988605\n",
      "-- Epoch: 3  Batch: 14 --\n",
      "    Loss_i: 0.21347924 Loss_f: 0.2981736 rho -0.22426534 lambda: 0.01\n",
      "    Loss_i: 0.21347924 Loss_f: 0.288508 rho -0.02180134 lambda: 0.099999994\n",
      "    Loss_i: 0.21347924 Loss_f: 0.25121933 rho -0.0011074068 lambda: 0.99999994\n",
      "    Loss_i: 0.21347924 Loss_f: 0.21291988 rho 1.6429435e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21291988 Loss_f: 0.21927615 rho -1.862218e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21291988 Loss_f: 0.21927615 rho -1.862218e-05 lambda: 10.0\n",
      "    Train error: 37.72182\n",
      "-- Epoch: 3  Batch: 15 --\n",
      "    Loss_i: 0.2144349 Loss_f: 0.26033562 rho -0.073116034 lambda: 0.01\n",
      "    Loss_i: 0.2144349 Loss_f: 0.25444034 rho -0.00931897 lambda: 0.099999994\n",
      "    Loss_i: 0.2144349 Loss_f: 0.2330927 rho -0.000455689 lambda: 0.99999994\n",
      "    Loss_i: 0.2144349 Loss_f: 0.2141476 rho 7.051288e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2141476 Loss_f: 0.217181 rho -7.4227687e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2141476 Loss_f: 0.217181 rho -7.4227687e-06 lambda: 10.0\n",
      "    Train error: 38.428024\n",
      "-- Epoch: 3  Batch: 16 --\n",
      "    Loss_i: 0.21185564 Loss_f: 0.24009861 rho -0.06203009 lambda: 0.01\n",
      "    Loss_i: 0.21185564 Loss_f: 0.23674496 rho -0.006382369 lambda: 0.099999994\n",
      "    Loss_i: 0.21185564 Loss_f: 0.22223563 rho -0.00027071027 lambda: 0.99999994\n",
      "    Loss_i: 0.21185564 Loss_f: 0.2116647 rho 4.987913e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2116647 Loss_f: 0.2131957 rho -3.9953584e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2116647 Loss_f: 0.2131957 rho -3.995358e-06 lambda: 10.0\n",
      "    Train error: 37.46616\n",
      "-- Epoch: 3  Batch: 17 --\n",
      "    Loss_i: 0.2129299 Loss_f: 0.22296377 rho -0.019395543 lambda: 0.01\n",
      "    Loss_i: 0.2129299 Loss_f: 0.2235688 rho -0.0024125217 lambda: 0.099999994\n",
      "    Loss_i: 0.2129299 Loss_f: 0.21741287 rho -0.000103448576 lambda: 0.99999994\n",
      "    Loss_i: 0.2129299 Loss_f: 0.21281748 rho 2.598985e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21281748 Loss_f: 0.21348259 rho -1.5351435e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21281748 Loss_f: 0.21348259 rho -1.5351434e-06 lambda: 10.0\n",
      "    Train error: 37.964413\n",
      "-- Epoch: 3  Batch: 18 --\n",
      "    Loss_i: 0.214432 Loss_f: 0.2177291 rho -0.0057312353 lambda: 0.01\n",
      "    Loss_i: 0.214432 Loss_f: 0.21930684 rho -0.0010091588 lambda: 0.099999994\n",
      "    Loss_i: 0.214432 Loss_f: 0.2165264 rho -4.4200937e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.214432 Loss_f: 0.21437061 rho 1.2981793e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21437061 Loss_f: 0.21467763 rho -6.489784e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21437061 Loss_f: 0.21467763 rho -6.4897836e-07 lambda: 10.0\n",
      "    Train error: 38.161266\n",
      "-- Epoch: 3  Batch: 19 --\n",
      "    Loss_i: 0.2178713 Loss_f: 0.21609297 rho 0.0031433261 lambda: 0.01\n",
      "    Loss_i: 0.21609297 Loss_f: 0.21964987 rho -0.005869113 lambda: 0.01\n",
      "    Loss_i: 0.21609297 Loss_f: 0.2211946 rho -0.0009507814 lambda: 0.099999994\n",
      "    Loss_i: 0.21609297 Loss_f: 0.21930198 rho -6.059025e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.21609297 Loss_f: 0.21671766 rho -1.1810379e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21609297 Loss_f: 0.21671766 rho -1.1810375e-06 lambda: 10.0\n",
      "    Train error: 39.11929\n",
      "-- Epoch: 3  Batch: 20 --\n",
      "    Loss_i: 0.21145591 Loss_f: 0.30101728 rho -0.20607999 lambda: 0.01\n",
      "    Loss_i: 0.21145591 Loss_f: 0.2953845 rho -0.022779055 lambda: 0.099999994\n",
      "    Loss_i: 0.21145591 Loss_f: 0.2593215 rho -0.001322871 lambda: 0.99999994\n",
      "    Loss_i: 0.21145591 Loss_f: 0.21088295 rho 1.5864134e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21088295 Loss_f: 0.21994872 rho -2.5037152e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21088295 Loss_f: 0.21994872 rho -2.503715e-05 lambda: 10.0\n",
      "    Train error: 37.188515\n",
      "-- Epoch: 3  Batch: 21 --\n",
      "    Loss_i: 0.21398711 Loss_f: 0.27230638 rho -0.09427693 lambda: 0.01\n",
      "    Loss_i: 0.21398711 Loss_f: 0.2666644 rho -0.011820093 lambda: 0.099999994\n",
      "    Loss_i: 0.21398711 Loss_f: 0.24241523 rho -0.00066364236 lambda: 0.99999994\n",
      "    Loss_i: 0.21398711 Loss_f: 0.2135781 rho 9.5868e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2135781 Loss_f: 0.21892741 rho -1.2555884e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2135781 Loss_f: 0.2189274 rho -1.2555845e-05 lambda: 10.0\n",
      "    Train error: 38.15916\n",
      "-- Epoch: 3  Batch: 22 --\n",
      "    Loss_i: 0.20949455 Loss_f: 0.25899068 rho -0.11835753 lambda: 0.01\n",
      "    Loss_i: 0.20949455 Loss_f: 0.2532463 rho -0.010893306 lambda: 0.099999994\n",
      "    Loss_i: 0.20949455 Loss_f: 0.22916855 rho -0.0004918698 lambda: 0.99999994\n",
      "    Loss_i: 0.20949455 Loss_f: 0.20920236 rho 7.3078644e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20920236 Loss_f: 0.21224344 rho -7.581578e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20920236 Loss_f: 0.21224344 rho -7.581577e-06 lambda: 10.0\n",
      "    Train error: 36.892174\n",
      "-- Epoch: 3  Batch: 23 --\n",
      "    Loss_i: 0.21337919 Loss_f: 0.24014194 rho -0.041385666 lambda: 0.01\n",
      "    Loss_i: 0.21337919 Loss_f: 0.23772058 rho -0.005576004 lambda: 0.099999994\n",
      "    Loss_i: 0.21337919 Loss_f: 0.22454676 rho -0.00026875787 lambda: 0.99999994\n",
      "    Loss_i: 0.21337919 Loss_f: 0.21324924 rho 3.1433356e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21324924 Loss_f: 0.21507645 rho -4.4145536e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21324924 Loss_f: 0.21507645 rho -4.4145527e-06 lambda: 10.0\n",
      "    Train error: 37.598873\n",
      "-- Epoch: 3  Batch: 24 --\n",
      "    Loss_i: 0.2129511 Loss_f: 0.23388878 rho -0.04254434 lambda: 0.01\n",
      "    Loss_i: 0.2129511 Loss_f: 0.23223828 rho -0.00452513 lambda: 0.099999994\n",
      "    Loss_i: 0.2129511 Loss_f: 0.22098522 rho -0.00019145627 lambda: 0.99999994\n",
      "    Loss_i: 0.2129511 Loss_f: 0.21285774 rho 2.2282036e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21285774 Loss_f: 0.21405026 rho -2.842073e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21285774 Loss_f: 0.21405026 rho -2.8420727e-06 lambda: 10.0\n",
      "    Train error: 37.623165\n",
      "-- Epoch: 3  Batch: 25 --\n",
      "    Loss_i: 0.21480063 Loss_f: 0.22399195 rho -0.017882992 lambda: 0.01\n",
      "    Loss_i: 0.21480063 Loss_f: 0.22475246 rho -0.0022861422 lambda: 0.099999994\n",
      "    Loss_i: 0.21480063 Loss_f: 0.2190931 rho -0.00010042166 lambda: 0.99999994\n",
      "    Loss_i: 0.21480063 Loss_f: 0.21474949 rho 1.1986346e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21474949 Loss_f: 0.21540809 rho -1.5434564e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21474949 Loss_f: 0.21540809 rho -1.5434563e-06 lambda: 10.0\n",
      "    Train error: 38.309418\n",
      "-- Epoch: 3  Batch: 26 --\n",
      "    Loss_i: 0.20972411 Loss_f: 0.21541543 rho -0.008201056 lambda: 0.01\n",
      "    Loss_i: 0.20972411 Loss_f: 0.21653795 rho -0.0011865186 lambda: 0.099999994\n",
      "    Loss_i: 0.20972411 Loss_f: 0.2126087 rho -5.129958e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20972411 Loss_f: 0.2096895 rho 6.16916e-08 lambda: 9.999999\n",
      "    Loss_i: 0.2096895 Loss_f: 0.21011545 rho -7.5853893e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2096895 Loss_f: 0.21011545 rho -7.5853876e-07 lambda: 10.0\n",
      "    Train error: 37.108112\n",
      "-- Epoch: 3  Batch: 27 --\n",
      "    Loss_i: 0.20844814 Loss_f: 0.20958804 rho -0.001897453 lambda: 0.01\n",
      "    Loss_i: 0.20844814 Loss_f: 0.21155407 rho -0.0005678305 lambda: 0.099999994\n",
      "    Loss_i: 0.20844814 Loss_f: 0.2099497 rho -2.7724322e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20844814 Loss_f: 0.20840643 rho 7.7085495e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20840643 Loss_f: 0.20863967 rho -4.3108003e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20840643 Loss_f: 0.20863968 rho -4.3110757e-07 lambda: 10.0\n",
      "    Train error: 36.635944\n",
      "-- Epoch: 3  Batch: 28 --\n",
      "    Loss_i: 0.20859805 Loss_f: 0.20873249 rho -0.00019667581 lambda: 0.01\n",
      "    Loss_i: 0.20859805 Loss_f: 0.21078531 rho -0.0003512231 lambda: 0.099999994\n",
      "    Loss_i: 0.20859805 Loss_f: 0.20961396 rho -1.6474029e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20859805 Loss_f: 0.20857558 rho 3.647468e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20857558 Loss_f: 0.20872891 rho -2.4880063e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20857558 Loss_f: 0.20872891 rho -2.4880063e-07 lambda: 10.0\n",
      "    Train error: 36.65186\n",
      "-- Epoch: 3  Batch: 29 --\n",
      "    Loss_i: 0.210451 Loss_f: 0.20803969 rho 0.004046571 lambda: 0.01\n",
      "    Loss_i: 0.20803969 Loss_f: 0.20979257 rho -0.002775482 lambda: 0.01\n",
      "    Loss_i: 0.20803969 Loss_f: 0.21148843 rho -0.00057315675 lambda: 0.099999994\n",
      "    Loss_i: 0.20803969 Loss_f: 0.21029922 rho -3.773918e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20803969 Loss_f: 0.20848541 rho -7.448267e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20803969 Loss_f: 0.20848541 rho -7.448266e-07 lambda: 10.0\n",
      "    Train error: 36.787132\n",
      "-- Epoch: 3  Batch: 30 --\n",
      "    Loss_i: 0.21205871 Loss_f: 0.2881199 rho -0.14810188 lambda: 0.01\n",
      "    Loss_i: 0.21205871 Loss_f: 0.27834278 rho -0.015285064 lambda: 0.099999994\n",
      "    Loss_i: 0.21205871 Loss_f: 0.24459979 rho -0.00076448487 lambda: 0.99999994\n",
      "    Loss_i: 0.21205871 Loss_f: 0.21154815 rho 1.2017066e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21154815 Loss_f: 0.21702491 rho -1.2854481e-05 lambda: 9.999999\n",
      "    Loss_i: 0.21154815 Loss_f: 0.2170249 rho -1.2854444e-05 lambda: 10.0\n",
      "    Train error: 37.17175\n",
      "\n",
      "*** Epoch: 3 Train error: 38.31939303080241  Test error: 36.70466  Time: 686.6743017000001 sec\n",
      "\n",
      "-- Epoch: 4  Batch: 1 --\n",
      "    Loss_i: 0.20725282 Loss_f: 0.24433431 rho -0.053799454 lambda: 0.01\n",
      "    Loss_i: 0.20725282 Loss_f: 0.23988962 rho -0.006654351 lambda: 0.099999994\n",
      "    Loss_i: 0.20725282 Loss_f: 0.22225402 rho -0.00031878243 lambda: 0.99999994\n",
      "    Loss_i: 0.20725282 Loss_f: 0.20703995 rho 4.542627e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20703995 Loss_f: 0.20947212 rho -5.1839957e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20703995 Loss_f: 0.20947212 rho -5.1839947e-06 lambda: 10.0\n",
      "    Train error: 36.480057\n",
      "-- Epoch: 4  Batch: 2 --\n",
      "    Loss_i: 0.20786951 Loss_f: 0.22445555 rho -0.02739844 lambda: 0.01\n",
      "    Loss_i: 0.20786951 Loss_f: 0.22349295 rho -0.0029736052 lambda: 0.099999994\n",
      "    Loss_i: 0.20786951 Loss_f: 0.21428141 rho -0.0001239234 lambda: 0.99999994\n",
      "    Loss_i: 0.20786951 Loss_f: 0.20774151 rho 2.4777177e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20774151 Loss_f: 0.20867026 rho -1.7955768e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20774151 Loss_f: 0.20867026 rho -1.7955768e-06 lambda: 10.0\n",
      "    Train error: 36.41918\n",
      "-- Epoch: 4  Batch: 3 --\n",
      "    Loss_i: 0.21179251 Loss_f: 0.21471795 rho -0.004544906 lambda: 0.01\n",
      "    Loss_i: 0.21179251 Loss_f: 0.21629824 rho -0.0007731626 lambda: 0.099999994\n",
      "    Loss_i: 0.21179251 Loss_f: 0.21377182 rho -3.432278e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.21179251 Loss_f: 0.21174805 rho 7.718754e-08 lambda: 9.999999\n",
      "    Loss_i: 0.21174805 Loss_f: 0.21204618 rho -5.174918e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21174805 Loss_f: 0.21204618 rho -5.1749174e-07 lambda: 10.0\n",
      "    Train error: 37.524445\n",
      "-- Epoch: 4  Batch: 4 --\n",
      "    Loss_i: 0.20469041 Loss_f: 0.20498641 rho -0.00042737467 lambda: 0.01\n",
      "    Loss_i: 0.20469041 Loss_f: 0.20679209 rho -0.00032207198 lambda: 0.099999994\n",
      "    Loss_i: 0.20469041 Loss_f: 0.20564151 rho -1.4665107e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20469041 Loss_f: 0.20466825 rho 3.418694e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20466825 Loss_f: 0.20480861 rho -2.1648434e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20466825 Loss_f: 0.20480861 rho -2.1648432e-07 lambda: 10.0\n",
      "    Train error: 35.73333\n",
      "-- Epoch: 4  Batch: 5 --\n",
      "    Loss_i: 0.20977432 Loss_f: 0.20897046 rho 0.0011930118 lambda: 0.01\n",
      "    Loss_i: 0.20897046 Loss_f: 0.21382698 rho -0.006879144 lambda: 0.01\n",
      "    Loss_i: 0.20897046 Loss_f: 0.2150755 rho -0.00088741013 lambda: 0.099999994\n",
      "    Loss_i: 0.20897046 Loss_f: 0.2128318 rho -5.6274646e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20897046 Loss_f: 0.20973693 rho -1.1173385e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20897046 Loss_f: 0.20973693 rho -1.1173385e-06 lambda: 10.0\n",
      "    Train error: 37.25651\n",
      "-- Epoch: 4  Batch: 6 --\n",
      "    Loss_i: 0.2045087 Loss_f: 0.32113922 rho -0.219131 lambda: 0.01\n",
      "    Loss_i: 0.2045087 Loss_f: 0.3103331 rho -0.024489302 lambda: 0.099999994\n",
      "    Loss_i: 0.2045087 Loss_f: 0.26676434 rho -0.0014748549 lambda: 0.99999994\n",
      "    Loss_i: 0.2045087 Loss_f: 0.20371528 rho 1.8841222e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20371528 Loss_f: 0.21573958 rho -2.8270497e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20371528 Loss_f: 0.2157396 rho -2.8270528e-05 lambda: 10.0\n",
      "    Train error: 35.663525\n",
      "-- Epoch: 4  Batch: 7 --\n",
      "    Loss_i: 0.21373178 Loss_f: 0.28818944 rho -0.11659628 lambda: 0.01\n",
      "    Loss_i: 0.21373178 Loss_f: 0.28217202 rho -0.014883765 lambda: 0.099999994\n",
      "    Loss_i: 0.21373178 Loss_f: 0.2511631 rho -0.00084694807 lambda: 0.99999994\n",
      "    Loss_i: 0.21373178 Loss_f: 0.2132546 rho 1.0840865e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2132546 Loss_f: 0.2201464 rho -1.5639574e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2132546 Loss_f: 0.22014639 rho -1.5639538e-05 lambda: 10.0\n",
      "    Train error: 38.012424\n",
      "-- Epoch: 4  Batch: 8 --\n",
      "    Loss_i: 0.21108657 Loss_f: 0.2767253 rho -0.12982355 lambda: 0.01\n",
      "    Loss_i: 0.21108657 Loss_f: 0.26695257 rho -0.011470421 lambda: 0.099999994\n",
      "    Loss_i: 0.21108657 Loss_f: 0.2352706 rho -0.00049844594 lambda: 0.99999994\n",
      "    Loss_i: 0.21108657 Loss_f: 0.21068567 rho 8.2659415e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21068567 Loss_f: 0.21425007 rho -7.329845e-06 lambda: 9.999999\n",
      "    Loss_i: 0.21068567 Loss_f: 0.21425007 rho -7.329845e-06 lambda: 10.0\n",
      "    Train error: 37.16644\n",
      "-- Epoch: 4  Batch: 9 --\n",
      "    Loss_i: 0.20570317 Loss_f: 0.23807462 rho -0.04973738 lambda: 0.01\n",
      "    Loss_i: 0.20570317 Loss_f: 0.23460282 rho -0.0049589784 lambda: 0.099999994\n",
      "    Loss_i: 0.20570317 Loss_f: 0.21869662 rho -0.0002255938 lambda: 0.99999994\n",
      "    Loss_i: 0.20570317 Loss_f: 0.20558755 rho 2.0097502e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20558755 Loss_f: 0.20764266 rho -3.5702617e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20558755 Loss_f: 0.20764266 rho -3.5702608e-06 lambda: 10.0\n",
      "    Train error: 36.07933\n",
      "-- Epoch: 4  Batch: 10 --\n",
      "    Loss_i: 0.20731519 Loss_f: 0.23377292 rho -0.04044392 lambda: 0.01\n",
      "    Loss_i: 0.20731519 Loss_f: 0.23103607 rho -0.0045216694 lambda: 0.099999994\n",
      "    Loss_i: 0.20731519 Loss_f: 0.21684913 rho -0.00018633847 lambda: 0.99999994\n",
      "    Loss_i: 0.20731519 Loss_f: 0.20719928 rho 2.2713033e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20719928 Loss_f: 0.2085588 rho -2.6581667e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20719928 Loss_f: 0.2085588 rho -2.6581665e-06 lambda: 10.0\n",
      "    Train error: 36.478703\n",
      "-- Epoch: 4  Batch: 11 --\n",
      "    Loss_i: 0.20712201 Loss_f: 0.21967806 rho -0.019016352 lambda: 0.01\n",
      "    Loss_i: 0.20712201 Loss_f: 0.21973948 rho -0.0021257421 lambda: 0.099999994\n",
      "    Loss_i: 0.20712201 Loss_f: 0.21244557 rho -9.070895e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20712201 Loss_f: 0.20705214 rho 1.19190695e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20705214 Loss_f: 0.20785017 rho -1.3608084e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20705214 Loss_f: 0.20785017 rho -1.360808e-06 lambda: 10.0\n",
      "    Train error: 36.53815\n",
      "-- Epoch: 4  Batch: 12 --\n",
      "    Loss_i: 0.21207158 Loss_f: 0.21854444 rho -0.00885546 lambda: 0.01\n",
      "    Loss_i: 0.21207158 Loss_f: 0.21938735 rho -0.0012003148 lambda: 0.099999994\n",
      "    Loss_i: 0.21207158 Loss_f: 0.2151075 rho -5.0823804e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.21207158 Loss_f: 0.21203455 rho 6.2116584e-08 lambda: 9.999999\n",
      "    Loss_i: 0.21203455 Loss_f: 0.21248123 rho -7.4830075e-07 lambda: 9.999999\n",
      "    Loss_i: 0.21203455 Loss_f: 0.21248123 rho -7.4830075e-07 lambda: 10.0\n",
      "    Train error: 37.52841\n",
      "-- Epoch: 4  Batch: 13 --\n",
      "    Loss_i: 0.20190227 Loss_f: 0.20610654 rho -0.0059118955 lambda: 0.01\n",
      "    Loss_i: 0.20190227 Loss_f: 0.2073974 rho -0.0008635268 lambda: 0.099999994\n",
      "    Loss_i: 0.20190227 Loss_f: 0.2043114 rho -3.830827e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20190227 Loss_f: 0.2018729 rho 4.6757997e-08 lambda: 9.999999\n",
      "    Loss_i: 0.2018729 Loss_f: 0.20224276 rho -5.8852044e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2018729 Loss_f: 0.20224276 rho -5.8852044e-07 lambda: 10.0\n",
      "    Train error: 35.466938\n",
      "-- Epoch: 4  Batch: 14 --\n",
      "    Loss_i: 0.20497967 Loss_f: 0.20807295 rho -0.004098676 lambda: 0.01\n",
      "    Loss_i: 0.20497967 Loss_f: 0.20950018 rho -0.00071851164 lambda: 0.099999994\n",
      "    Loss_i: 0.20497967 Loss_f: 0.20694844 rho -3.1929805e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20497967 Loss_f: 0.20493826 rho 6.729687e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20493826 Loss_f: 0.2052318 rho -4.766776e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20493826 Loss_f: 0.20523179 rho -4.7665336e-07 lambda: 10.0\n",
      "    Train error: 35.681076\n",
      "-- Epoch: 4  Batch: 15 --\n",
      "    Loss_i: 0.2061656 Loss_f: 0.20579018 rho 0.0004951398 lambda: 0.01\n",
      "    Loss_i: 0.20579018 Loss_f: 0.21172808 rho -0.0075610615 lambda: 0.01\n",
      "    Loss_i: 0.20579018 Loss_f: 0.21282075 rho -0.00092060486 lambda: 0.099999994\n",
      "    Loss_i: 0.20579018 Loss_f: 0.21015158 rho -5.727181e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20579018 Loss_f: 0.20665549 rho -1.1366058e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20579018 Loss_f: 0.20665549 rho -1.1366056e-06 lambda: 10.0\n",
      "    Train error: 36.372437\n",
      "-- Epoch: 4  Batch: 16 --\n",
      "    Loss_i: 0.20479812 Loss_f: 0.32295114 rho -0.17655313 lambda: 0.01\n",
      "    Loss_i: 0.20479812 Loss_f: 0.32646635 rho -0.028948452 lambda: 0.099999994\n",
      "    Loss_i: 0.20479812 Loss_f: 0.28601024 rho -0.0020539227 lambda: 0.99999994\n",
      "    Loss_i: 0.20479812 Loss_f: 0.20447339 rho 8.264627e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20447339 Loss_f: 0.22162046 rho -4.3593445e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20447339 Loss_f: 0.22162046 rho -4.3593445e-05 lambda: 10.0\n",
      "    Train error: 35.39557\n",
      "-- Epoch: 4  Batch: 17 --\n",
      "    Loss_i: 0.20486304 Loss_f: 0.29400614 rho -0.12982456 lambda: 0.01\n",
      "    Loss_i: 0.20486304 Loss_f: 0.28866455 rho -0.015394704 lambda: 0.099999994\n",
      "    Loss_i: 0.20486304 Loss_f: 0.2589848 rho -0.0010209269 lambda: 0.99999994\n",
      "    Loss_i: 0.20486304 Loss_f: 0.20422441 rho 1.2079304e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20422441 Loss_f: 0.21617658 rho -2.2562881e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20422441 Loss_f: 0.21617658 rho -2.256288e-05 lambda: 10.0\n",
      "    Train error: 35.846016\n",
      "-- Epoch: 4  Batch: 18 --\n",
      "    Loss_i: 0.20599706 Loss_f: 0.30642998 rho -0.14661354 lambda: 0.01\n",
      "    Loss_i: 0.20599706 Loss_f: 0.29781422 rho -0.017325541 lambda: 0.099999994\n",
      "    Loss_i: 0.20599706 Loss_f: 0.25364816 rho -0.00092626025 lambda: 0.99999994\n",
      "    Loss_i: 0.20599706 Loss_f: 0.2053251 rho 1.3101462e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2053251 Loss_f: 0.21348733 rho -1.5901716e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2053251 Loss_f: 0.21348733 rho -1.5901716e-05 lambda: 10.0\n",
      "    Train error: 35.58039\n",
      "-- Epoch: 4  Batch: 19 --\n",
      "    Loss_i: 0.20833646 Loss_f: 0.26648018 rho -0.072946236 lambda: 0.01\n",
      "    Loss_i: 0.20833646 Loss_f: 0.2604346 rho -0.008715979 lambda: 0.099999994\n",
      "    Loss_i: 0.20833646 Loss_f: 0.23428155 rho -0.00044903508 lambda: 0.99999994\n",
      "    Loss_i: 0.20833646 Loss_f: 0.20808513 rho 4.3647378e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20808513 Loss_f: 0.21252196 rho -7.719036e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20808513 Loss_f: 0.21252196 rho -7.719034e-06 lambda: 10.0\n",
      "    Train error: 36.840244\n",
      "-- Epoch: 4  Batch: 20 --\n",
      "    Loss_i: 0.20312595 Loss_f: 0.24411274 rho -0.058258634 lambda: 0.01\n",
      "    Loss_i: 0.20312595 Loss_f: 0.23885077 rho -0.006526413 lambda: 0.099999994\n",
      "    Loss_i: 0.20312595 Loss_f: 0.218078 rho -0.00028117295 lambda: 0.99999994\n",
      "    Loss_i: 0.20312595 Loss_f: 0.20286854 rho 4.8549884e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20286854 Loss_f: 0.2050193 rho -4.046573e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20286854 Loss_f: 0.2050193 rho -4.046573e-06 lambda: 10.0\n",
      "    Train error: 35.15367\n",
      "-- Epoch: 4  Batch: 21 --\n",
      "    Loss_i: 0.20567709 Loss_f: 0.22209017 rho -0.022401042 lambda: 0.01\n",
      "    Loss_i: 0.20567709 Loss_f: 0.22160923 rho -0.0025843803 lambda: 0.099999994\n",
      "    Loss_i: 0.20567709 Loss_f: 0.21270011 rho -0.00011611044 lambda: 0.99999994\n",
      "    Loss_i: 0.20567709 Loss_f: 0.20558996 rho 1.4432305e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20558996 Loss_f: 0.20669405 rho -1.8271104e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20558996 Loss_f: 0.20669408 rho -1.8271595e-06 lambda: 10.0\n",
      "    Train error: 35.97379\n",
      "-- Epoch: 4  Batch: 22 --\n",
      "    Loss_i: 0.20172997 Loss_f: 0.20946652 rho -0.01051814 lambda: 0.01\n",
      "    Loss_i: 0.20172997 Loss_f: 0.21015055 rho -0.0012905407 lambda: 0.099999994\n",
      "    Loss_i: 0.20172997 Loss_f: 0.20518517 rho -5.363731e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20172997 Loss_f: 0.2016649 rho 1.0114783e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2016649 Loss_f: 0.20216635 rho -7.7827633e-07 lambda: 9.999999\n",
      "    Loss_i: 0.2016649 Loss_f: 0.20216635 rho -7.7827633e-07 lambda: 10.0\n",
      "    Train error: 34.95519\n",
      "-- Epoch: 4  Batch: 23 --\n",
      "    Loss_i: 0.20593594 Loss_f: 0.2093154 rho -0.004616945 lambda: 0.01\n",
      "    Loss_i: 0.20593594 Loss_f: 0.21064222 rho -0.00074842246 lambda: 0.099999994\n",
      "    Loss_i: 0.20593594 Loss_f: 0.20797947 rho -3.303947e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20593594 Loss_f: 0.20590366 rho 5.227035e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20590366 Loss_f: 0.20620729 rho -4.914587e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20590366 Loss_f: 0.20620728 rho -4.914345e-07 lambda: 10.0\n",
      "    Train error: 35.621048\n",
      "-- Epoch: 4  Batch: 24 --\n",
      "    Loss_i: 0.20566545 Loss_f: 0.20703304 rho -0.0017447069 lambda: 0.01\n",
      "    Loss_i: 0.20566545 Loss_f: 0.2084666 rho -0.00038620253 lambda: 0.099999994\n",
      "    Loss_i: 0.20566545 Loss_f: 0.2068576 rho -1.6570284e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20566545 Loss_f: 0.20563999 rho 3.542531e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20563999 Loss_f: 0.2058126 rho -2.3991745e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20563999 Loss_f: 0.2058126 rho -2.3991743e-07 lambda: 10.0\n",
      "    Train error: 35.74868\n",
      "-- Epoch: 4  Batch: 25 --\n",
      "    Loss_i: 0.20763257 Loss_f: 0.20704493 rho 0.0006416261 lambda: 0.01\n",
      "    Loss_i: 0.20704493 Loss_f: 0.21060401 rho -0.003854962 lambda: 0.01\n",
      "    Loss_i: 0.20704493 Loss_f: 0.21168521 rho -0.0005356501 lambda: 0.099999994\n",
      "    Loss_i: 0.20704493 Loss_f: 0.20995049 rho -3.376229e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20704493 Loss_f: 0.20761432 rho -6.620603e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20704493 Loss_f: 0.20761432 rho -6.620602e-07 lambda: 10.0\n",
      "    Train error: 36.523758\n",
      "-- Epoch: 4  Batch: 26 --\n",
      "    Loss_i: 0.20249166 Loss_f: 0.31207457 rho -0.15138635 lambda: 0.01\n",
      "    Loss_i: 0.20249166 Loss_f: 0.30571288 rho -0.0190258 lambda: 0.099999994\n",
      "    Loss_i: 0.20249166 Loss_f: 0.2601163 rho -0.0010988684 lambda: 0.99999994\n",
      "    Loss_i: 0.20249166 Loss_f: 0.20217909 rho 5.981145e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20217909 Loss_f: 0.21297318 rho -2.0485602e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20217909 Loss_f: 0.21297315 rho -2.0485546e-05 lambda: 10.0\n",
      "    Train error: 34.938015\n",
      "-- Epoch: 4  Batch: 27 --\n",
      "    Loss_i: 0.20085599 Loss_f: 0.2749866 rho -0.06782988 lambda: 0.01\n",
      "    Loss_i: 0.20085599 Loss_f: 0.26812547 rho -0.01016372 lambda: 0.099999994\n",
      "    Loss_i: 0.20085599 Loss_f: 0.23741573 rho -0.0005908591 lambda: 0.99999994\n",
      "    Loss_i: 0.20085599 Loss_f: 0.20035985 rho 8.074503e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20035985 Loss_f: 0.20715593 rho -1.1038537e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20035985 Loss_f: 0.20715596 rho -1.10385845e-05 lambda: 10.0\n",
      "    Train error: 34.69987\n",
      "-- Epoch: 4  Batch: 28 --\n",
      "    Loss_i: 0.20044973 Loss_f: 0.27698576 rho -0.12222204 lambda: 0.01\n",
      "    Loss_i: 0.20044973 Loss_f: 0.26688218 rho -0.011217354 lambda: 0.099999994\n",
      "    Loss_i: 0.20044973 Loss_f: 0.23118955 rho -0.0005220476 lambda: 0.99999994\n",
      "    Loss_i: 0.20044973 Loss_f: 0.20018992 rho 4.414956e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20018992 Loss_f: 0.20505469 rho -8.245072e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20018992 Loss_f: 0.20505469 rho -8.2450715e-06 lambda: 10.0\n",
      "    Train error: 34.43328\n",
      "-- Epoch: 4  Batch: 29 --\n",
      "    Loss_i: 0.20179768 Loss_f: 0.24287854 rho -0.05246474 lambda: 0.01\n",
      "    Loss_i: 0.20179768 Loss_f: 0.23878248 rho -0.005532216 lambda: 0.099999994\n",
      "    Loss_i: 0.20179768 Loss_f: 0.21938875 rho -0.00026771304 lambda: 0.99999994\n",
      "    Loss_i: 0.20179768 Loss_f: 0.20160082 rho 3.0011685e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20160082 Loss_f: 0.20454147 rho -4.4847798e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20160082 Loss_f: 0.20454149 rho -4.484802e-06 lambda: 10.0\n",
      "    Train error: 34.894882\n",
      "-- Epoch: 4  Batch: 30 --\n",
      "    Loss_i: 0.2049674 Loss_f: 0.2274205 rho -0.025699707 lambda: 0.01\n",
      "    Loss_i: 0.2049674 Loss_f: 0.22548488 rho -0.0032152254 lambda: 0.099999994\n",
      "    Loss_i: 0.2049674 Loss_f: 0.21343869 rho -0.00013783845 lambda: 0.99999994\n",
      "    Loss_i: 0.2049674 Loss_f: 0.20481357 rho 2.5125462e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20481357 Loss_f: 0.20606223 rho -2.037518e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20481357 Loss_f: 0.20606223 rho -2.037518e-06 lambda: 10.0\n",
      "    Train error: 35.391953\n",
      "\n",
      "*** Epoch: 4 Train error: 36.0132438659668  Test error: 34.77287  Time: 708.6892518999998 sec\n",
      "\n",
      "-- Epoch: 5  Batch: 1 --\n",
      "    Loss_i: 0.20032464 Loss_f: 0.20580891 rho -0.0060014753 lambda: 0.01\n",
      "    Loss_i: 0.20032464 Loss_f: 0.2066909 rho -0.0008194543 lambda: 0.099999994\n",
      "    Loss_i: 0.20032464 Loss_f: 0.2030092 rho -3.5175264e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20032464 Loss_f: 0.20028158 rho 5.6527742e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20028158 Loss_f: 0.20067564 rho -5.169288e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20028158 Loss_f: 0.20067564 rho -5.1692876e-07 lambda: 10.0\n",
      "    Train error: 34.52316\n",
      "-- Epoch: 5  Batch: 2 --\n",
      "    Loss_i: 0.2014693 Loss_f: 0.20106597 rho 0.00041381214 lambda: 0.01\n",
      "    Loss_i: 0.20106597 Loss_f: 0.20614155 rho -0.0045750695 lambda: 0.01\n",
      "    Loss_i: 0.20106597 Loss_f: 0.20707127 rho -0.00065342506 lambda: 0.099999994\n",
      "    Loss_i: 0.20106597 Loss_f: 0.20477991 rho -4.1265255e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20106597 Loss_f: 0.201787 rho -8.0282047e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20106597 Loss_f: 0.201787 rho -8.028204e-07 lambda: 10.0\n",
      "    Train error: 34.395042\n",
      "-- Epoch: 5  Batch: 3 --\n",
      "    Loss_i: 0.20503072 Loss_f: 0.3038835 rho -0.15636536 lambda: 0.01\n",
      "    Loss_i: 0.20503072 Loss_f: 0.2955681 rho -0.018288659 lambda: 0.099999994\n",
      "    Loss_i: 0.20503072 Loss_f: 0.25561264 rho -0.0010508733 lambda: 0.99999994\n",
      "    Loss_i: 0.20503072 Loss_f: 0.2042645 rho 1.5964177e-06 lambda: 9.999999\n",
      "    Loss_i: 0.2042645 Loss_f: 0.21368758 rho -1.9630757e-05 lambda: 9.999999\n",
      "    Loss_i: 0.2042645 Loss_f: 0.21368758 rho -1.9630757e-05 lambda: 10.0\n",
      "    Train error: 35.654648\n",
      "-- Epoch: 5  Batch: 4 --\n",
      "    Loss_i: 0.19767801 Loss_f: 0.28649125 rho -0.1348223 lambda: 0.01\n",
      "    Loss_i: 0.19767801 Loss_f: 0.2752719 rho -0.012102483 lambda: 0.099999994\n",
      "    Loss_i: 0.19767801 Loss_f: 0.23480375 rho -0.00058065215 lambda: 0.99999994\n",
      "    Loss_i: 0.19767801 Loss_f: 0.19715485 rho 8.1846315e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19715485 Loss_f: 0.2031295 rho -9.324686e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19715485 Loss_f: 0.20312953 rho -9.324731e-06 lambda: 10.0\n",
      "    Train error: 33.71985\n",
      "-- Epoch: 5  Batch: 5 --\n",
      "    Loss_i: 0.20194812 Loss_f: 0.25528538 rho -0.06539294 lambda: 0.01\n",
      "    Loss_i: 0.20194812 Loss_f: 0.24906234 rho -0.0067964527 lambda: 0.099999994\n",
      "    Loss_i: 0.20194812 Loss_f: 0.2238813 rho -0.0003220849 lambda: 0.99999994\n",
      "    Loss_i: 0.20194812 Loss_f: 0.20169678 rho 3.6974984e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20169678 Loss_f: 0.20520225 rho -5.153622e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20169678 Loss_f: 0.20520227 rho -5.153644e-06 lambda: 10.0\n",
      "    Train error: 35.130733\n",
      "-- Epoch: 5  Batch: 6 --\n",
      "    Loss_i: 0.19693366 Loss_f: 0.23244147 rho -0.04273765 lambda: 0.01\n",
      "    Loss_i: 0.19693366 Loss_f: 0.2279699 rho -0.0040803878 lambda: 0.099999994\n",
      "    Loss_i: 0.19693366 Loss_f: 0.20999269 rho -0.00017328908 lambda: 0.99999994\n",
      "    Loss_i: 0.19693366 Loss_f: 0.19671096 rho 2.957883e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19671096 Loss_f: 0.19864342 rho -2.5622244e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19671096 Loss_f: 0.19864342 rho -2.5622242e-06 lambda: 10.0\n",
      "    Train error: 33.904243\n",
      "-- Epoch: 5  Batch: 7 --\n",
      "    Loss_i: 0.20623577 Loss_f: 0.21779133 rho -0.014321954 lambda: 0.01\n",
      "    Loss_i: 0.20623577 Loss_f: 0.2179786 rho -0.0016754124 lambda: 0.099999994\n",
      "    Loss_i: 0.20623577 Loss_f: 0.21116534 rho -7.141236e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20623577 Loss_f: 0.20612384 rho 1.6238624e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20612384 Loss_f: 0.20685159 rho -1.0553956e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20612384 Loss_f: 0.2068516 rho -1.0554171e-06 lambda: 10.0\n",
      "    Train error: 35.981552\n",
      "-- Epoch: 5  Batch: 8 --\n",
      "    Loss_i: 0.20471504 Loss_f: 0.20776762 rho -0.003442841 lambda: 0.01\n",
      "    Loss_i: 0.20471504 Loss_f: 0.20898442 rho -0.0005777247 lambda: 0.099999994\n",
      "    Loss_i: 0.20471504 Loss_f: 0.2064994 rho -2.4637824e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20471504 Loss_f: 0.20466179 rho 7.368554e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20466179 Loss_f: 0.20491706 rho -3.5304004e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20466179 Loss_f: 0.20491706 rho -3.5303998e-07 lambda: 10.0\n",
      "    Train error: 35.622524\n",
      "-- Epoch: 5  Batch: 9 --\n",
      "    Loss_i: 0.1998251 Loss_f: 0.2006413 rho -0.0008090708 lambda: 0.01\n",
      "    Loss_i: 0.1998251 Loss_f: 0.20194618 rho -0.000248958 lambda: 0.099999994\n",
      "    Loss_i: 0.1998251 Loss_f: 0.20071329 rho -1.0620526e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1998251 Loss_f: 0.19980396 rho 2.5313222e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19980396 Loss_f: 0.19992997 rho -1.5091248e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19980396 Loss_f: 0.19992997 rho -1.5091244e-07 lambda: 10.0\n",
      "    Train error: 34.475327\n",
      "-- Epoch: 5  Batch: 10 --\n",
      "    Loss_i: 0.2015262 Loss_f: 0.20092973 rho 0.0006654474 lambda: 0.01\n",
      "    Loss_i: 0.20092973 Loss_f: 0.20612809 rho -0.005044485 lambda: 0.01\n",
      "    Loss_i: 0.20092973 Loss_f: 0.2069891 rho -0.00070009206 lambda: 0.099999994\n",
      "    Loss_i: 0.20092973 Loss_f: 0.20462173 rho -4.348596e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20092973 Loss_f: 0.20163877 rho -8.367657e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20092973 Loss_f: 0.20163877 rho -8.367656e-07 lambda: 10.0\n",
      "    Train error: 34.586647\n",
      "-- Epoch: 5  Batch: 11 --\n",
      "    Loss_i: 0.2011691 Loss_f: 0.30026808 rho -0.08801636 lambda: 0.01\n",
      "    Loss_i: 0.2011691 Loss_f: 0.2932433 rho -0.013457798 lambda: 0.099999994\n",
      "    Loss_i: 0.2011691 Loss_f: 0.25568715 rho -0.0008518513 lambda: 0.99999994\n",
      "    Loss_i: 0.2011691 Loss_f: 0.20043707 rho 1.1517628e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20043707 Loss_f: 0.21129285 rho -1.7079507e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20043707 Loss_f: 0.21129285 rho -1.7079505e-05 lambda: 10.0\n",
      "    Train error: 34.950798\n",
      "-- Epoch: 5  Batch: 12 --\n",
      "    Loss_i: 0.20556433 Loss_f: 0.3055004 rho -0.10244744 lambda: 0.01\n",
      "    Loss_i: 0.20556433 Loss_f: 0.29248103 rho -0.012303672 lambda: 0.099999994\n",
      "    Loss_i: 0.20556433 Loss_f: 0.24861163 rho -0.0006334923 lambda: 0.99999994\n",
      "    Loss_i: 0.20556433 Loss_f: 0.20512037 rho 6.559451e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20512037 Loss_f: 0.21229951 rho -1.058201e-05 lambda: 9.999999\n",
      "    Loss_i: 0.20512037 Loss_f: 0.21229951 rho -1.058201e-05 lambda: 10.0\n",
      "    Train error: 35.54018\n",
      "-- Epoch: 5  Batch: 13 --\n",
      "    Loss_i: 0.19498295 Loss_f: 0.25567648 rho -0.068783894 lambda: 0.01\n",
      "    Loss_i: 0.19498295 Loss_f: 0.24875909 rho -0.007161695 lambda: 0.099999994\n",
      "    Loss_i: 0.19498295 Loss_f: 0.2214167 rho -0.00035830901 lambda: 0.99999994\n",
      "    Loss_i: 0.19498295 Loss_f: 0.19464245 rho 4.6235968e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19464245 Loss_f: 0.19907561 rho -6.022101e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19464245 Loss_f: 0.19907561 rho -6.022101e-06 lambda: 10.0\n",
      "    Train error: 33.628036\n",
      "-- Epoch: 5  Batch: 14 --\n",
      "    Loss_i: 0.19804706 Loss_f: 0.2536367 rho -0.059994366 lambda: 0.01\n",
      "    Loss_i: 0.19804706 Loss_f: 0.24646683 rho -0.006905218 lambda: 0.099999994\n",
      "    Loss_i: 0.19804706 Loss_f: 0.21912791 rho -0.00031062076 lambda: 0.99999994\n",
      "    Loss_i: 0.19804706 Loss_f: 0.19781147 rho 3.482885e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19781147 Loss_f: 0.2010163 rho -4.7313392e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19781147 Loss_f: 0.20101628 rho -4.7312947e-06 lambda: 10.0\n",
      "    Train error: 33.700016\n",
      "-- Epoch: 5  Batch: 15 --\n",
      "    Loss_i: 0.19846086 Loss_f: 0.22756134 rho -0.0289681 lambda: 0.01\n",
      "    Loss_i: 0.19846086 Loss_f: 0.22420791 rho -0.003288223 lambda: 0.099999994\n",
      "    Loss_i: 0.19846086 Loss_f: 0.20917754 rho -0.00014085119 lambda: 0.99999994\n",
      "    Loss_i: 0.19846086 Loss_f: 0.19828838 rho 2.2735672e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19828838 Loss_f: 0.19985208 rho -2.0611465e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19828838 Loss_f: 0.19985208 rho -2.0611465e-06 lambda: 10.0\n",
      "    Train error: 34.066017\n",
      "-- Epoch: 5  Batch: 16 --\n",
      "    Loss_i: 0.19725794 Loss_f: 0.21291845 rho -0.01992725 lambda: 0.01\n",
      "    Loss_i: 0.19725794 Loss_f: 0.21198708 rho -0.0020415103 lambda: 0.099999994\n",
      "    Loss_i: 0.19725794 Loss_f: 0.20323011 rho -8.352201e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19725794 Loss_f: 0.19717462 rho 1.16618935e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19717462 Loss_f: 0.19803508 rho -1.2027223e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19717462 Loss_f: 0.19803508 rho -1.2027222e-06 lambda: 10.0\n",
      "    Train error: 33.623993\n",
      "-- Epoch: 5  Batch: 17 --\n",
      "    Loss_i: 0.19742113 Loss_f: 0.20586444 rho -0.008997474 lambda: 0.01\n",
      "    Loss_i: 0.19742113 Loss_f: 0.20632586 rho -0.0010844263 lambda: 0.099999994\n",
      "    Loss_i: 0.19742113 Loss_f: 0.20109028 rho -4.533051e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19742113 Loss_f: 0.19736467 rho 6.985546e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19736467 Loss_f: 0.19790047 rho -6.6246884e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19736467 Loss_f: 0.19790047 rho -6.624688e-07 lambda: 10.0\n",
      "    Train error: 33.761345\n",
      "-- Epoch: 5  Batch: 18 --\n",
      "    Loss_i: 0.19924662 Loss_f: 0.20270902 rho -0.0037603956 lambda: 0.01\n",
      "    Loss_i: 0.19924662 Loss_f: 0.20378244 rho -0.00054161233 lambda: 0.099999994\n",
      "    Loss_i: 0.19924662 Loss_f: 0.20111744 rho -2.2563505e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19924662 Loss_f: 0.19920114 rho 5.49053e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19920114 Loss_f: 0.19946805 rho -3.2211932e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19920114 Loss_f: 0.19946805 rho -3.221193e-07 lambda: 10.0\n",
      "    Train error: 34.047913\n",
      "-- Epoch: 5  Batch: 19 --\n",
      "    Loss_i: 0.2021088 Loss_f: 0.20118752 rho 0.0008899421 lambda: 0.01\n",
      "    Loss_i: 0.20118752 Loss_f: 0.20422934 rho -0.0028636716 lambda: 0.01\n",
      "    Loss_i: 0.20118752 Loss_f: 0.20531304 rho -0.0004381385 lambda: 0.099999994\n",
      "    Loss_i: 0.20118752 Loss_f: 0.20375945 rho -2.7668772e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20118752 Loss_f: 0.20169008 rho -5.4135273e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20118752 Loss_f: 0.20169008 rho -5.413527e-07 lambda: 10.0\n",
      "    Train error: 34.99769\n",
      "-- Epoch: 5  Batch: 20 --\n",
      "    Loss_i: 0.19692487 Loss_f: 0.28951892 rho -0.117666356 lambda: 0.01\n",
      "    Loss_i: 0.19692487 Loss_f: 0.28101534 rho -0.011436019 lambda: 0.099999994\n",
      "    Loss_i: 0.19692487 Loss_f: 0.24097033 rho -0.0006032372 lambda: 0.99999994\n",
      "    Loss_i: 0.19692487 Loss_f: 0.19662656 rho 4.0884305e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19662656 Loss_f: 0.2045119 rho -1.0799765e-05 lambda: 9.999999\n",
      "    Loss_i: 0.19662656 Loss_f: 0.2045119 rho -1.0799761e-05 lambda: 10.0\n",
      "    Train error: 33.324436\n",
      "-- Epoch: 5  Batch: 21 --\n",
      "    Loss_i: 0.19969174 Loss_f: 0.26510292 rho -0.05655398 lambda: 0.01\n",
      "    Loss_i: 0.19969174 Loss_f: 0.25858492 rho -0.008176397 lambda: 0.099999994\n",
      "    Loss_i: 0.19969174 Loss_f: 0.22960801 rho -0.00044212348 lambda: 0.99999994\n",
      "    Loss_i: 0.19969174 Loss_f: 0.19932155 rho 5.506428e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19932155 Loss_f: 0.20465942 rho -7.940827e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19932155 Loss_f: 0.2046594 rho -7.940805e-06 lambda: 10.0\n",
      "    Train error: 34.360172\n",
      "-- Epoch: 5  Batch: 22 --\n",
      "    Loss_i: 0.19521026 Loss_f: 0.25175357 rho -0.06173037 lambda: 0.01\n",
      "    Loss_i: 0.19521026 Loss_f: 0.24524072 rho -0.006662875 lambda: 0.099999994\n",
      "    Loss_i: 0.19521026 Loss_f: 0.21731693 rho -0.0003010269 lambda: 0.99999994\n",
      "    Loss_i: 0.19521026 Loss_f: 0.1950179 rho 2.6252593e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1950179 Loss_f: 0.19836524 rho -4.5628226e-06 lambda: 9.999999\n",
      "    Loss_i: 0.1950179 Loss_f: 0.19836524 rho -4.562822e-06 lambda: 10.0\n",
      "    Train error: 33.038124\n",
      "-- Epoch: 5  Batch: 23 --\n",
      "    Loss_i: 0.19973326 Loss_f: 0.23292805 rho -0.037874226 lambda: 0.01\n",
      "    Loss_i: 0.19973326 Loss_f: 0.22966288 rho -0.0042653815 lambda: 0.099999994\n",
      "    Loss_i: 0.19973326 Loss_f: 0.21305019 rho -0.00019463197 lambda: 0.99999994\n",
      "    Loss_i: 0.19973326 Loss_f: 0.19956215 rho 2.5072424e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19956215 Loss_f: 0.2016533 rho -3.0635836e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19956215 Loss_f: 0.2016533 rho -3.063583e-06 lambda: 10.0\n",
      "    Train error: 33.975483\n",
      "-- Epoch: 5  Batch: 24 --\n",
      "    Loss_i: 0.19913824 Loss_f: 0.21866938 rho -0.021251708 lambda: 0.01\n",
      "    Loss_i: 0.19913824 Loss_f: 0.21700445 rho -0.0025097895 lambda: 0.099999994\n",
      "    Loss_i: 0.19913824 Loss_f: 0.20646848 rho -0.00010605964 lambda: 0.99999994\n",
      "    Loss_i: 0.19913824 Loss_f: 0.19900255 rho 1.9691718e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19900255 Loss_f: 0.20006345 rho -1.5373022e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19900255 Loss_f: 0.20006345 rho -1.5373021e-06 lambda: 10.0\n",
      "    Train error: 33.896896\n",
      "-- Epoch: 5  Batch: 25 --\n",
      "    Loss_i: 0.20062931 Loss_f: 0.20761834 rho -0.0072446307 lambda: 0.01\n",
      "    Loss_i: 0.20062931 Loss_f: 0.20803049 rho -0.0008967701 lambda: 0.099999994\n",
      "    Loss_i: 0.20062931 Loss_f: 0.20369267 rho -3.775524e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.20062931 Loss_f: 0.20058088 rho 5.9790004e-08 lambda: 9.999999\n",
      "    Loss_i: 0.20058088 Loss_f: 0.20103313 rho -5.581058e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20058088 Loss_f: 0.20103313 rho -5.5810574e-07 lambda: 10.0\n",
      "    Train error: 34.47692\n",
      "-- Epoch: 5  Batch: 26 --\n",
      "    Loss_i: 0.19577089 Loss_f: 0.19841368 rho -0.0028390172 lambda: 0.01\n",
      "    Loss_i: 0.19577089 Loss_f: 0.19974646 rho -0.00048044242 lambda: 0.099999994\n",
      "    Loss_i: 0.19577089 Loss_f: 0.19745408 rho -2.059854e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19577089 Loss_f: 0.19574645 rho 2.9944516e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19574645 Loss_f: 0.19599102 rho -2.9938357e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19574645 Loss_f: 0.19599102 rho -2.9938357e-07 lambda: 10.0\n",
      "    Train error: 33.32615\n",
      "-- Epoch: 5  Batch: 27 --\n",
      "    Loss_i: 0.1947492 Loss_f: 0.19463405 rho 0.000105588566 lambda: 0.01\n",
      "    Loss_i: 0.19463405 Loss_f: 0.20084012 rho -0.005684048 lambda: 0.01\n",
      "    Loss_i: 0.19463405 Loss_f: 0.20137608 rho -0.00067485217 lambda: 0.099999994\n",
      "    Loss_i: 0.19463405 Loss_f: 0.1987763 rho -4.185117e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19463405 Loss_f: 0.19544627 rho -8.213922e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19463405 Loss_f: 0.19544627 rho -8.21392e-07 lambda: 10.0\n",
      "    Train error: 33.38109\n",
      "-- Epoch: 5  Batch: 28 --\n",
      "    Loss_i: 0.19529249 Loss_f: 0.33245307 rho -0.11422544 lambda: 0.01\n",
      "    Loss_i: 0.19529249 Loss_f: 0.3232063 rho -0.018147817 lambda: 0.099999994\n",
      "    Loss_i: 0.19529249 Loss_f: 0.2724364 rho -0.0011773208 lambda: 0.99999994\n",
      "    Loss_i: 0.19529249 Loss_f: 0.1949039 rho 5.975678e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1949039 Loss_f: 0.2103922 rho -2.3699982e-05 lambda: 9.999999\n",
      "    Loss_i: 0.1949039 Loss_f: 0.21039218 rho -2.3699935e-05 lambda: 10.0\n",
      "    Train error: 32.983616\n",
      "-- Epoch: 5  Batch: 29 --\n",
      "    Loss_i: 0.19652088 Loss_f: 0.28957552 rho -0.096316695 lambda: 0.01\n",
      "    Loss_i: 0.19652088 Loss_f: 0.28455257 rho -0.011991767 lambda: 0.099999994\n",
      "    Loss_i: 0.19652088 Loss_f: 0.24841219 rho -0.0007299404 lambda: 0.99999994\n",
      "    Loss_i: 0.19652088 Loss_f: 0.19584872 rho 9.486065e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19584872 Loss_f: 0.20619771 rho -1.4591264e-05 lambda: 9.999999\n",
      "    Loss_i: 0.19584872 Loss_f: 0.20619771 rho -1.4591259e-05 lambda: 10.0\n",
      "    Train error: 33.570637\n",
      "-- Epoch: 5  Batch: 30 --\n",
      "    Loss_i: 0.1986064 Loss_f: 0.28913203 rho -0.08788899 lambda: 0.01\n",
      "    Loss_i: 0.1986064 Loss_f: 0.27574542 rho -0.010608582 lambda: 0.099999994\n",
      "    Loss_i: 0.1986064 Loss_f: 0.23474215 rho -0.00051855744 lambda: 0.99999994\n",
      "    Loss_i: 0.1986064 Loss_f: 0.198093 rho 7.3996506e-07 lambda: 9.999999\n",
      "    Loss_i: 0.198093 Loss_f: 0.20375252 rho -8.158398e-06 lambda: 9.999999\n",
      "    Loss_i: 0.198093 Loss_f: 0.20375252 rho -8.158397e-06 lambda: 10.0\n",
      "    Train error: 33.567593\n",
      "\n",
      "*** Epoch: 5 Train error: 34.20702756245931  Test error: 33.091293  Time: 702.9823000000001 sec\n",
      "\n",
      "-- Epoch: 6  Batch: 1 --\n",
      "    Loss_i: 0.19381838 Loss_f: 0.2340509 rho -0.03960847 lambda: 0.01\n",
      "    Loss_i: 0.19381838 Loss_f: 0.22954544 rho -0.0040928666 lambda: 0.099999994\n",
      "    Loss_i: 0.19381838 Loss_f: 0.20955594 rho -0.00018328762 lambda: 0.99999994\n",
      "    Loss_i: 0.19381838 Loss_f: 0.19358681 rho 2.701406e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19358681 Loss_f: 0.19599366 rho -2.8093689e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19358681 Loss_f: 0.19599368 rho -2.8093857e-06 lambda: 10.0\n",
      "    Train error: 32.850697\n",
      "-- Epoch: 6  Batch: 2 --\n",
      "    Loss_i: 0.19469358 Loss_f: 0.21395487 rho -0.018483253 lambda: 0.01\n",
      "    Loss_i: 0.19469358 Loss_f: 0.21232721 rho -0.0018474442 lambda: 0.099999994\n",
      "    Loss_i: 0.19469358 Loss_f: 0.20155342 rho -7.2535055e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19469358 Loss_f: 0.19458534 rho 1.14559874e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19458534 Loss_f: 0.19551854 rho -9.865363e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19458534 Loss_f: 0.19551854 rho -9.865362e-07 lambda: 10.0\n",
      "    Train error: 32.88226\n",
      "-- Epoch: 6  Batch: 3 --\n",
      "    Loss_i: 0.19814433 Loss_f: 0.20290336 rho -0.0040260972 lambda: 0.01\n",
      "    Loss_i: 0.19814433 Loss_f: 0.20351695 rho -0.00055726565 lambda: 0.099999994\n",
      "    Loss_i: 0.19814433 Loss_f: 0.20028657 rho -2.2733904e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19814433 Loss_f: 0.19808449 rho 6.3654085e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19808449 Loss_f: 0.19838022 rho -3.1450332e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19808449 Loss_f: 0.19838022 rho -3.1450327e-07 lambda: 10.0\n",
      "    Train error: 33.805035\n",
      "-- Epoch: 6  Batch: 4 --\n",
      "    Loss_i: 0.19163688 Loss_f: 0.19230404 rho -0.0005892127 lambda: 0.01\n",
      "    Loss_i: 0.19163688 Loss_f: 0.1935373 rho -0.00018819926 lambda: 0.099999994\n",
      "    Loss_i: 0.19163688 Loss_f: 0.19244663 rho -8.11756e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.19163688 Loss_f: 0.19160865 rho 2.8327204e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19160865 Loss_f: 0.19172254 rho -1.1426514e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19160865 Loss_f: 0.19172254 rho -1.14265134e-07 lambda: 10.0\n",
      "    Train error: 32.273\n",
      "-- Epoch: 6  Batch: 5 --\n",
      "    Loss_i: 0.19636053 Loss_f: 0.19617976 rho 0.00014324188 lambda: 0.01\n",
      "    Loss_i: 0.19617976 Loss_f: 0.2007801 rho -0.0036206816 lambda: 0.01\n",
      "    Loss_i: 0.19617976 Loss_f: 0.20144986 rho -0.0004621725 lambda: 0.099999994\n",
      "    Loss_i: 0.19617976 Loss_f: 0.1994532 rho -2.903898e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19617976 Loss_f: 0.19682401 rho -5.721823e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19617976 Loss_f: 0.19682401 rho -5.721822e-07 lambda: 10.0\n",
      "    Train error: 33.786903\n",
      "-- Epoch: 6  Batch: 6 --\n",
      "    Loss_i: 0.19156595 Loss_f: 0.31052345 rho -0.11428698 lambda: 0.01\n",
      "    Loss_i: 0.19156595 Loss_f: 0.2969821 rho -0.012696059 lambda: 0.099999994\n",
      "    Loss_i: 0.19156595 Loss_f: 0.24670434 rho -0.00068135146 lambda: 0.99999994\n",
      "    Loss_i: 0.19156595 Loss_f: 0.19082437 rho 9.1875836e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19082437 Loss_f: 0.20066042 rho -1.21461e-05 lambda: 9.999999\n",
      "    Loss_i: 0.19082437 Loss_f: 0.20066044 rho -1.2146118e-05 lambda: 10.0\n",
      "    Train error: 32.14567\n",
      "-- Epoch: 6  Batch: 7 --\n",
      "    Loss_i: 0.20049348 Loss_f: 0.27496144 rho -0.060133815 lambda: 0.01\n",
      "    Loss_i: 0.20049348 Loss_f: 0.2678149 rho -0.009261444 lambda: 0.099999994\n",
      "    Loss_i: 0.20049348 Loss_f: 0.23320344 rho -0.0004840521 lambda: 0.99999994\n",
      "    Loss_i: 0.20049348 Loss_f: 0.20006254 rho 6.4258467e-07 lambda: 9.999999\n",
      "    Loss_i: 0.20006254 Loss_f: 0.2054672 rho -8.050743e-06 lambda: 9.999999\n",
      "    Loss_i: 0.20006254 Loss_f: 0.2054672 rho -8.0507425e-06 lambda: 10.0\n",
      "    Train error: 34.390343\n",
      "-- Epoch: 6  Batch: 8 --\n",
      "    Loss_i: 0.19862947 Loss_f: 0.25372395 rho -0.06232768 lambda: 0.01\n",
      "    Loss_i: 0.19862947 Loss_f: 0.24726082 rho -0.0059764045 lambda: 0.099999994\n",
      "    Loss_i: 0.19862947 Loss_f: 0.22035113 rho -0.00026926576 lambda: 0.99999994\n",
      "    Loss_i: 0.19862947 Loss_f: 0.19844371 rho 2.3046957e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19844371 Loss_f: 0.20183705 rho -4.2122247e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19844371 Loss_f: 0.20183705 rho -4.212224e-06 lambda: 10.0\n",
      "    Train error: 33.857944\n",
      "-- Epoch: 6  Batch: 9 --\n",
      "    Loss_i: 0.19387288 Loss_f: 0.22770065 rho -0.029400028 lambda: 0.01\n",
      "    Loss_i: 0.19387288 Loss_f: 0.224452 rho -0.003537056 lambda: 0.099999994\n",
      "    Loss_i: 0.19387288 Loss_f: 0.20761196 rho -0.00016435693 lambda: 0.99999994\n",
      "    Loss_i: 0.19387288 Loss_f: 0.19371524 rho 1.8922738e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19371524 Loss_f: 0.19590443 rho -2.6263401e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19371524 Loss_f: 0.19590443 rho -2.62634e-06 lambda: 10.0\n",
      "    Train error: 32.89283\n",
      "-- Epoch: 6  Batch: 10 --\n",
      "    Loss_i: 0.1948408 Loss_f: 0.21689892 rho -0.0232405 lambda: 0.01\n",
      "    Loss_i: 0.1948408 Loss_f: 0.21470308 rho -0.0024102095 lambda: 0.099999994\n",
      "    Loss_i: 0.1948408 Loss_f: 0.202887 rho -9.914168e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1948408 Loss_f: 0.19469588 rho 1.7885023e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19469588 Loss_f: 0.19585681 rho -1.430532e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19469588 Loss_f: 0.19585681 rho -1.4305319e-06 lambda: 10.0\n",
      "    Train error: 33.072468\n",
      "-- Epoch: 6  Batch: 11 --\n",
      "    Loss_i: 0.19458255 Loss_f: 0.20431437 rho -0.008755329 lambda: 0.01\n",
      "    Loss_i: 0.19458255 Loss_f: 0.20434606 rho -0.0010203302 lambda: 0.099999994\n",
      "    Loss_i: 0.19458255 Loss_f: 0.1984963 rho -4.1572202e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19458255 Loss_f: 0.19452077 rho 6.573154e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19452077 Loss_f: 0.19507267 rho -5.868427e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19452077 Loss_f: 0.19507267 rho -5.8684265e-07 lambda: 10.0\n",
      "    Train error: 33.133686\n",
      "-- Epoch: 6  Batch: 12 --\n",
      "    Loss_i: 0.19959678 Loss_f: 0.2014841 rho -0.0017256376 lambda: 0.01\n",
      "    Loss_i: 0.19959678 Loss_f: 0.20259479 rho -0.00030938312 lambda: 0.099999994\n",
      "    Loss_i: 0.19959678 Loss_f: 0.20086624 rho -1.3271075e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19959678 Loss_f: 0.1995678 rho 3.0338438e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1995678 Loss_f: 0.19975227 rho -1.9301342e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1995678 Loss_f: 0.19975227 rho -1.9301339e-07 lambda: 10.0\n",
      "    Train error: 34.145306\n",
      "-- Epoch: 6  Batch: 13 --\n",
      "    Loss_i: 0.18968846 Loss_f: 0.18975379 rho -4.7500253e-05 lambda: 0.01\n",
      "    Loss_i: 0.18968846 Loss_f: 0.1910434 rho -0.000116049174 lambda: 0.099999994\n",
      "    Loss_i: 0.18968846 Loss_f: 0.19031134 rho -5.4315383e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18968846 Loss_f: 0.18967056 rho 1.5633871e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18967056 Loss_f: 0.18976156 rho -7.9495976e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18967056 Loss_f: 0.18976156 rho -7.949597e-08 lambda: 10.0\n",
      "    Train error: 32.133488\n",
      "-- Epoch: 6  Batch: 14 --\n",
      "    Loss_i: 0.19311897 Loss_f: 0.19334345 rho -0.00018421702 lambda: 0.01\n",
      "    Loss_i: 0.19311897 Loss_f: 0.19473727 rho -0.00015849333 lambda: 0.099999994\n",
      "    Loss_i: 0.19311897 Loss_f: 0.1938372 rho -7.172866e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.19311897 Loss_f: 0.19310383 rho 1.5149759e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19310383 Loss_f: 0.19320661 rho -1.0279327e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19310383 Loss_f: 0.19320661 rho -1.0279326e-07 lambda: 10.0\n",
      "    Train error: 32.403053\n",
      "-- Epoch: 6  Batch: 15 --\n",
      "    Loss_i: 0.19344118 Loss_f: 0.1924314 rho 0.0007535652 lambda: 0.01\n",
      "    Loss_i: 0.1924314 Loss_f: 0.19388126 rho -0.0010655512 lambda: 0.01\n",
      "    Loss_i: 0.1924314 Loss_f: 0.19494875 rho -0.0002016595 lambda: 0.099999994\n",
      "    Loss_i: 0.1924314 Loss_f: 0.19403045 rho -1.29259715e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1924314 Loss_f: 0.1927433 rho -2.5235252e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1924314 Loss_f: 0.1927433 rho -2.523525e-07 lambda: 10.0\n",
      "    Train error: 32.587383\n",
      "-- Epoch: 6  Batch: 16 --\n",
      "    Loss_i: 0.19238643 Loss_f: 0.27277043 rho -0.095756724 lambda: 0.01\n",
      "    Loss_i: 0.19238643 Loss_f: 0.2654177 rho -0.009411748 lambda: 0.099999994\n",
      "    Loss_i: 0.19238643 Loss_f: 0.22622564 rho -0.00043969377 lambda: 0.99999994\n",
      "    Loss_i: 0.19238643 Loss_f: 0.19210726 rho 3.6304658e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19210726 Loss_f: 0.19736543 rho -6.8240133e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19210726 Loss_f: 0.19736543 rho -6.8240133e-06 lambda: 10.0\n",
      "    Train error: 32.19549\n",
      "-- Epoch: 6  Batch: 17 --\n",
      "    Loss_i: 0.19178423 Loss_f: 0.23989892 rho -0.040295657 lambda: 0.01\n",
      "    Loss_i: 0.19178423 Loss_f: 0.23486866 rho -0.004304582 lambda: 0.099999994\n",
      "    Loss_i: 0.19178423 Loss_f: 0.21156007 rho -0.00020146904 lambda: 0.99999994\n",
      "    Loss_i: 0.19178423 Loss_f: 0.19153228 rho 2.5718222e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19153228 Loss_f: 0.19471389 rho -3.2481305e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19153228 Loss_f: 0.19471389 rho -3.24813e-06 lambda: 10.0\n",
      "    Train error: 32.264515\n",
      "-- Epoch: 6  Batch: 18 --\n",
      "    Loss_i: 0.19355221 Loss_f: 0.23075813 rho -0.0366622 lambda: 0.01\n",
      "    Loss_i: 0.19355221 Loss_f: 0.2266194 rho -0.0037935115 lambda: 0.099999994\n",
      "    Loss_i: 0.19355221 Loss_f: 0.20696473 rho -0.00015643938 lambda: 0.99999994\n",
      "    Loss_i: 0.19355221 Loss_f: 0.19338365 rho 1.9693412e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19338365 Loss_f: 0.19529606 rho -2.2322952e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19338365 Loss_f: 0.19529606 rho -2.2322952e-06 lambda: 10.0\n",
      "    Train error: 32.42567\n",
      "-- Epoch: 6  Batch: 19 --\n",
      "    Loss_i: 0.19594356 Loss_f: 0.211452 rho -0.013164965 lambda: 0.01\n",
      "    Loss_i: 0.19594356 Loss_f: 0.21032023 rho -0.0014508943 lambda: 0.099999994\n",
      "    Loss_i: 0.19594356 Loss_f: 0.20176677 rho -5.989906e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19594356 Loss_f: 0.19585487 rho 9.1406534e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19585487 Loss_f: 0.19668373 rho -8.536522e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19585487 Loss_f: 0.19668372 rho -8.536368e-07 lambda: 10.0\n",
      "    Train error: 33.38867\n",
      "-- Epoch: 6  Batch: 20 --\n",
      "    Loss_i: 0.19126953 Loss_f: 0.19534154 rho -0.003596829 lambda: 0.01\n",
      "    Loss_i: 0.19126953 Loss_f: 0.19619103 rho -0.0005292333 lambda: 0.099999994\n",
      "    Loss_i: 0.19126953 Loss_f: 0.19321358 rho -2.1369971e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19126953 Loss_f: 0.19122341 rho 5.080936e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19122341 Loss_f: 0.19148886 rho -2.9215303e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19122341 Loss_f: 0.19148888 rho -2.9216937e-07 lambda: 10.0\n",
      "    Train error: 32.057594\n",
      "-- Epoch: 6  Batch: 21 --\n",
      "    Loss_i: 0.19408147 Loss_f: 0.19415782 rho -5.849939e-05 lambda: 0.01\n",
      "    Loss_i: 0.19408147 Loss_f: 0.19562416 rho -0.00014339201 lambda: 0.099999994\n",
      "    Loss_i: 0.19408147 Loss_f: 0.19482541 rho -7.0655133e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.19408147 Loss_f: 0.19406033 rho 2.0125887e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19406033 Loss_f: 0.19417255 rho -1.0680192e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19406033 Loss_f: 0.19417255 rho -1.0680192e-07 lambda: 10.0\n",
      "    Train error: 32.84379\n",
      "-- Epoch: 6  Batch: 22 --\n",
      "    Loss_i: 0.19015047 Loss_f: 0.1893652 rho 0.00055181776 lambda: 0.01\n",
      "    Loss_i: 0.1893652 Loss_f: 0.19142076 rho -0.0013261702 lambda: 0.01\n",
      "    Loss_i: 0.1893652 Loss_f: 0.19234414 rho -0.00022328449 lambda: 0.099999994\n",
      "    Loss_i: 0.1893652 Loss_f: 0.19122611 rho -1.4177724e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1893652 Loss_f: 0.18972385 rho -2.7369853e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1893652 Loss_f: 0.18972385 rho -2.7369853e-07 lambda: 10.0\n",
      "    Train error: 31.47453\n",
      "-- Epoch: 6  Batch: 23 --\n",
      "    Loss_i: 0.19477433 Loss_f: 0.26403245 rho -0.05911105 lambda: 0.01\n",
      "    Loss_i: 0.19477433 Loss_f: 0.25737444 rho -0.007387096 lambda: 0.099999994\n",
      "    Loss_i: 0.19477433 Loss_f: 0.22555105 rho -0.00037762782 lambda: 0.99999994\n",
      "    Loss_i: 0.19477433 Loss_f: 0.19431324 rho 5.6800934e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19431324 Loss_f: 0.19942534 rho -6.2971226e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19431324 Loss_f: 0.19942534 rho -6.2971217e-06 lambda: 10.0\n",
      "    Train error: 32.705082\n",
      "-- Epoch: 6  Batch: 24 --\n",
      "    Loss_i: 0.19399123 Loss_f: 0.23914512 rho -0.038676154 lambda: 0.01\n",
      "    Loss_i: 0.19399123 Loss_f: 0.23357697 rho -0.004237272 lambda: 0.099999994\n",
      "    Loss_i: 0.19399123 Loss_f: 0.21122904 rho -0.00018923909 lambda: 0.99999994\n",
      "    Loss_i: 0.19399123 Loss_f: 0.19369334 rho 3.2786627e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19369334 Loss_f: 0.19631976 rho -2.888517e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19369334 Loss_f: 0.19631973 rho -2.8884838e-06 lambda: 10.0\n",
      "    Train error: 32.506035\n",
      "-- Epoch: 6  Batch: 25 --\n",
      "    Loss_i: 0.19541472 Loss_f: 0.2172984 rho -0.017503629 lambda: 0.01\n",
      "    Loss_i: 0.19541472 Loss_f: 0.21515627 rho -0.002177939 lambda: 0.099999994\n",
      "    Loss_i: 0.19541472 Loss_f: 0.20369768 rho -9.498233e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19541472 Loss_f: 0.19530784 rho 1.2305344e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19530784 Loss_f: 0.19655038 rho -1.4304255e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19530784 Loss_f: 0.19655038 rho -1.4304255e-06 lambda: 10.0\n",
      "    Train error: 33.159\n",
      "-- Epoch: 6  Batch: 26 --\n",
      "    Loss_i: 0.1903211 Loss_f: 0.20194961 rho -0.008953196 lambda: 0.01\n",
      "    Loss_i: 0.1903211 Loss_f: 0.20161857 rho -0.001134569 lambda: 0.099999994\n",
      "    Loss_i: 0.1903211 Loss_f: 0.19493264 rho -4.776601e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1903211 Loss_f: 0.19025801 rho 6.5555554e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19025801 Loss_f: 0.19092773 rho -6.950424e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19025801 Loss_f: 0.19092773 rho -6.950422e-07 lambda: 10.0\n",
      "    Train error: 31.897345\n",
      "-- Epoch: 6  Batch: 27 --\n",
      "    Loss_i: 0.18872511 Loss_f: 0.19199964 rho -0.002619498 lambda: 0.01\n",
      "    Loss_i: 0.18872511 Loss_f: 0.19279858 rho -0.0003794983 lambda: 0.099999994\n",
      "    Loss_i: 0.18872511 Loss_f: 0.19046248 rho -1.6456816e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18872511 Loss_f: 0.18868834 rho 3.4893613e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18868834 Loss_f: 0.18894714 rho -2.4543414e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18868834 Loss_f: 0.18894714 rho -2.4543414e-07 lambda: 10.0\n",
      "    Train error: 31.43621\n",
      "-- Epoch: 6  Batch: 28 --\n",
      "    Loss_i: 0.18874095 Loss_f: 0.18918623 rho -0.00031111322 lambda: 0.01\n",
      "    Loss_i: 0.18874095 Loss_f: 0.19032872 rho -0.0001304412 lambda: 0.099999994\n",
      "    Loss_i: 0.18874095 Loss_f: 0.18941434 rho -5.631124e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18874095 Loss_f: 0.18871929 rho 1.815077e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18871929 Loss_f: 0.18881246 rho -7.803273e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18871929 Loss_f: 0.18881246 rho -7.803273e-08 lambda: 10.0\n",
      "    Train error: 31.377129\n",
      "-- Epoch: 6  Batch: 29 --\n",
      "    Loss_i: 0.19082226 Loss_f: 0.18910351 rho 0.001322635 lambda: 0.01\n",
      "    Loss_i: 0.18910351 Loss_f: 0.18875892 rho 0.00025972506 lambda: 0.01\n",
      "    Loss_i: 0.18875892 Loss_f: 0.18978141 rho -0.00074978307 lambda: 0.01\n",
      "    Loss_i: 0.18875892 Loss_f: 0.19050552 rho -0.00014114396 lambda: 0.099999994\n",
      "    Loss_i: 0.18875892 Loss_f: 0.18984678 rho -8.881709e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18875892 Loss_f: 0.18898635 rho -1.8586746e-07 lambda: 9.999999\n",
      "    Train error: 31.674227\n",
      "-- Epoch: 6  Batch: 30 --\n",
      "    Loss_i: 0.19268724 Loss_f: 0.26074684 rho -0.052739386 lambda: 0.01\n",
      "    Loss_i: 0.19268724 Loss_f: 0.25106245 rho -0.0069419686 lambda: 0.099999994\n",
      "    Loss_i: 0.19268724 Loss_f: 0.21886913 rho -0.00032894136 lambda: 0.99999994\n",
      "    Loss_i: 0.19268724 Loss_f: 0.19211724 rho 7.2019765e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19211724 Loss_f: 0.19622456 rho -5.1798465e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19211724 Loss_f: 0.19622457 rho -5.1798647e-06 lambda: 10.0\n",
      "    Train error: 32.0889\n",
      "\n",
      "*** Epoch: 6 Train error: 32.661808522542316  Test error: 31.621216  Time: 714.5689670999996 sec\n",
      "\n",
      "-- Epoch: 7  Batch: 1 --\n",
      "    Loss_i: 0.1878007 Loss_f: 0.21463022 rho -0.022729827 lambda: 0.01\n",
      "    Loss_i: 0.1878007 Loss_f: 0.2117797 rho -0.002413383 lambda: 0.099999994\n",
      "    Loss_i: 0.1878007 Loss_f: 0.19794428 rho -0.00010404672 lambda: 0.99999994\n",
      "    Loss_i: 0.1878007 Loss_f: 0.18761173 rho 1.9421283e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18761173 Loss_f: 0.18910751 rho -1.5354483e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18761173 Loss_f: 0.18910751 rho -1.5354481e-06 lambda: 10.0\n",
      "    Train error: 31.341494\n",
      "-- Epoch: 7  Batch: 2 --\n",
      "    Loss_i: 0.18881768 Loss_f: 0.19667833 rho -0.0057734693 lambda: 0.01\n",
      "    Loss_i: 0.18881768 Loss_f: 0.19673848 rho -0.0006838087 lambda: 0.099999994\n",
      "    Loss_i: 0.18881768 Loss_f: 0.19206294 rho -2.8516803e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18881768 Loss_f: 0.18872526 rho 8.1353946e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18872526 Loss_f: 0.18918495 rho -4.044991e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18872526 Loss_f: 0.18918495 rho -4.0449905e-07 lambda: 10.0\n",
      "    Train error: 31.44932\n",
      "-- Epoch: 7  Batch: 3 --\n",
      "    Loss_i: 0.19213544 Loss_f: 0.19239233 rho -0.00017309662 lambda: 0.01\n",
      "    Loss_i: 0.19213544 Loss_f: 0.19384634 rho -0.00014206649 lambda: 0.099999994\n",
      "    Loss_i: 0.19213544 Loss_f: 0.19295083 rho -6.9317275e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.19213544 Loss_f: 0.19209161 rho 3.7344318e-08 lambda: 9.999999\n",
      "    Loss_i: 0.19209161 Loss_f: 0.19221002 rho -1.0089212e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19209161 Loss_f: 0.19221002 rho -1.0089212e-07 lambda: 10.0\n",
      "    Train error: 32.312595\n",
      "-- Epoch: 7  Batch: 4 --\n",
      "    Loss_i: 0.18581657 Loss_f: 0.18558197 rho 0.00015076714 lambda: 0.01\n",
      "    Loss_i: 0.18558197 Loss_f: 0.1916938 rho -0.0035914986 lambda: 0.01\n",
      "    Loss_i: 0.18558197 Loss_f: 0.19180353 rho -0.0004614027 lambda: 0.099999994\n",
      "    Loss_i: 0.18558197 Loss_f: 0.18932338 rho -2.8493694e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18558197 Loss_f: 0.18629925 rho -5.477387e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18558197 Loss_f: 0.18629925 rho -5.477387e-07 lambda: 10.0\n",
      "    Train error: 30.528269\n",
      "-- Epoch: 7  Batch: 5 --\n",
      "    Loss_i: 0.19044636 Loss_f: 0.30171832 rho -0.06679258 lambda: 0.01\n",
      "    Loss_i: 0.19044636 Loss_f: 0.29273716 rho -0.011175775 lambda: 0.099999994\n",
      "    Loss_i: 0.19044636 Loss_f: 0.24640875 rho -0.00066603953 lambda: 0.99999994\n",
      "    Loss_i: 0.19044636 Loss_f: 0.18947491 rho 1.1666003e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18947491 Loss_f: 0.19978693 rho -1.2395081e-05 lambda: 9.999999\n",
      "    Loss_i: 0.18947491 Loss_f: 0.19978692 rho -1.2395061e-05 lambda: 10.0\n",
      "    Train error: 31.990807\n",
      "-- Epoch: 7  Batch: 6 --\n",
      "    Loss_i: 0.18461312 Loss_f: 0.2771042 rho -0.06502367 lambda: 0.01\n",
      "    Loss_i: 0.18461312 Loss_f: 0.2628683 rho -0.0068474063 lambda: 0.099999994\n",
      "    Loss_i: 0.18461312 Loss_f: 0.21999457 rho -0.0003173549 lambda: 0.99999994\n",
      "    Loss_i: 0.18461312 Loss_f: 0.18399191 rho 5.586008e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18399191 Loss_f: 0.18940946 rho -4.8615448e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18399191 Loss_f: 0.18940946 rho -4.8615448e-06 lambda: 10.0\n",
      "    Train error: 30.440567\n",
      "-- Epoch: 7  Batch: 7 --\n",
      "    Loss_i: 0.19373195 Loss_f: 0.23607641 rho -0.024992613 lambda: 0.01\n",
      "    Loss_i: 0.19373195 Loss_f: 0.23120584 rho -0.0037771158 lambda: 0.099999994\n",
      "    Loss_i: 0.19373195 Loss_f: 0.20958203 rho -0.00017192564 lambda: 0.99999994\n",
      "    Loss_i: 0.19373195 Loss_f: 0.19348468 rho 2.7027176e-07 lambda: 9.999999\n",
      "    Loss_i: 0.19348468 Loss_f: 0.19581406 rho -2.5424822e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19348468 Loss_f: 0.19581406 rho -2.5424818e-06 lambda: 10.0\n",
      "    Train error: 32.605255\n",
      "-- Epoch: 7  Batch: 8 --\n",
      "    Loss_i: 0.19189398 Loss_f: 0.2091569 rho -0.014624493 lambda: 0.01\n",
      "    Loss_i: 0.19189398 Loss_f: 0.20772214 rho -0.0015128201 lambda: 0.099999994\n",
      "    Loss_i: 0.19189398 Loss_f: 0.19802077 rho -5.9319027e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.19189398 Loss_f: 0.1917475 rho 1.4200347e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1917475 Loss_f: 0.19257346 rho -7.999534e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1917475 Loss_f: 0.19257346 rho -7.999532e-07 lambda: 10.0\n",
      "    Train error: 32.227085\n",
      "-- Epoch: 7  Batch: 9 --\n",
      "    Loss_i: 0.18741491 Loss_f: 0.19312498 rho -0.003818051 lambda: 0.01\n",
      "    Loss_i: 0.18741491 Loss_f: 0.19332263 rho -0.00047285727 lambda: 0.099999994\n",
      "    Loss_i: 0.18741491 Loss_f: 0.1897898 rho -1.9390834e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18741491 Loss_f: 0.18737316 rho 3.4159807e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18737316 Loss_f: 0.18771325 rho -2.7816594e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18737316 Loss_f: 0.18771325 rho -2.7816594e-07 lambda: 10.0\n",
      "    Train error: 31.203293\n",
      "-- Epoch: 7  Batch: 10 --\n",
      "    Loss_i: 0.18835561 Loss_f: 0.1904833 rho -0.001555137 lambda: 0.01\n",
      "    Loss_i: 0.18835561 Loss_f: 0.1913136 rho -0.00024038383 lambda: 0.099999994\n",
      "    Loss_i: 0.18835561 Loss_f: 0.18957563 rho -1.0026765e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18835561 Loss_f: 0.18832123 rho 2.8284864e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18832123 Loss_f: 0.18849263 rho -1.4099601e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18832123 Loss_f: 0.18849261 rho -1.4098372e-07 lambda: 10.0\n",
      "    Train error: 31.463102\n",
      "-- Epoch: 7  Batch: 11 --\n",
      "    Loss_i: 0.18815494 Loss_f: 0.18806162 rho 4.9225466e-05 lambda: 0.01\n",
      "    Loss_i: 0.18806162 Loss_f: 0.19291307 rho -0.00259303 lambda: 0.01\n",
      "    Loss_i: 0.18806162 Loss_f: 0.193326 rho -0.00036896984 lambda: 0.099999994\n",
      "    Loss_i: 0.18806162 Loss_f: 0.19119708 rho -2.2681961e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18806162 Loss_f: 0.18865928 rho -4.337391e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18806162 Loss_f: 0.18865928 rho -4.3373902e-07 lambda: 10.0\n",
      "    Train error: 31.728113\n",
      "-- Epoch: 7  Batch: 12 --\n",
      "    Loss_i: 0.19387248 Loss_f: 0.30107757 rho -0.07109157 lambda: 0.01\n",
      "    Loss_i: 0.19387248 Loss_f: 0.2869303 rho -0.009555547 lambda: 0.099999994\n",
      "    Loss_i: 0.19387248 Loss_f: 0.23958173 rho -0.0004965972 lambda: 0.99999994\n",
      "    Loss_i: 0.19387248 Loss_f: 0.19288985 rho 1.0737826e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19288985 Loss_f: 0.20058815 rho -8.414238e-06 lambda: 9.999999\n",
      "    Loss_i: 0.19288985 Loss_f: 0.20058815 rho -8.414237e-06 lambda: 10.0\n",
      "    Train error: 32.277493\n",
      "-- Epoch: 7  Batch: 13 --\n",
      "    Loss_i: 0.18284136 Loss_f: 0.24206018 rho -0.03691188 lambda: 0.01\n",
      "    Loss_i: 0.18284136 Loss_f: 0.234926 rho -0.004257186 lambda: 0.099999994\n",
      "    Loss_i: 0.18284136 Loss_f: 0.20707747 rho -0.00020446132 lambda: 0.99999994\n",
      "    Loss_i: 0.18284136 Loss_f: 0.18252344 rho 2.6906594e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18252344 Loss_f: 0.18637787 rho -3.2621201e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18252344 Loss_f: 0.18637787 rho -3.2621197e-06 lambda: 10.0\n",
      "    Train error: 30.404762\n",
      "-- Epoch: 7  Batch: 14 --\n",
      "    Loss_i: 0.18605624 Loss_f: 0.23374556 rho -0.038829166 lambda: 0.01\n",
      "    Loss_i: 0.18605624 Loss_f: 0.22766015 rho -0.003852775 lambda: 0.099999994\n",
      "    Loss_i: 0.18605624 Loss_f: 0.20348111 rho -0.00016361242 lambda: 0.99999994\n",
      "    Loss_i: 0.18605624 Loss_f: 0.185832 rho 2.1085305e-07 lambda: 9.999999\n",
      "    Loss_i: 0.185832 Loss_f: 0.18841402 rho -2.4265823e-06 lambda: 9.999999\n",
      "    Loss_i: 0.185832 Loss_f: 0.18841402 rho -2.4265821e-06 lambda: 10.0\n",
      "    Train error: 30.569735\n",
      "-- Epoch: 7  Batch: 15 --\n",
      "    Loss_i: 0.18583295 Loss_f: 0.20494017 rho -0.012396298 lambda: 0.01\n",
      "    Loss_i: 0.18583295 Loss_f: 0.20270197 rho -0.0014055693 lambda: 0.099999994\n",
      "    Loss_i: 0.18583295 Loss_f: 0.19264038 rho -5.838099e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18583295 Loss_f: 0.18568851 rho 1.2423362e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18568851 Loss_f: 0.18666974 rho -8.434898e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18568851 Loss_f: 0.18666977 rho -8.435153e-07 lambda: 10.0\n",
      "    Train error: 30.674282\n",
      "-- Epoch: 7  Batch: 16 --\n",
      "    Loss_i: 0.18551889 Loss_f: 0.190557 rho -0.0039231163 lambda: 0.01\n",
      "    Loss_i: 0.18551889 Loss_f: 0.19094047 rho -0.000476404 lambda: 0.099999994\n",
      "    Loss_i: 0.18551889 Loss_f: 0.18776412 rho -1.9986006e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18551889 Loss_f: 0.18547487 rho 3.9233857e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18547487 Loss_f: 0.1858003 rho -2.8984718e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18547487 Loss_f: 0.1858003 rho -2.8984718e-07 lambda: 10.0\n",
      "    Train error: 30.590723\n",
      "-- Epoch: 7  Batch: 17 --\n",
      "    Loss_i: 0.18514061 Loss_f: 0.18664823 rho -0.0009353482 lambda: 0.01\n",
      "    Loss_i: 0.18514061 Loss_f: 0.18765645 rho -0.00018517503 lambda: 0.099999994\n",
      "    Loss_i: 0.18514061 Loss_f: 0.18622252 rho -8.114518e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18514061 Loss_f: 0.18511549 rho 1.8878758e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18511549 Loss_f: 0.18527435 rho -1.1930197e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18511549 Loss_f: 0.18527435 rho -1.1930196e-07 lambda: 10.0\n",
      "    Train error: 30.47002\n",
      "-- Epoch: 7  Batch: 18 --\n",
      "    Loss_i: 0.18718949 Loss_f: 0.18739659 rho -0.00013176346 lambda: 0.01\n",
      "    Loss_i: 0.18718949 Loss_f: 0.1885159 rho -9.9815305e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.18718949 Loss_f: 0.18779151 rho -4.6146783e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18718949 Loss_f: 0.18717378 rho 1.2061435e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18717378 Loss_f: 0.18726136 rho -6.7236485e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18717378 Loss_f: 0.18726136 rho -6.7236485e-08 lambda: 10.0\n",
      "    Train error: 30.732025\n",
      "-- Epoch: 7  Batch: 19 --\n",
      "    Loss_i: 0.18965673 Loss_f: 0.18825798 rho 0.00095096265 lambda: 0.01\n",
      "    Loss_i: 0.18825798 Loss_f: 0.18836142 rho -6.875989e-05 lambda: 0.01\n",
      "    Loss_i: 0.18825798 Loss_f: 0.18940394 rho -8.4201136e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.18825798 Loss_f: 0.18901227 rho -5.601355e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18825798 Loss_f: 0.18840228 rho -1.0727251e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18825798 Loss_f: 0.18840231 rho -1.0729465e-07 lambda: 10.0\n",
      "    Train error: 31.375103\n",
      "-- Epoch: 7  Batch: 20 --\n",
      "    Loss_i: 0.18430294 Loss_f: 0.21656892 rho -0.02144609 lambda: 0.01\n",
      "    Loss_i: 0.18430294 Loss_f: 0.21307272 rho -0.002267772 lambda: 0.099999994\n",
      "    Loss_i: 0.18430294 Loss_f: 0.19653983 rho -9.828453e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18430294 Loss_f: 0.18415935 rho 1.1554592e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18415935 Loss_f: 0.18598482 rho -1.4686939e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18415935 Loss_f: 0.18598482 rho -1.4686939e-06 lambda: 10.0\n",
      "    Train error: 30.100739\n",
      "-- Epoch: 7  Batch: 21 --\n",
      "    Loss_i: 0.18728174 Loss_f: 0.20435633 rho -0.011571458 lambda: 0.01\n",
      "    Loss_i: 0.18728174 Loss_f: 0.20288889 rho -0.0013397861 lambda: 0.099999994\n",
      "    Loss_i: 0.18728174 Loss_f: 0.19375403 rho -5.7083413e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18728174 Loss_f: 0.18715422 rho 1.1278101e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18715422 Loss_f: 0.18810992 rho -8.4440677e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18715422 Loss_f: 0.18810993 rho -8.4441984e-07 lambda: 10.0\n",
      "    Train error: 31.029211\n",
      "-- Epoch: 7  Batch: 22 --\n",
      "    Loss_i: 0.18295345 Loss_f: 0.18877715 rho -0.0037686774 lambda: 0.01\n",
      "    Loss_i: 0.18295345 Loss_f: 0.1890786 rho -0.00045385293 lambda: 0.099999994\n",
      "    Loss_i: 0.18295345 Loss_f: 0.18551303 rho -1.9244744e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18295345 Loss_f: 0.18290645 rho 3.5388588e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18290645 Loss_f: 0.183286 rho -2.8560035e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18290645 Loss_f: 0.183286 rho -2.856003e-07 lambda: 10.0\n",
      "    Train error: 29.891493\n",
      "-- Epoch: 7  Batch: 23 --\n",
      "    Loss_i: 0.18820506 Loss_f: 0.189602 rho -0.0008604303 lambda: 0.01\n",
      "    Loss_i: 0.18820506 Loss_f: 0.19059847 rho -0.00018368711 lambda: 0.099999994\n",
      "    Loss_i: 0.18820506 Loss_f: 0.18926965 rho -8.376436e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18820506 Loss_f: 0.18817681 rho 2.228613e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18817681 Loss_f: 0.18833205 rho -1.224142e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18817681 Loss_f: 0.18833205 rho -1.2241419e-07 lambda: 10.0\n",
      "    Train error: 30.912094\n",
      "-- Epoch: 7  Batch: 24 --\n",
      "    Loss_i: 0.18763313 Loss_f: 0.18738058 rho 0.00014649346 lambda: 0.01\n",
      "    Loss_i: 0.18738058 Loss_f: 0.19035095 rho -0.0015959106 lambda: 0.01\n",
      "    Loss_i: 0.18738058 Loss_f: 0.19082107 rho -0.0002217221 lambda: 0.099999994\n",
      "    Loss_i: 0.18738058 Loss_f: 0.18945639 rho -1.36498165e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18738058 Loss_f: 0.18777694 rho -2.611622e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18738058 Loss_f: 0.18777694 rho -2.611622e-07 lambda: 10.0\n",
      "    Train error: 30.733395\n",
      "-- Epoch: 7  Batch: 25 --\n",
      "    Loss_i: 0.18927452 Loss_f: 0.27357626 rho -0.04643704 lambda: 0.01\n",
      "    Loss_i: 0.18927452 Loss_f: 0.26522547 rho -0.006224304 lambda: 0.099999994\n",
      "    Loss_i: 0.18927452 Loss_f: 0.22671854 rho -0.00032259442 lambda: 0.99999994\n",
      "    Loss_i: 0.18927452 Loss_f: 0.18889242 rho 3.308862e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18889242 Loss_f: 0.19520868 rho -5.469555e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18889242 Loss_f: 0.19520868 rho -5.4695547e-06 lambda: 10.0\n",
      "    Train error: 31.573883\n",
      "-- Epoch: 7  Batch: 26 --\n",
      "    Loss_i: 0.18390821 Loss_f: 0.25260484 rho -0.038621858 lambda: 0.01\n",
      "    Loss_i: 0.18390821 Loss_f: 0.24530382 rho -0.0047632 lambda: 0.099999994\n",
      "    Loss_i: 0.18390821 Loss_f: 0.21104795 rho -0.00021887179 lambda: 0.99999994\n",
      "    Loss_i: 0.18390821 Loss_f: 0.18367614 rho 1.8789838e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18367614 Loss_f: 0.18781091 rho -3.346488e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18367614 Loss_f: 0.18781091 rho -3.3464876e-06 lambda: 10.0\n",
      "    Train error: 30.046082\n",
      "-- Epoch: 7  Batch: 27 --\n",
      "    Loss_i: 0.18259989 Loss_f: 0.21586235 rho -0.020455698 lambda: 0.01\n",
      "    Loss_i: 0.18259989 Loss_f: 0.21189173 rho -0.0021571643 lambda: 0.099999994\n",
      "    Loss_i: 0.18259989 Loss_f: 0.19479021 rho -9.1583126e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18259989 Loss_f: 0.18235496 rho 1.8438207e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18235496 Loss_f: 0.184145 rho -1.3484414e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18235496 Loss_f: 0.18414503 rho -1.3484637e-06 lambda: 10.0\n",
      "    Train error: 29.834463\n",
      "-- Epoch: 7  Batch: 28 --\n",
      "    Loss_i: 0.18195878 Loss_f: 0.19624275 rho -0.00878954 lambda: 0.01\n",
      "    Loss_i: 0.18195878 Loss_f: 0.19510005 rho -0.0009132607 lambda: 0.099999994\n",
      "    Loss_i: 0.18195878 Loss_f: 0.1869944 rho -3.5454083e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18195878 Loss_f: 0.18184742 rho 7.850505e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18184742 Loss_f: 0.18252814 rho -4.795543e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18184742 Loss_f: 0.18252814 rho -4.795542e-07 lambda: 10.0\n",
      "    Train error: 29.625462\n",
      "-- Epoch: 7  Batch: 29 --\n",
      "    Loss_i: 0.18375239 Loss_f: 0.18598366 rho -0.00137663 lambda: 0.01\n",
      "    Loss_i: 0.18375239 Loss_f: 0.18670057 rho -0.00020969402 lambda: 0.099999994\n",
      "    Loss_i: 0.18375239 Loss_f: 0.18498206 rho -8.882002e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18375239 Loss_f: 0.18372035 rho 2.3176842e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18372035 Loss_f: 0.183895 rho -1.2633963e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18372035 Loss_f: 0.183895 rho -1.2633963e-07 lambda: 10.0\n",
      "    Train error: 30.102137\n",
      "-- Epoch: 7  Batch: 30 --\n",
      "    Loss_i: 0.1867715 Loss_f: 0.18463573 rho 0.0014006832 lambda: 0.01\n",
      "    Loss_i: 0.18463573 Loss_f: 0.18533015 rho -0.00042527047 lambda: 0.01\n",
      "    Loss_i: 0.18463573 Loss_f: 0.18633784 rho -0.00013040553 lambda: 0.099999994\n",
      "    Loss_i: 0.18463573 Loss_f: 0.18570009 rho -8.36444e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18463573 Loss_f: 0.18482976 rho -1.5287353e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18463573 Loss_f: 0.18482976 rho -1.5287353e-07 lambda: 10.0\n",
      "    Train error: 29.963892\n",
      "\n",
      "*** Epoch: 7 Train error: 30.939896392822266  Test error: 29.896406  Time: 712.8119820000002 sec\n",
      "\n",
      "-- Epoch: 8  Batch: 1 --\n",
      "    Loss_i: 0.18138668 Loss_f: 0.20579536 rho -0.01346596 lambda: 0.01\n",
      "    Loss_i: 0.18138668 Loss_f: 0.20329994 rho -0.0015667246 lambda: 0.099999994\n",
      "    Loss_i: 0.18138668 Loss_f: 0.19057137 rho -6.767023e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18138668 Loss_f: 0.18113695 rho 1.8455596e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18113695 Loss_f: 0.18250474 rho -1.0101795e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18113695 Loss_f: 0.18250474 rho -1.0101795e-06 lambda: 10.0\n",
      "    Train error: 29.497667\n",
      "-- Epoch: 8  Batch: 2 --\n",
      "    Loss_i: 0.18280843 Loss_f: 0.18937398 rho -0.003504651 lambda: 0.01\n",
      "    Loss_i: 0.18280843 Loss_f: 0.18936703 rho -0.00044186047 lambda: 0.099999994\n",
      "    Loss_i: 0.18280843 Loss_f: 0.18538655 rho -1.7836657e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18280843 Loss_f: 0.1827248 rho 5.801202e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1827248 Loss_f: 0.1830798 rho -2.461489e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1827248 Loss_f: 0.1830798 rho -2.4614886e-07 lambda: 10.0\n",
      "    Train error: 29.757614\n",
      "-- Epoch: 8  Batch: 3 --\n",
      "    Loss_i: 0.1858685 Loss_f: 0.18687943 rho -0.0005780577 lambda: 0.01\n",
      "    Loss_i: 0.1858685 Loss_f: 0.18782793 rho -0.0001275521 lambda: 0.099999994\n",
      "    Loss_i: 0.1858685 Loss_f: 0.1866637 rho -5.2491478e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1858685 Loss_f: 0.18580806 rho 3.9952166e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18580806 Loss_f: 0.18591172 rho -6.8507774e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18580806 Loss_f: 0.18591172 rho -6.850776e-08 lambda: 10.0\n",
      "    Train error: 30.502985\n",
      "-- Epoch: 8  Batch: 4 --\n",
      "    Loss_i: 0.179835 Loss_f: 0.18111992 rho -0.000673038 lambda: 0.01\n",
      "    Loss_i: 0.179835 Loss_f: 0.18188836 rho -0.00012535944 lambda: 0.099999994\n",
      "    Loss_i: 0.179835 Loss_f: 0.18072684 rho -5.5364053e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.179835 Loss_f: 0.1798055 rho 1.8346821e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1798055 Loss_f: 0.17993611 rho -8.119503e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1798055 Loss_f: 0.17993611 rho -8.119503e-08 lambda: 10.0\n",
      "    Train error: 29.20096\n",
      "-- Epoch: 8  Batch: 5 --\n",
      "    Loss_i: 0.18434781 Loss_f: 0.18575777 rho -0.00076760654 lambda: 0.01\n",
      "    Loss_i: 0.18434781 Loss_f: 0.18661456 rho -0.00014067425 lambda: 0.099999994\n",
      "    Loss_i: 0.18434781 Loss_f: 0.18532151 rho -6.128541e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18434781 Loss_f: 0.18431355 rho 2.1592703e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18431355 Loss_f: 0.1844518 rho -8.7147974e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18431355 Loss_f: 0.18445179 rho -8.713858e-08 lambda: 10.0\n",
      "    Train error: 30.38023\n",
      "-- Epoch: 8  Batch: 6 --\n",
      "    Loss_i: 0.17904748 Loss_f: 0.17898545 rho 3.019392e-05 lambda: 0.01\n",
      "    Loss_i: 0.17898545 Loss_f: 0.18427846 rho -0.0023813338 lambda: 0.01\n",
      "    Loss_i: 0.17898545 Loss_f: 0.18455958 rho -0.0003179952 lambda: 0.099999994\n",
      "    Loss_i: 0.17898545 Loss_f: 0.18235318 rho -1.974148e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17898545 Loss_f: 0.17963058 rho -3.792164e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17898545 Loss_f: 0.17963058 rho -3.7921637e-07 lambda: 10.0\n",
      "    Train error: 28.919378\n",
      "-- Epoch: 8  Batch: 7 --\n",
      "    Loss_i: 0.18878885 Loss_f: 0.3009258 rho -0.0860345 lambda: 0.01\n",
      "    Loss_i: 0.18878885 Loss_f: 0.2920207 rho -0.010305608 lambda: 0.099999994\n",
      "    Loss_i: 0.18878885 Loss_f: 0.24516414 rho -0.0005802693 lambda: 0.99999994\n",
      "    Loss_i: 0.18878885 Loss_f: 0.18817803 rho 6.30667e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18817803 Loss_f: 0.19847427 rho -1.0617766e-05 lambda: 9.999999\n",
      "    Loss_i: 0.18817803 Loss_f: 0.19847424 rho -1.0617735e-05 lambda: 10.0\n",
      "    Train error: 31.269041\n",
      "-- Epoch: 8  Batch: 8 --\n",
      "    Loss_i: 0.18773362 Loss_f: 0.2942463 rho -0.090034366 lambda: 0.01\n",
      "    Loss_i: 0.18773362 Loss_f: 0.28075254 rho -0.009033118 lambda: 0.099999994\n",
      "    Loss_i: 0.18773362 Loss_f: 0.23015332 rho -0.00041816392 lambda: 0.99999994\n",
      "    Loss_i: 0.18773362 Loss_f: 0.18725759 rho 4.6997218e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18725759 Loss_f: 0.19390287 rho -6.5533627e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18725759 Loss_f: 0.19390287 rho -6.5533623e-06 lambda: 10.0\n",
      "    Train error: 30.886518\n",
      "-- Epoch: 8  Batch: 9 --\n",
      "    Loss_i: 0.18298341 Loss_f: 0.2399577 rho -0.035188224 lambda: 0.01\n",
      "    Loss_i: 0.18298341 Loss_f: 0.23348948 rho -0.0038663314 lambda: 0.099999994\n",
      "    Loss_i: 0.18298341 Loss_f: 0.2068046 rho -0.00018682958 lambda: 0.99999994\n",
      "    Loss_i: 0.18298341 Loss_f: 0.18269563 rho 2.2626583e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18269563 Loss_f: 0.18665482 rho -3.1174086e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18269563 Loss_f: 0.18665482 rho -3.1174086e-06 lambda: 10.0\n",
      "    Train error: 30.01291\n",
      "-- Epoch: 8  Batch: 10 --\n",
      "    Loss_i: 0.18343607 Loss_f: 0.22137772 rho -0.02066726 lambda: 0.01\n",
      "    Loss_i: 0.18343607 Loss_f: 0.21669191 rho -0.002497076 lambda: 0.099999994\n",
      "    Loss_i: 0.18343607 Loss_f: 0.19708243 rho -0.00010649683 lambda: 0.99999994\n",
      "    Loss_i: 0.18343607 Loss_f: 0.183205 rho 1.8104201e-07 lambda: 9.999999\n",
      "    Loss_i: 0.183205 Loss_f: 0.18518883 rho -1.5524415e-06 lambda: 9.999999\n",
      "    Loss_i: 0.183205 Loss_f: 0.18518883 rho -1.5524414e-06 lambda: 10.0\n",
      "    Train error: 29.972015\n",
      "-- Epoch: 8  Batch: 11 --\n",
      "    Loss_i: 0.1826755 Loss_f: 0.19788955 rho -0.009130704 lambda: 0.01\n",
      "    Loss_i: 0.1826755 Loss_f: 0.19691499 rho -0.0010023103 lambda: 0.099999994\n",
      "    Loss_i: 0.1826755 Loss_f: 0.1885991 rho -4.242936e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1826755 Loss_f: 0.18255728 rho 8.482094e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18255728 Loss_f: 0.1834328 rho -6.279563e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18255728 Loss_f: 0.1834328 rho -6.2795624e-07 lambda: 10.0\n",
      "    Train error: 29.936415\n",
      "-- Epoch: 8  Batch: 12 --\n",
      "    Loss_i: 0.18791486 Loss_f: 0.19185075 rho -0.002346033 lambda: 0.01\n",
      "    Loss_i: 0.18791486 Loss_f: 0.19231495 rho -0.00031008403 lambda: 0.099999994\n",
      "    Loss_i: 0.18791486 Loss_f: 0.18964317 rho -1.2405949e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18791486 Loss_f: 0.18786675 rho 3.4602163e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18786675 Loss_f: 0.18810174 rho -1.689117e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18786675 Loss_f: 0.18810174 rho -1.6891167e-07 lambda: 10.0\n",
      "    Train error: 31.014406\n",
      "-- Epoch: 8  Batch: 13 --\n",
      "    Loss_i: 0.17826214 Loss_f: 0.17822851 rho 1.3230466e-05 lambda: 0.01\n",
      "    Loss_i: 0.17822851 Loss_f: 0.18267028 rho -0.0017750202 lambda: 0.01\n",
      "    Loss_i: 0.17822851 Loss_f: 0.18295808 rho -0.00026219347 lambda: 0.099999994\n",
      "    Loss_i: 0.17822851 Loss_f: 0.18107243 rho -1.6400965e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17822851 Loss_f: 0.17877418 rho -3.159601e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17822851 Loss_f: 0.17877418 rho -3.1596008e-07 lambda: 10.0\n",
      "    Train error: 29.381777\n",
      "-- Epoch: 8  Batch: 14 --\n",
      "    Loss_i: 0.18260866 Loss_f: 0.3079842 rho -0.10454201 lambda: 0.01\n",
      "    Loss_i: 0.18260866 Loss_f: 0.295089 rho -0.010521343 lambda: 0.099999994\n",
      "    Loss_i: 0.18260866 Loss_f: 0.23932745 rho -0.00053708587 lambda: 0.99999994\n",
      "    Loss_i: 0.18260866 Loss_f: 0.18199764 rho 5.793081e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18199764 Loss_f: 0.19179612 rho -9.286281e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18199764 Loss_f: 0.19179612 rho -9.286281e-06 lambda: 10.0\n",
      "    Train error: 29.452911\n",
      "-- Epoch: 8  Batch: 15 --\n",
      "    Loss_i: 0.18121116 Loss_f: 0.2597437 rho -0.03950679 lambda: 0.01\n",
      "    Loss_i: 0.18121116 Loss_f: 0.2510239 rho -0.0046285675 lambda: 0.099999994\n",
      "    Loss_i: 0.18121116 Loss_f: 0.21477401 rho -0.00022982758 lambda: 0.99999994\n",
      "    Loss_i: 0.18121116 Loss_f: 0.18081 rho 2.756023e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18081 Loss_f: 0.18641396 rho -3.8518306e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18081 Loss_f: 0.18641396 rho -3.85183e-06 lambda: 10.0\n",
      "    Train error: 29.472105\n",
      "-- Epoch: 8  Batch: 16 --\n",
      "    Loss_i: 0.18148334 Loss_f: 0.2443316 rho -0.04720316 lambda: 0.01\n",
      "    Loss_i: 0.18148334 Loss_f: 0.23739108 rho -0.0048265583 lambda: 0.099999994\n",
      "    Loss_i: 0.18148334 Loss_f: 0.2062454 rho -0.0002170159 lambda: 0.99999994\n",
      "    Loss_i: 0.18148334 Loss_f: 0.18129994 rho 1.6098002e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18129994 Loss_f: 0.18513508 rho -3.3614217e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18129994 Loss_f: 0.18513508 rho -3.3614212e-06 lambda: 10.0\n",
      "    Train error: 29.415258\n",
      "-- Epoch: 8  Batch: 17 --\n",
      "    Loss_i: 0.1803296 Loss_f: 0.22124135 rho -0.02129223 lambda: 0.01\n",
      "    Loss_i: 0.1803296 Loss_f: 0.21717128 rho -0.0023588813 lambda: 0.099999994\n",
      "    Loss_i: 0.1803296 Loss_f: 0.19677839 rho -0.000107799526 lambda: 0.99999994\n",
      "    Loss_i: 0.1803296 Loss_f: 0.18015258 rho 1.162906e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18015258 Loss_f: 0.18274817 rho -1.7031917e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18015258 Loss_f: 0.18274818 rho -1.7032013e-06 lambda: 10.0\n",
      "    Train error: 29.246262\n",
      "-- Epoch: 8  Batch: 18 --\n",
      "    Loss_i: 0.18242714 Loss_f: 0.21421844 rho -0.01841372 lambda: 0.01\n",
      "    Loss_i: 0.18242714 Loss_f: 0.21014589 rho -0.0019941232 lambda: 0.099999994\n",
      "    Loss_i: 0.18242714 Loss_f: 0.19350965 rho -8.1706894e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18242714 Loss_f: 0.18226208 rho 1.2199477e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18226208 Loss_f: 0.18383092 rho -1.1593728e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18226208 Loss_f: 0.18383092 rho -1.1593728e-06 lambda: 10.0\n",
      "    Train error: 29.501701\n",
      "-- Epoch: 8  Batch: 19 --\n",
      "    Loss_i: 0.1844069 Loss_f: 0.197551 rho -0.006940489 lambda: 0.01\n",
      "    Loss_i: 0.1844069 Loss_f: 0.19641903 rho -0.00079182937 lambda: 0.099999994\n",
      "    Loss_i: 0.1844069 Loss_f: 0.18927452 rho -3.2904234e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1844069 Loss_f: 0.18434733 rho 4.037443e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18434733 Loss_f: 0.18504998 rho -4.7618644e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18434733 Loss_f: 0.18504998 rho -4.7618644e-07 lambda: 10.0\n",
      "    Train error: 30.298859\n",
      "-- Epoch: 8  Batch: 20 --\n",
      "    Loss_i: 0.18003544 Loss_f: 0.1843899 rho -0.0024746044 lambda: 0.01\n",
      "    Loss_i: 0.18003544 Loss_f: 0.18470143 rho -0.00030631266 lambda: 0.099999994\n",
      "    Loss_i: 0.18003544 Loss_f: 0.18190648 rho -1.2476584e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18003544 Loss_f: 0.17999503 rho 2.6990376e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17999503 Loss_f: 0.1802612 rho -1.7772646e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17999503 Loss_f: 0.1802612 rho -1.7772643e-07 lambda: 10.0\n",
      "    Train error: 29.099695\n",
      "-- Epoch: 8  Batch: 21 --\n",
      "    Loss_i: 0.18281361 Loss_f: 0.18358 rho -0.0003956852 lambda: 0.01\n",
      "    Loss_i: 0.18281361 Loss_f: 0.18461826 rho -0.000117255426 lambda: 0.099999994\n",
      "    Loss_i: 0.18281361 Loss_f: 0.18362482 rho -5.4105576e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.18281361 Loss_f: 0.18279175 rho 1.4618929e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18279175 Loss_f: 0.18291083 rho -7.962459e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18279175 Loss_f: 0.18291083 rho -7.962459e-08 lambda: 10.0\n",
      "    Train error: 29.88764\n",
      "-- Epoch: 8  Batch: 22 --\n",
      "    Loss_i: 0.1788019 Loss_f: 0.17836516 rho 0.00020297307 lambda: 0.01\n",
      "    Loss_i: 0.17836516 Loss_f: 0.18039858 rho -0.000884456 lambda: 0.01\n",
      "    Loss_i: 0.17836516 Loss_f: 0.1809353 rho -0.00013118418 lambda: 0.099999994\n",
      "    Loss_i: 0.17836516 Loss_f: 0.17991394 rho -8.044814e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17836516 Loss_f: 0.17865844 rho -1.5260983e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17836516 Loss_f: 0.17865844 rho -1.5260983e-07 lambda: 10.0\n",
      "    Train error: 28.577848\n",
      "-- Epoch: 8  Batch: 23 --\n",
      "    Loss_i: 0.18422765 Loss_f: 0.25287062 rho -0.03666193 lambda: 0.01\n",
      "    Loss_i: 0.18422765 Loss_f: 0.24518423 rho -0.004655952 lambda: 0.099999994\n",
      "    Loss_i: 0.18422765 Loss_f: 0.21358359 rho -0.00023430245 lambda: 0.99999994\n",
      "    Loss_i: 0.18422765 Loss_f: 0.18391141 rho 2.535383e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18391141 Loss_f: 0.18884973 rho -3.9655674e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18391141 Loss_f: 0.18884973 rho -3.9655665e-06 lambda: 10.0\n",
      "    Train error: 29.99666\n",
      "-- Epoch: 8  Batch: 24 --\n",
      "    Loss_i: 0.18342717 Loss_f: 0.23802659 rho -0.028558597 lambda: 0.01\n",
      "    Loss_i: 0.18342717 Loss_f: 0.23126848 rho -0.0031301659 lambda: 0.099999994\n",
      "    Loss_i: 0.18342717 Loss_f: 0.20415594 rho -0.00013911446 lambda: 0.99999994\n",
      "    Loss_i: 0.18342717 Loss_f: 0.18323521 rho 1.2915793e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18323521 Loss_f: 0.18640478 rho -2.1289397e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18323521 Loss_f: 0.18640478 rho -2.1289395e-06 lambda: 10.0\n",
      "    Train error: 29.733805\n",
      "-- Epoch: 8  Batch: 25 --\n",
      "    Loss_i: 0.18487145 Loss_f: 0.2166005 rho -0.01622493 lambda: 0.01\n",
      "    Loss_i: 0.18487145 Loss_f: 0.21294902 rho -0.0019904173 lambda: 0.099999994\n",
      "    Loss_i: 0.18487145 Loss_f: 0.19658522 rho -8.637558e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18487145 Loss_f: 0.18473336 rho 1.0223558e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18473336 Loss_f: 0.18646751 rho -1.2845704e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18473336 Loss_f: 0.18646751 rho -1.2845703e-06 lambda: 10.0\n",
      "    Train error: 30.371067\n",
      "-- Epoch: 8  Batch: 26 --\n",
      "    Loss_i: 0.17975447 Loss_f: 0.19562313 rho -0.0072234664 lambda: 0.01\n",
      "    Loss_i: 0.17975447 Loss_f: 0.19424137 rho -0.0008506583 lambda: 0.099999994\n",
      "    Loss_i: 0.17975447 Loss_f: 0.18553962 rho -3.4984252e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17975447 Loss_f: 0.17965773 rho 5.8675308e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17965773 Loss_f: 0.18048543 rho -5.019239e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17965773 Loss_f: 0.18048541 rho -5.0191477e-07 lambda: 10.0\n",
      "    Train error: 29.08532\n",
      "-- Epoch: 8  Batch: 27 --\n",
      "    Loss_i: 0.17831896 Loss_f: 0.18261398 rho -0.0022109754 lambda: 0.01\n",
      "    Loss_i: 0.17831896 Loss_f: 0.1828458 rho -0.00027623217 lambda: 0.099999994\n",
      "    Loss_i: 0.17831896 Loss_f: 0.18008414 rho -1.0974738e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17831896 Loss_f: 0.17827152 rho 2.9554272e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17827152 Loss_f: 0.17851351 rho -1.5073377e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17827152 Loss_f: 0.17851351 rho -1.5073377e-07 lambda: 10.0\n",
      "    Train error: 28.721449\n",
      "-- Epoch: 8  Batch: 28 --\n",
      "    Loss_i: 0.17795041 Loss_f: 0.17839907 rho -0.00019969788 lambda: 0.01\n",
      "    Loss_i: 0.17795041 Loss_f: 0.17913294 rho -6.2352105e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17795041 Loss_f: 0.17844212 rho -2.6414427e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17795041 Loss_f: 0.17793341 rho 9.150756e-09 lambda: 9.999999\n",
      "    Loss_i: 0.17793341 Loss_f: 0.17800044 rho -3.607124e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17793341 Loss_f: 0.17800044 rho -3.6071235e-08 lambda: 10.0\n",
      "    Train error: 28.56109\n",
      "-- Epoch: 8  Batch: 29 --\n",
      "    Loss_i: 0.18010914 Loss_f: 0.17871684 rho 0.000700585 lambda: 0.01\n",
      "    Loss_i: 0.17871684 Loss_f: 0.1782437 rho 0.00023399403 lambda: 0.01\n",
      "    Loss_i: 0.1782437 Loss_f: 0.1787007 rho -0.00022174399 lambda: 0.01\n",
      "    Loss_i: 0.1782437 Loss_f: 0.17918853 rho -5.543791e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1782437 Loss_f: 0.17877562 rho -3.1877576e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1782437 Loss_f: 0.1783469 rho -6.198199e-08 lambda: 9.999999\n",
      "    Train error: 28.90868\n",
      "-- Epoch: 8  Batch: 30 --\n",
      "    Loss_i: 0.18143092 Loss_f: 0.20121509 rho -0.0109111015 lambda: 0.01\n",
      "    Loss_i: 0.18143092 Loss_f: 0.19903524 rho -0.0013544824 lambda: 0.099999994\n",
      "    Loss_i: 0.18143092 Loss_f: 0.18835595 rho -5.5473152e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18143092 Loss_f: 0.18119766 rho 1.8762759e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18119766 Loss_f: 0.18215358 rho -7.684478e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18119766 Loss_f: 0.18215358 rho -7.684478e-07 lambda: 10.0\n",
      "    Train error: 29.226316\n",
      "\n",
      "*** Epoch: 8 Train error: 29.67628618876139  Test error: 28.874775  Time: 710.0931212000005 sec\n",
      "\n",
      "-- Epoch: 9  Batch: 1 --\n",
      "    Loss_i: 0.17721958 Loss_f: 0.18043824 rho -0.0017458006 lambda: 0.01\n",
      "    Loss_i: 0.17721958 Loss_f: 0.18106034 rho -0.0002377598 lambda: 0.099999994\n",
      "    Loss_i: 0.17721958 Loss_f: 0.17870586 rho -9.332539e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17721958 Loss_f: 0.17711057 rho 6.855174e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17711057 Loss_f: 0.17730325 rho -1.2116776e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17711057 Loss_f: 0.17730325 rho -1.2116773e-07 lambda: 10.0\n",
      "    Train error: 28.47066\n",
      "-- Epoch: 9  Batch: 2 --\n",
      "    Loss_i: 0.17875893 Loss_f: 0.17810512 rho 0.00027801158 lambda: 0.01\n",
      "    Loss_i: 0.17810512 Loss_f: 0.18194352 rho -0.0015348145 lambda: 0.01\n",
      "    Loss_i: 0.17810512 Loss_f: 0.18202604 rho -0.00020886955 lambda: 0.099999994\n",
      "    Loss_i: 0.17810512 Loss_f: 0.180264 rho -1.1895665e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17810512 Loss_f: 0.17849073 rho -2.1320925e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17810512 Loss_f: 0.17849073 rho -2.1320925e-07 lambda: 10.0\n",
      "    Train error: 28.411331\n",
      "-- Epoch: 9  Batch: 3 --\n",
      "    Loss_i: 0.18128268 Loss_f: 0.23053154 rho -0.030419428 lambda: 0.01\n",
      "    Loss_i: 0.18128268 Loss_f: 0.22275852 rho -0.0030054701 lambda: 0.099999994\n",
      "    Loss_i: 0.18128268 Loss_f: 0.19788387 rho -0.00012241746 lambda: 0.99999994\n",
      "    Loss_i: 0.18128268 Loss_f: 0.18076994 rho 3.7876936e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18076994 Loss_f: 0.18310244 rho -1.7223139e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18076994 Loss_f: 0.18310244 rho -1.7223139e-06 lambda: 10.0\n",
      "    Train error: 29.265417\n",
      "-- Epoch: 9  Batch: 4 --\n",
      "    Loss_i: 0.17505242 Loss_f: 0.1991565 rho -0.011800632 lambda: 0.01\n",
      "    Loss_i: 0.17505242 Loss_f: 0.19528121 rho -0.0012253608 lambda: 0.099999994\n",
      "    Loss_i: 0.17505242 Loss_f: 0.18265553 rho -4.717543e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17505242 Loss_f: 0.17478831 rho 1.6427204e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17478831 Loss_f: 0.17581858 rho -6.408623e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17478831 Loss_f: 0.17581858 rho -6.408623e-07 lambda: 10.0\n",
      "    Train error: 27.875223\n",
      "-- Epoch: 9  Batch: 5 --\n",
      "    Loss_i: 0.17896701 Loss_f: 0.19146387 rho -0.0064910846 lambda: 0.01\n",
      "    Loss_i: 0.17896701 Loss_f: 0.190088 rho -0.00068809587 lambda: 0.099999994\n",
      "    Loss_i: 0.17896701 Loss_f: 0.18314049 rho -2.6326186e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17896701 Loss_f: 0.1788144 rho 9.6458976e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1788144 Loss_f: 0.17937425 rho -3.538151e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1788144 Loss_f: 0.17937425 rho -3.5381504e-07 lambda: 10.0\n",
      "    Train error: 28.975306\n",
      "-- Epoch: 9  Batch: 6 --\n",
      "    Loss_i: 0.17373934 Loss_f: 0.17773676 rho -0.0018253061 lambda: 0.01\n",
      "    Loss_i: 0.17373934 Loss_f: 0.17807537 rho -0.00024101883 lambda: 0.099999994\n",
      "    Loss_i: 0.17373934 Loss_f: 0.17537484 rho -9.292848e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17373934 Loss_f: 0.17365533 rho 4.7842242e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17365533 Loss_f: 0.1738641 rho -1.1890336e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17365533 Loss_f: 0.1738641 rho -1.1890336e-07 lambda: 10.0\n",
      "    Train error: 27.764193\n",
      "-- Epoch: 9  Batch: 7 --\n",
      "    Loss_i: 0.18304719 Loss_f: 0.1825853 rho 0.00024872104 lambda: 0.01\n",
      "    Loss_i: 0.1825853 Loss_f: 0.18744601 rho -0.0026611716 lambda: 0.01\n",
      "    Loss_i: 0.1825853 Loss_f: 0.18775529 rho -0.00032481164 lambda: 0.099999994\n",
      "    Loss_i: 0.1825853 Loss_f: 0.18562971 rho -1.9413368e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1825853 Loss_f: 0.18315358 rho -3.629235e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1825853 Loss_f: 0.18315358 rho -3.6292346e-07 lambda: 10.0\n",
      "    Train error: 29.895794\n",
      "-- Epoch: 9  Batch: 8 --\n",
      "    Loss_i: 0.1828155 Loss_f: 0.29729706 rho -0.078244455 lambda: 0.01\n",
      "    Loss_i: 0.1828155 Loss_f: 0.285099 rho -0.008860233 lambda: 0.099999994\n",
      "    Loss_i: 0.1828155 Loss_f: 0.23273689 rho -0.00044432224 lambda: 0.99999994\n",
      "    Loss_i: 0.1828155 Loss_f: 0.18220109 rho 5.483673e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18220109 Loss_f: 0.19048321 rho -7.3730453e-06 lambda: 9.999999\n",
      "    Loss_i: 0.18220109 Loss_f: 0.19048321 rho -7.3730444e-06 lambda: 10.0\n",
      "    Train error: 29.58518\n",
      "-- Epoch: 9  Batch: 9 --\n",
      "    Loss_i: 0.17820275 Loss_f: 0.24710372 rho -0.035587076 lambda: 0.01\n",
      "    Loss_i: 0.17820275 Loss_f: 0.24037388 rho -0.0038472344 lambda: 0.099999994\n",
      "    Loss_i: 0.17820275 Loss_f: 0.20900464 rho -0.00019445848 lambda: 0.99999994\n",
      "    Loss_i: 0.17820275 Loss_f: 0.17785756 rho 2.183637e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17785756 Loss_f: 0.18321432 rho -3.3903991e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17785756 Loss_f: 0.18321432 rho -3.3903987e-06 lambda: 10.0\n",
      "    Train error: 28.845524\n",
      "-- Epoch: 9  Batch: 10 --\n",
      "    Loss_i: 0.17859487 Loss_f: 0.23273806 rho -0.028238866 lambda: 0.01\n",
      "    Loss_i: 0.17859487 Loss_f: 0.2254002 rho -0.0027650748 lambda: 0.099999994\n",
      "    Loss_i: 0.17859487 Loss_f: 0.19793227 rho -0.000115773895 lambda: 0.99999994\n",
      "    Loss_i: 0.17859487 Loss_f: 0.17829159 rho 1.8182166e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17829159 Loss_f: 0.18109566 rho -1.6796871e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17829159 Loss_f: 0.18109566 rho -1.6796868e-06 lambda: 10.0\n",
      "    Train error: 28.700558\n",
      "-- Epoch: 9  Batch: 11 --\n",
      "    Loss_i: 0.17785609 Loss_f: 0.20401834 rho -0.011782856 lambda: 0.01\n",
      "    Loss_i: 0.17785609 Loss_f: 0.2015157 rho -0.0012411689 lambda: 0.099999994\n",
      "    Loss_i: 0.17785609 Loss_f: 0.18800144 rho -5.4113654e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17785609 Loss_f: 0.17772746 rho 6.872261e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17772746 Loss_f: 0.17928272 rho -8.3139895e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17772746 Loss_f: 0.17928274 rho -8.314068e-07 lambda: 10.0\n",
      "    Train error: 28.739042\n",
      "-- Epoch: 9  Batch: 12 --\n",
      "    Loss_i: 0.18265769 Loss_f: 0.19357812 rho -0.0049993587 lambda: 0.01\n",
      "    Loss_i: 0.18265769 Loss_f: 0.19282012 rho -0.00058246404 lambda: 0.099999994\n",
      "    Loss_i: 0.18265769 Loss_f: 0.18670936 rho -2.3822626e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.18265769 Loss_f: 0.18256183 rho 5.6508416e-08 lambda: 9.999999\n",
      "    Loss_i: 0.18256183 Loss_f: 0.18313725 rho -3.389708e-07 lambda: 9.999999\n",
      "    Loss_i: 0.18256183 Loss_f: 0.18313725 rho -3.3897072e-07 lambda: 10.0\n",
      "    Train error: 29.641956\n",
      "-- Epoch: 9  Batch: 13 --\n",
      "    Loss_i: 0.17337249 Loss_f: 0.17585886 rho -0.001019369 lambda: 0.01\n",
      "    Loss_i: 0.17337249 Loss_f: 0.17647399 rho -0.00016409044 lambda: 0.099999994\n",
      "    Loss_i: 0.17337249 Loss_f: 0.1746885 rho -7.170887e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17337249 Loss_f: 0.17334327 rho 1.5970269e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17334327 Loss_f: 0.17353432 rho -1.0441247e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17334327 Loss_f: 0.17353432 rho -1.0441246e-07 lambda: 10.0\n",
      "    Train error: 27.825676\n",
      "-- Epoch: 9  Batch: 14 --\n",
      "    Loss_i: 0.17735523 Loss_f: 0.17907372 rho -0.0008363663 lambda: 0.01\n",
      "    Loss_i: 0.17735523 Loss_f: 0.17988028 rho -0.00015520616 lambda: 0.099999994\n",
      "    Loss_i: 0.17735523 Loss_f: 0.17844293 rho -6.866255e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17735523 Loss_f: 0.17732653 rho 1.8166169e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17732653 Loss_f: 0.17748274 rho -9.885224e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17732653 Loss_f: 0.17748274 rho -9.885223e-08 lambda: 10.0\n",
      "    Train error: 28.373793\n",
      "-- Epoch: 9  Batch: 15 --\n",
      "    Loss_i: 0.17635722 Loss_f: 0.17612834 rho 9.829269e-05 lambda: 0.01\n",
      "    Loss_i: 0.17612834 Loss_f: 0.17939918 rho -0.0014103085 lambda: 0.01\n",
      "    Loss_i: 0.17612834 Loss_f: 0.17976534 rho -0.00017140388 lambda: 0.099999994\n",
      "    Loss_i: 0.17612834 Loss_f: 0.17836416 rho -1.0635822e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17612834 Loss_f: 0.17656107 rho -2.0604395e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17612834 Loss_f: 0.17656107 rho -2.0604392e-07 lambda: 10.0\n",
      "    Train error: 28.30977\n",
      "-- Epoch: 9  Batch: 16 --\n",
      "    Loss_i: 0.17726728 Loss_f: 0.27814502 rho -0.049032245 lambda: 0.01\n",
      "    Loss_i: 0.17726728 Loss_f: 0.2659138 rho -0.0064876257 lambda: 0.099999994\n",
      "    Loss_i: 0.17726728 Loss_f: 0.2176836 rho -0.00031154277 lambda: 0.99999994\n",
      "    Loss_i: 0.17726728 Loss_f: 0.17671442 rho 4.2844755e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17671442 Loss_f: 0.18302375 rho -4.8876323e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17671442 Loss_f: 0.18302375 rho -4.887632e-06 lambda: 10.0\n",
      "    Train error: 28.186937\n",
      "-- Epoch: 9  Batch: 17 --\n",
      "    Loss_i: 0.17544249 Loss_f: 0.23180883 rho -0.02413158 lambda: 0.01\n",
      "    Loss_i: 0.17544249 Loss_f: 0.22551806 rho -0.002516692 lambda: 0.099999994\n",
      "    Loss_i: 0.17544249 Loss_f: 0.19768849 rho -0.00011378263 lambda: 0.99999994\n",
      "    Loss_i: 0.17544249 Loss_f: 0.17516805 rho 1.4061521e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17516805 Loss_f: 0.17860807 rho -1.7623413e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17516805 Loss_f: 0.17860807 rho -1.7623412e-06 lambda: 10.0\n",
      "    Train error: 27.96006\n",
      "-- Epoch: 9  Batch: 18 --\n",
      "    Loss_i: 0.17733856 Loss_f: 0.22011434 rho -0.018518187 lambda: 0.01\n",
      "    Loss_i: 0.17733856 Loss_f: 0.21540669 rho -0.002028656 lambda: 0.099999994\n",
      "    Loss_i: 0.17733856 Loss_f: 0.19326466 rho -8.6877e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17733856 Loss_f: 0.17719595 rho 7.797498e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17719595 Loss_f: 0.17954771 rho -1.2853234e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17719595 Loss_f: 0.17954771 rho -1.2853232e-06 lambda: 10.0\n",
      "    Train error: 28.13369\n",
      "-- Epoch: 9  Batch: 19 --\n",
      "    Loss_i: 0.17960447 Loss_f: 0.2028703 rho -0.011314527 lambda: 0.01\n",
      "    Loss_i: 0.17960447 Loss_f: 0.20011291 rho -0.0011634468 lambda: 0.099999994\n",
      "    Loss_i: 0.17960447 Loss_f: 0.1878942 rho -4.7824138e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17960447 Loss_f: 0.17950211 rho 5.9150288e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17950211 Loss_f: 0.1806891 rho -6.862506e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17950211 Loss_f: 0.18068908 rho -6.862419e-07 lambda: 10.0\n",
      "    Train error: 29.016506\n",
      "-- Epoch: 9  Batch: 20 --\n",
      "    Loss_i: 0.17512508 Loss_f: 0.1831763 rho -0.003911437 lambda: 0.01\n",
      "    Loss_i: 0.17512508 Loss_f: 0.18285128 rho -0.00042494573 lambda: 0.099999994\n",
      "    Loss_i: 0.17512508 Loss_f: 0.1781389 rho -1.6798109e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17512508 Loss_f: 0.17506322 rho 3.452216e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17506322 Loss_f: 0.17547788 rho -2.312337e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17506322 Loss_f: 0.17547788 rho -2.3123366e-07 lambda: 10.0\n",
      "    Train error: 27.797882\n",
      "-- Epoch: 9  Batch: 21 --\n",
      "    Loss_i: 0.1775285 Loss_f: 0.18001519 rho -0.0010810466 lambda: 0.01\n",
      "    Loss_i: 0.1775285 Loss_f: 0.18055727 rho -0.00016666166 lambda: 0.099999994\n",
      "    Loss_i: 0.1775285 Loss_f: 0.1787897 rho -7.1293985e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1775285 Loss_f: 0.17749767 rho 1.7475736e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17749767 Loss_f: 0.17767903 rho -1.02790466e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17749767 Loss_f: 0.17767903 rho -1.0279046e-07 lambda: 10.0\n",
      "    Train error: 28.50258\n",
      "-- Epoch: 9  Batch: 22 --\n",
      "    Loss_i: 0.17378761 Loss_f: 0.17375676 rho 1.2232742e-05 lambda: 0.01\n",
      "    Loss_i: 0.17375676 Loss_f: 0.17677197 rho -0.0011282071 lambda: 0.01\n",
      "    Loss_i: 0.17375676 Loss_f: 0.17705627 rho -0.00014226833 lambda: 0.099999994\n",
      "    Loss_i: 0.17375676 Loss_f: 0.17570615 rho -8.535422e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17375676 Loss_f: 0.1741239 rho -1.6099985e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17375676 Loss_f: 0.1741239 rho -1.6099983e-07 lambda: 10.0\n",
      "    Train error: 27.357426\n",
      "-- Epoch: 9  Batch: 23 --\n",
      "    Loss_i: 0.17972752 Loss_f: 0.2650872 rho -0.04863023 lambda: 0.01\n",
      "    Loss_i: 0.17972752 Loss_f: 0.25430462 rho -0.0054884967 lambda: 0.099999994\n",
      "    Loss_i: 0.17972752 Loss_f: 0.21469352 rho -0.0002650665 lambda: 0.99999994\n",
      "    Loss_i: 0.17972752 Loss_f: 0.1792207 rho 3.8536183e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1792207 Loss_f: 0.18476397 rho -4.219639e-06 lambda: 9.999999\n",
      "    Loss_i: 0.1792207 Loss_f: 0.18476395 rho -4.219627e-06 lambda: 10.0\n",
      "    Train error: 28.79461\n",
      "-- Epoch: 9  Batch: 24 --\n",
      "    Loss_i: 0.17877124 Loss_f: 0.2356067 rho -0.027277319 lambda: 0.01\n",
      "    Loss_i: 0.17877124 Loss_f: 0.22839531 rho -0.0029337178 lambda: 0.099999994\n",
      "    Loss_i: 0.17877124 Loss_f: 0.20017493 rho -0.000129539 lambda: 0.99999994\n",
      "    Loss_i: 0.17877124 Loss_f: 0.17854486 rho 1.3733424e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17854486 Loss_f: 0.18177256 rho -1.9544404e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17854486 Loss_f: 0.18177256 rho -1.9544404e-06 lambda: 10.0\n",
      "    Train error: 28.515064\n",
      "-- Epoch: 9  Batch: 25 --\n",
      "    Loss_i: 0.1800656 Loss_f: 0.21129954 rho -0.013930162 lambda: 0.01\n",
      "    Loss_i: 0.1800656 Loss_f: 0.20787185 rho -0.0016535762 lambda: 0.099999994\n",
      "    Loss_i: 0.1800656 Loss_f: 0.19187297 rho -7.2637326e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1800656 Loss_f: 0.17993979 rho 7.766477e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17993979 Loss_f: 0.18172078 rho -1.0991698e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17993979 Loss_f: 0.18172078 rho -1.0991697e-06 lambda: 10.0\n",
      "    Train error: 29.118053\n",
      "-- Epoch: 9  Batch: 26 --\n",
      "    Loss_i: 0.17511773 Loss_f: 0.19227219 rho -0.0072649424 lambda: 0.01\n",
      "    Loss_i: 0.17511773 Loss_f: 0.19088784 rho -0.00086195354 lambda: 0.099999994\n",
      "    Loss_i: 0.17511773 Loss_f: 0.18161966 rho -3.6601537e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17511773 Loss_f: 0.17505349 rho 3.6270798e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17505349 Loss_f: 0.17600873 rho -5.3893535e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17505349 Loss_f: 0.17600873 rho -5.389353e-07 lambda: 10.0\n",
      "    Train error: 27.856247\n",
      "-- Epoch: 9  Batch: 27 --\n",
      "    Loss_i: 0.17369772 Loss_f: 0.1793275 rho -0.002514484 lambda: 0.01\n",
      "    Loss_i: 0.17369772 Loss_f: 0.17931265 rho -0.0002918587 lambda: 0.099999994\n",
      "    Loss_i: 0.17369772 Loss_f: 0.17593148 rho -1.1804214e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17369772 Loss_f: 0.17364585 rho 2.745672e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17364585 Loss_f: 0.17395978 rho -1.6608223e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17364585 Loss_f: 0.17395978 rho -1.660822e-07 lambda: 10.0\n",
      "    Train error: 27.531654\n",
      "-- Epoch: 9  Batch: 28 --\n",
      "    Loss_i: 0.1732676 Loss_f: 0.17394522 rho -0.0002892222 lambda: 0.01\n",
      "    Loss_i: 0.1732676 Loss_f: 0.17460686 rho -6.46246e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1732676 Loss_f: 0.17382567 rho -2.728501e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1732676 Loss_f: 0.17324604 rho 1.0556119e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17324604 Loss_f: 0.17332259 rho -3.746982e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17324604 Loss_f: 0.17332259 rho -3.7469814e-08 lambda: 10.0\n",
      "    Train error: 27.38527\n",
      "-- Epoch: 9  Batch: 29 --\n",
      "    Loss_i: 0.17519754 Loss_f: 0.17403004 rho 0.00049414404 lambda: 0.01\n",
      "    Loss_i: 0.17403004 Loss_f: 0.17362371 rho 0.00017000048 lambda: 0.01\n",
      "    Loss_i: 0.17362371 Loss_f: 0.1739854 rho -0.00014937962 lambda: 0.01\n",
      "    Loss_i: 0.17362371 Loss_f: 0.17434227 rho -3.415061e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17362371 Loss_f: 0.17404006 rho -2.0090583e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17362371 Loss_f: 0.17370807 rho -4.0767024e-08 lambda: 9.999999\n",
      "    Train error: 27.677626\n",
      "-- Epoch: 9  Batch: 30 --\n",
      "    Loss_i: 0.17669232 Loss_f: 0.19935247 rho -0.0100262305 lambda: 0.01\n",
      "    Loss_i: 0.17669232 Loss_f: 0.1968693 rho -0.0011914263 lambda: 0.099999994\n",
      "    Loss_i: 0.17669232 Loss_f: 0.18487085 rho -4.9964834e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17669232 Loss_f: 0.17650025 rho 1.1775192e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17650025 Loss_f: 0.17767823 rho -7.209292e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17650025 Loss_f: 0.17767823 rho -7.209292e-07 lambda: 10.0\n",
      "    Train error: 27.953598\n",
      "\n",
      "*** Epoch: 9 Train error: 28.415554173787434  Test error: 27.625225  Time: 720.7238385 sec\n",
      "\n",
      "-- Epoch: 10  Batch: 1 --\n",
      "    Loss_i: 0.17250589 Loss_f: 0.17738761 rho -0.002298004 lambda: 0.01\n",
      "    Loss_i: 0.17250589 Loss_f: 0.17755006 rho -0.00026899483 lambda: 0.099999994\n",
      "    Loss_i: 0.17250589 Loss_f: 0.17443523 rho -1.0427299e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17250589 Loss_f: 0.17242502 rho 4.3765056e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17242502 Loss_f: 0.17268054 rho -1.3831617e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17242502 Loss_f: 0.17268057 rho -1.3833227e-07 lambda: 10.0\n",
      "    Train error: 27.223972\n",
      "-- Epoch: 10  Batch: 2 --\n",
      "    Loss_i: 0.17419527 Loss_f: 0.17358257 rho 0.00025156653 lambda: 0.01\n",
      "    Loss_i: 0.17358257 Loss_f: 0.1763215 rho -0.0010559974 lambda: 0.01\n",
      "    Loss_i: 0.17358257 Loss_f: 0.17656268 rho -0.0001337916 lambda: 0.099999994\n",
      "    Loss_i: 0.17358257 Loss_f: 0.17531374 rho -7.902003e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17358257 Loss_f: 0.17390305 rho -1.4652902e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17358257 Loss_f: 0.17390305 rho -1.4652902e-07 lambda: 10.0\n",
      "    Train error: 27.257025\n",
      "-- Epoch: 10  Batch: 3 --\n",
      "    Loss_i: 0.17661667 Loss_f: 0.2407363 rho -0.0259878 lambda: 0.01\n",
      "    Loss_i: 0.17661667 Loss_f: 0.23263937 rho -0.003050727 lambda: 0.099999994\n",
      "    Loss_i: 0.17661667 Loss_f: 0.20101131 rho -0.00013756791 lambda: 0.99999994\n",
      "    Loss_i: 0.17661667 Loss_f: 0.17628014 rho 1.9045471e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17628014 Loss_f: 0.17996956 rho -2.0900193e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17628014 Loss_f: 0.17996956 rho -2.090019e-06 lambda: 10.0\n",
      "    Train error: 28.148392\n",
      "-- Epoch: 10  Batch: 4 --\n",
      "    Loss_i: 0.17083338 Loss_f: 0.20419599 rho -0.014965929 lambda: 0.01\n",
      "    Loss_i: 0.17083338 Loss_f: 0.19960745 rho -0.0015929398 lambda: 0.099999994\n",
      "    Loss_i: 0.17083338 Loss_f: 0.18219982 rho -6.4433385e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17083338 Loss_f: 0.17049584 rho 1.9180307e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17049584 Loss_f: 0.17207882 rho -8.987322e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17049584 Loss_f: 0.17207882 rho -8.987321e-07 lambda: 10.0\n",
      "    Train error: 26.77307\n",
      "-- Epoch: 10  Batch: 5 --\n",
      "    Loss_i: 0.17452097 Loss_f: 0.19000259 rho -0.0053789825 lambda: 0.01\n",
      "    Loss_i: 0.17452097 Loss_f: 0.1884221 rho -0.0006443224 lambda: 0.099999994\n",
      "    Loss_i: 0.17452097 Loss_f: 0.18011029 rho -2.6801994e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17452097 Loss_f: 0.17443912 rho 3.9385952e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17443912 Loss_f: 0.17523752 rho -3.8410118e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17443912 Loss_f: 0.17523752 rho -3.8410116e-07 lambda: 10.0\n",
      "    Train error: 27.853586\n",
      "-- Epoch: 10  Batch: 6 --\n",
      "    Loss_i: 0.16925263 Loss_f: 0.1763298 rho -0.0027871372 lambda: 0.01\n",
      "    Loss_i: 0.16925263 Loss_f: 0.17609054 rho -0.00031171454 lambda: 0.099999994\n",
      "    Loss_i: 0.16925263 Loss_f: 0.1718519 rho -1.2038754e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16925263 Loss_f: 0.16917585 rho 3.5621017e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16917585 Loss_f: 0.16952153 rho -1.6029678e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16917585 Loss_f: 0.16952153 rho -1.6029674e-07 lambda: 10.0\n",
      "    Train error: 26.606913\n",
      "-- Epoch: 10  Batch: 7 --\n",
      "    Loss_i: 0.17834578 Loss_f: 0.17897685 rho -0.00029000643 lambda: 0.01\n",
      "    Loss_i: 0.17834578 Loss_f: 0.17996779 rho -8.671062e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17834578 Loss_f: 0.17902116 rho -3.6704382e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17834578 Loss_f: 0.1783176 rho 1.5339202e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1783176 Loss_f: 0.17840876 rho -4.9621832e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1783176 Loss_f: 0.17840876 rho -4.962183e-08 lambda: 10.0\n",
      "    Train error: 28.510263\n",
      "-- Epoch: 10  Batch: 8 --\n",
      "    Loss_i: 0.17755441 Loss_f: 0.17689748 rho 0.00028162045 lambda: 0.01\n",
      "    Loss_i: 0.17689748 Loss_f: 0.17848925 rho -0.0006533507 lambda: 0.01\n",
      "    Loss_i: 0.17689748 Loss_f: 0.1789139 rho -9.8460245e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17689748 Loss_f: 0.17811073 rho -6.038717e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17689748 Loss_f: 0.17712426 rho -1.1309412e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17689748 Loss_f: 0.17712426 rho -1.1309412e-07 lambda: 10.0\n",
      "    Train error: 28.276537\n",
      "-- Epoch: 10  Batch: 9 --\n",
      "    Loss_i: 0.17374283 Loss_f: 0.2179766 rho -0.0182462 lambda: 0.01\n",
      "    Loss_i: 0.17374283 Loss_f: 0.21288444 rho -0.001967717 lambda: 0.099999994\n",
      "    Loss_i: 0.17374283 Loss_f: 0.1908935 rho -8.8147455e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17374283 Loss_f: 0.17347847 rho 1.3617557e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17347847 Loss_f: 0.17612714 rho -1.3651622e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17347847 Loss_f: 0.17612714 rho -1.3651622e-06 lambda: 10.0\n",
      "    Train error: 27.669928\n",
      "-- Epoch: 10  Batch: 10 --\n",
      "    Loss_i: 0.17386757 Loss_f: 0.19748059 rho -0.010071662 lambda: 0.01\n",
      "    Loss_i: 0.17386757 Loss_f: 0.19495124 rho -0.0010097563 lambda: 0.099999994\n",
      "    Loss_i: 0.17386757 Loss_f: 0.18251982 rho -4.1953466e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17386757 Loss_f: 0.17368546 rho 8.841094e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17368546 Loss_f: 0.1749414 rho -6.0891233e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17368546 Loss_f: 0.1749414 rho -6.0891233e-07 lambda: 10.0\n",
      "    Train error: 27.55589\n",
      "-- Epoch: 10  Batch: 11 --\n",
      "    Loss_i: 0.17326698 Loss_f: 0.18528308 rho -0.0039569587 lambda: 0.01\n",
      "    Loss_i: 0.17326698 Loss_f: 0.18438816 rho -0.00047718248 lambda: 0.099999994\n",
      "    Loss_i: 0.17326698 Loss_f: 0.17768158 rho -1.9533787e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17326698 Loss_f: 0.1731935 rho 3.2614366e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1731935 Loss_f: 0.17381674 rho -2.7651657e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1731935 Loss_f: 0.17381674 rho -2.7651657e-07 lambda: 10.0\n",
      "    Train error: 27.54107\n",
      "-- Epoch: 10  Batch: 12 --\n",
      "    Loss_i: 0.17843132 Loss_f: 0.1814676 rho -0.0012303649 lambda: 0.01\n",
      "    Loss_i: 0.17843132 Loss_f: 0.181794 rho -0.00016612251 lambda: 0.099999994\n",
      "    Loss_i: 0.17843132 Loss_f: 0.17979948 rho -6.910399e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17843132 Loss_f: 0.17839174 rho 2.0034891e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17839174 Loss_f: 0.17858598 rho -9.8320086e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17839174 Loss_f: 0.17858599 rho -9.8327625e-08 lambda: 10.0\n",
      "    Train error: 28.58224\n",
      "-- Epoch: 10  Batch: 13 --\n",
      "    Loss_i: 0.16903815 Loss_f: 0.16904439 rho -2.3585224e-06 lambda: 0.01\n",
      "    Loss_i: 0.16903815 Loss_f: 0.1698455 rho -3.398237e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16903815 Loss_f: 0.16939911 rho -1.5368904e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16903815 Loss_f: 0.16901895 rho 8.181163e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16901895 Loss_f: 0.1690675 rho -2.0695555e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16901895 Loss_f: 0.1690675 rho -2.0695555e-08 lambda: 10.0\n",
      "    Train error: 26.718937\n",
      "-- Epoch: 10  Batch: 14 --\n",
      "    Loss_i: 0.17286596 Loss_f: 0.17305872 rho -7.885028e-05 lambda: 0.01\n",
      "    Loss_i: 0.17286596 Loss_f: 0.17381553 rho -4.6162448e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17286596 Loss_f: 0.17328079 rho -2.0553905e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17286596 Loss_f: 0.17285447 rho 5.703348e-09 lambda: 9.999999\n",
      "    Loss_i: 0.17285447 Loss_f: 0.17291243 rho -2.8769634e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17285447 Loss_f: 0.17291243 rho -2.8769634e-08 lambda: 10.0\n",
      "    Train error: 27.17832\n",
      "-- Epoch: 10  Batch: 15 --\n",
      "    Loss_i: 0.17199181 Loss_f: 0.17112586 rho 0.00028499193 lambda: 0.01\n",
      "    Loss_i: 0.17112586 Loss_f: 0.17161505 rho -0.00015942853 lambda: 0.01\n",
      "    Loss_i: 0.17112586 Loss_f: 0.17216267 rho -4.0530595e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17112586 Loss_f: 0.17179444 rho -2.6668135e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17112586 Loss_f: 0.1712543 rho -5.1339086e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17112586 Loss_f: 0.1712543 rho -5.133908e-08 lambda: 10.0\n",
      "    Train error: 26.867403\n",
      "-- Epoch: 10  Batch: 16 --\n",
      "    Loss_i: 0.17255573 Loss_f: 0.20445548 rho -0.01393484 lambda: 0.01\n",
      "    Loss_i: 0.17255573 Loss_f: 0.20097458 rho -0.0014040505 lambda: 0.099999994\n",
      "    Loss_i: 0.17255573 Loss_f: 0.18425342 rho -5.8560294e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17255573 Loss_f: 0.172432 rho 6.2020455e-08 lambda: 9.999999\n",
      "    Loss_i: 0.172432 Loss_f: 0.17413048 rho -8.5127266e-07 lambda: 9.999999\n",
      "    Loss_i: 0.172432 Loss_f: 0.17413048 rho -8.512726e-07 lambda: 10.0\n",
      "    Train error: 27.164082\n",
      "-- Epoch: 10  Batch: 17 --\n",
      "    Loss_i: 0.17121118 Loss_f: 0.18851271 rho -0.0058822515 lambda: 0.01\n",
      "    Loss_i: 0.17121118 Loss_f: 0.18679217 rho -0.00068148423 lambda: 0.099999994\n",
      "    Loss_i: 0.17121118 Loss_f: 0.17750949 rho -2.8360084e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17121118 Loss_f: 0.17110129 rho 4.9630522e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17110129 Loss_f: 0.17200512 rho -4.0849082e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17110129 Loss_f: 0.17200512 rho -4.084908e-07 lambda: 10.0\n",
      "    Train error: 26.830944\n",
      "-- Epoch: 10  Batch: 18 --\n",
      "    Loss_i: 0.17323941 Loss_f: 0.18180515 rho -0.0036815102 lambda: 0.01\n",
      "    Loss_i: 0.17323941 Loss_f: 0.18137404 rho -0.00041349576 lambda: 0.099999994\n",
      "    Loss_i: 0.17323941 Loss_f: 0.17644097 rho -1.6576858e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17323941 Loss_f: 0.17316876 rho 3.664711e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17316876 Loss_f: 0.17361487 rho -2.3127286e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17316876 Loss_f: 0.17361487 rho -2.3127286e-07 lambda: 10.0\n",
      "    Train error: 27.163519\n",
      "-- Epoch: 10  Batch: 19 --\n",
      "    Loss_i: 0.17546602 Loss_f: 0.17639533 rho -0.00036099475 lambda: 0.01\n",
      "    Loss_i: 0.17546602 Loss_f: 0.17699479 rho -7.09463e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17546602 Loss_f: 0.17610428 rho -3.0208148e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17546602 Loss_f: 0.17543837 rho 1.3108487e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17543837 Loss_f: 0.17552747 rho -4.2249646e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17543837 Loss_f: 0.17552747 rho -4.2249635e-08 lambda: 10.0\n",
      "    Train error: 27.911226\n",
      "-- Epoch: 10  Batch: 20 --\n",
      "    Loss_i: 0.17118284 Loss_f: 0.17052205 rho 0.00020854925 lambda: 0.01\n",
      "    Loss_i: 0.17052205 Loss_f: 0.17071003 rho -5.779076e-05 lambda: 0.01\n",
      "    Loss_i: 0.17052205 Loss_f: 0.17122959 rho -2.6867845e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17052205 Loss_f: 0.17096058 rho -1.7053577e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17052205 Loss_f: 0.17060415 rho -3.2006536e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17052205 Loss_f: 0.17060415 rho -3.200653e-08 lambda: 10.0\n",
      "    Train error: 26.615376\n",
      "-- Epoch: 10  Batch: 21 --\n",
      "    Loss_i: 0.1732911 Loss_f: 0.18909954 rho -0.0053136903 lambda: 0.01\n",
      "    Loss_i: 0.1732911 Loss_f: 0.18787105 rho -0.00067507796 lambda: 0.099999994\n",
      "    Loss_i: 0.1732911 Loss_f: 0.17929074 rho -2.886921e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1732911 Loss_f: 0.1731354 rho 7.521628e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1731354 Loss_f: 0.17401065 rho -4.2289517e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1731354 Loss_f: 0.17401065 rho -4.2289514e-07 lambda: 10.0\n",
      "    Train error: 27.421171\n",
      "-- Epoch: 10  Batch: 22 --\n",
      "    Loss_i: 0.16936737 Loss_f: 0.1764819 rho -0.0027089359 lambda: 0.01\n",
      "    Loss_i: 0.16936737 Loss_f: 0.17619435 rho -0.00029363003 lambda: 0.099999994\n",
      "    Loss_i: 0.16936737 Loss_f: 0.17206569 rho -1.1757892e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16936737 Loss_f: 0.16930723 rho 2.6240869e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16930723 Loss_f: 0.16968422 rho -1.6441366e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16930723 Loss_f: 0.16968423 rho -1.6442014e-07 lambda: 10.0\n",
      "    Train error: 26.38537\n",
      "-- Epoch: 10  Batch: 23 --\n",
      "    Loss_i: 0.17492671 Loss_f: 0.17770952 rho -0.001054236 lambda: 0.01\n",
      "    Loss_i: 0.17492671 Loss_f: 0.1780517 rho -0.0001551112 lambda: 0.099999994\n",
      "    Loss_i: 0.17492671 Loss_f: 0.1762241 rho -6.645843e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17492671 Loss_f: 0.17490667 rho 1.0299505e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17490667 Loss_f: 0.17509367 rho -9.6078736e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17490667 Loss_f: 0.17509367 rho -9.607873e-08 lambda: 10.0\n",
      "    Train error: 27.533985\n",
      "-- Epoch: 10  Batch: 24 --\n",
      "    Loss_i: 0.17441681 Loss_f: 0.17541474 rho -0.00036223803 lambda: 0.01\n",
      "    Loss_i: 0.17441681 Loss_f: 0.17587118 rho -6.309487e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17441681 Loss_f: 0.17501985 rho -2.6682198e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17441681 Loss_f: 0.17440186 rho 6.6262134e-09 lambda: 9.999999\n",
      "    Loss_i: 0.17440186 Loss_f: 0.17448704 rho -3.7754166e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17440186 Loss_f: 0.17448704 rho -3.7754162e-08 lambda: 10.0\n",
      "    Train error: 27.510674\n",
      "-- Epoch: 10  Batch: 25 --\n",
      "    Loss_i: 0.17583427 Loss_f: 0.17543578 rho 0.00013103266 lambda: 0.01\n",
      "    Loss_i: 0.17543578 Loss_f: 0.17627089 rho -0.00027361908 lambda: 0.01\n",
      "    Loss_i: 0.17543578 Loss_f: 0.17668535 rho -4.8618822e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17543578 Loss_f: 0.17619883 rho -3.0256278e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17543578 Loss_f: 0.17558065 rho -5.7553624e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17543578 Loss_f: 0.17558065 rho -5.7553617e-08 lambda: 10.0\n",
      "    Train error: 27.972439\n",
      "-- Epoch: 10  Batch: 26 --\n",
      "    Loss_i: 0.17114262 Loss_f: 0.20753771 rho -0.01333745 lambda: 0.01\n",
      "    Loss_i: 0.17114262 Loss_f: 0.20316638 rho -0.0013679803 lambda: 0.099999994\n",
      "    Loss_i: 0.17114262 Loss_f: 0.18383947 rho -5.5151708e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17114262 Loss_f: 0.17096464 rho 7.743994e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17096464 Loss_f: 0.17273007 rho -7.68389e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17096464 Loss_f: 0.17273007 rho -7.6838893e-07 lambda: 10.0\n",
      "    Train error: 26.76595\n",
      "-- Epoch: 10  Batch: 27 --\n",
      "    Loss_i: 0.16978504 Loss_f: 0.18244776 rho -0.004679392 lambda: 0.01\n",
      "    Loss_i: 0.16978504 Loss_f: 0.1813757 rho -0.00051847077 lambda: 0.099999994\n",
      "    Loss_i: 0.16978504 Loss_f: 0.17437854 rho -2.0989331e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16978504 Loss_f: 0.1696877 rho 4.4571316e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1696877 Loss_f: 0.17033045 rho -2.941502e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1696877 Loss_f: 0.17033045 rho -2.941502e-07 lambda: 10.0\n",
      "    Train error: 26.522512\n",
      "-- Epoch: 10  Batch: 28 --\n",
      "    Loss_i: 0.16900443 Loss_f: 0.17328532 rho -0.0015389981 lambda: 0.01\n",
      "    Loss_i: 0.16900443 Loss_f: 0.17337416 rho -0.00017779434 lambda: 0.099999994\n",
      "    Loss_i: 0.16900443 Loss_f: 0.1707521 rho -7.2057983e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16900443 Loss_f: 0.16896127 rho 1.7816498e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16896127 Loss_f: 0.16920811 rho -1.0186198e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16896127 Loss_f: 0.16920811 rho -1.0186196e-07 lambda: 10.0\n",
      "    Train error: 26.265808\n",
      "-- Epoch: 10  Batch: 29 --\n",
      "    Loss_i: 0.17075196 Loss_f: 0.17090888 rho -5.191627e-05 lambda: 0.01\n",
      "    Loss_i: 0.17075196 Loss_f: 0.1714779 rho -2.7048594e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17075196 Loss_f: 0.1710698 rho -1.1994231e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17075196 Loss_f: 0.1707401 rho 4.4817776e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1707401 Loss_f: 0.17078568 rho -1.7222874e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1707401 Loss_f: 0.17078568 rho -1.7222874e-08 lambda: 10.0\n",
      "    Train error: 26.713219\n",
      "-- Epoch: 10  Batch: 30 --\n",
      "    Loss_i: 0.17295371 Loss_f: 0.17138584 rho 0.00050277467 lambda: 0.01\n",
      "    Loss_i: 0.17138584 Loss_f: 0.17112449 rho 8.246506e-05 lambda: 0.01\n",
      "    Loss_i: 0.17112449 Loss_f: 0.17218286 rho -0.0003283226 lambda: 0.01\n",
      "    Loss_i: 0.17112449 Loss_f: 0.1722997 rho -5.232206e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17112449 Loss_f: 0.17169693 rho -2.664554e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17112449 Loss_f: 0.17122878 rho -4.8767323e-08 lambda: 9.999999\n",
      "    Train error: 26.623623\n",
      "\n",
      "*** Epoch: 10 Train error: 27.272114690144857  Test error: 27.062656  Time: 711.4619665999999 sec\n",
      "\n",
      "-- Epoch: 11  Batch: 1 --\n",
      "    Loss_i: 0.16921808 Loss_f: 0.17350891 rho -0.0015585736 lambda: 0.01\n",
      "    Loss_i: 0.16921808 Loss_f: 0.17330039 rho -0.00018922752 lambda: 0.099999994\n",
      "    Loss_i: 0.16921808 Loss_f: 0.17010744 rho -4.2395245e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16921808 Loss_f: 0.16876857 rho 2.1488789e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16876857 Loss_f: 0.16874157 rho 1.2905572e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16874157 Loss_f: 0.1691389 rho -1.8987365e-07 lambda: 9.999999\n",
      "    Train error: 26.312868\n",
      "-- Epoch: 11  Batch: 2 --\n",
      "    Loss_i: 0.17070058 Loss_f: 0.18282375 rho -0.0044385395 lambda: 0.01\n",
      "    Loss_i: 0.17070058 Loss_f: 0.1818454 rho -0.00049383554 lambda: 0.099999994\n",
      "    Loss_i: 0.17070058 Loss_f: 0.17510618 rho -1.9940886e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17070058 Loss_f: 0.17055704 rho 6.5111024e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17055704 Loss_f: 0.17116597 rho -2.7623065e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17055704 Loss_f: 0.17116597 rho -2.7623062e-07 lambda: 10.0\n",
      "    Train error: 26.643179\n",
      "-- Epoch: 11  Batch: 3 --\n",
      "    Loss_i: 0.17343922 Loss_f: 0.17552017 rho -0.00074613735 lambda: 0.01\n",
      "    Loss_i: 0.17343922 Loss_f: 0.17625654 rho -0.0001174391 lambda: 0.099999994\n",
      "    Loss_i: 0.17343922 Loss_f: 0.17462653 rho -5.0310564e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17343922 Loss_f: 0.17338073 rho 2.4824082e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17338073 Loss_f: 0.173543 rho -6.888483e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17338073 Loss_f: 0.173543 rho -6.8884816e-08 lambda: 10.0\n",
      "    Train error: 27.292393\n",
      "-- Epoch: 11  Batch: 4 --\n",
      "    Loss_i: 0.16813734 Loss_f: 0.16951029 rho -0.00045927547 lambda: 0.01\n",
      "    Loss_i: 0.16813734 Loss_f: 0.17025264 rho -8.5257656e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16813734 Loss_f: 0.1690751 rho -3.858732e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16813734 Loss_f: 0.16809571 rho 1.7167567e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16809571 Loss_f: 0.16823225 rho -5.6306593e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16809571 Loss_f: 0.16823225 rho -5.630659e-08 lambda: 10.0\n",
      "    Train error: 26.226402\n",
      "-- Epoch: 11  Batch: 5 --\n",
      "    Loss_i: 0.17240219 Loss_f: 0.17363974 rho -0.00044807914 lambda: 0.01\n",
      "    Loss_i: 0.17240219 Loss_f: 0.17461431 rho -9.538007e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17240219 Loss_f: 0.17343983 rho -4.5610523e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17240219 Loss_f: 0.1723566 rho 2.007533e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1723566 Loss_f: 0.17250858 rho -6.692452e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1723566 Loss_f: 0.17250858 rho -6.6924514e-08 lambda: 10.0\n",
      "    Train error: 27.313622\n",
      "-- Epoch: 11  Batch: 6 --\n",
      "    Loss_i: 0.1664774 Loss_f: 0.16650204 rho -7.74016e-06 lambda: 0.01\n",
      "    Loss_i: 0.1664774 Loss_f: 0.16763961 rho -4.2749787e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1664774 Loss_f: 0.16708173 rho -2.2616439e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1664774 Loss_f: 0.16644342 rho 1.2736829e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16644342 Loss_f: 0.16653089 rho -3.2795537e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16644342 Loss_f: 0.16653088 rho -3.278994e-08 lambda: 10.0\n",
      "    Train error: 25.653782\n",
      "-- Epoch: 11  Batch: 7 --\n",
      "    Loss_i: 0.17530432 Loss_f: 0.17420791 rho 0.00041288944 lambda: 0.01\n",
      "    Loss_i: 0.17420791 Loss_f: 0.17797413 rho -0.0014375844 lambda: 0.01\n",
      "    Loss_i: 0.17420791 Loss_f: 0.17799075 rho -0.00016442112 lambda: 0.099999994\n",
      "    Loss_i: 0.17420791 Loss_f: 0.17645113 rho -9.887309e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17420791 Loss_f: 0.17463265 rho -1.8747494e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17420791 Loss_f: 0.17463265 rho -1.8747494e-07 lambda: 10.0\n",
      "    Train error: 27.60916\n",
      "-- Epoch: 11  Batch: 8 --\n",
      "    Loss_i: 0.1749296 Loss_f: 0.27953312 rho -0.061224177 lambda: 0.01\n",
      "    Loss_i: 0.1749296 Loss_f: 0.2663753 rho -0.006427882 lambda: 0.099999994\n",
      "    Loss_i: 0.1749296 Loss_f: 0.21561396 rho -0.00029184247 lambda: 0.99999994\n",
      "    Loss_i: 0.1749296 Loss_f: 0.17445388 rho 3.4195085e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17445388 Loss_f: 0.1807219 rho -4.4900644e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17445388 Loss_f: 0.1807219 rho -4.4900644e-06 lambda: 10.0\n",
      "    Train error: 27.565891\n",
      "-- Epoch: 11  Batch: 9 --\n",
      "    Loss_i: 0.17078304 Loss_f: 0.2223084 rho -0.017957913 lambda: 0.01\n",
      "    Loss_i: 0.17078304 Loss_f: 0.21585575 rho -0.002052333 lambda: 0.099999994\n",
      "    Loss_i: 0.17078304 Loss_f: 0.19043413 rho -9.230785e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17078304 Loss_f: 0.17043842 rho 1.6239284e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17043842 Loss_f: 0.17345476 rho -1.4223866e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17043842 Loss_f: 0.17345476 rho -1.4223864e-06 lambda: 10.0\n",
      "    Train error: 26.82608\n",
      "-- Epoch: 11  Batch: 10 --\n",
      "    Loss_i: 0.17089249 Loss_f: 0.19489487 rho -0.009777485 lambda: 0.01\n",
      "    Loss_i: 0.17089249 Loss_f: 0.19220445 rho -0.0010095168 lambda: 0.099999994\n",
      "    Loss_i: 0.17089249 Loss_f: 0.17961583 rho -4.200517e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17089249 Loss_f: 0.17073421 rho 7.634246e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17073421 Loss_f: 0.17200723 rho -6.138421e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17073421 Loss_f: 0.17200722 rho -6.1383474e-07 lambda: 10.0\n",
      "    Train error: 26.713469\n",
      "-- Epoch: 11  Batch: 11 --\n",
      "    Loss_i: 0.17019631 Loss_f: 0.17967187 rho -0.0034022457 lambda: 0.01\n",
      "    Loss_i: 0.17019631 Loss_f: 0.17934397 rho -0.00039181908 lambda: 0.099999994\n",
      "    Loss_i: 0.17019631 Loss_f: 0.17405851 rho -1.6868296e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.17019631 Loss_f: 0.1701203 rho 3.326342e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1701203 Loss_f: 0.17069978 rho -2.5351426e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1701203 Loss_f: 0.17069978 rho -2.535142e-07 lambda: 10.0\n",
      "    Train error: 26.661852\n",
      "-- Epoch: 11  Batch: 12 --\n",
      "    Loss_i: 0.17510746 Loss_f: 0.17727926 rho -0.0007789772 lambda: 0.01\n",
      "    Loss_i: 0.17510746 Loss_f: 0.17770013 rho -0.000109317756 lambda: 0.099999994\n",
      "    Loss_i: 0.17510746 Loss_f: 0.17619134 rho -4.6517557e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17510746 Loss_f: 0.17507857 rho 1.2422527e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17507857 Loss_f: 0.17523502 rho -6.724898e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17507857 Loss_f: 0.17523502 rho -6.7248976e-08 lambda: 10.0\n",
      "    Train error: 27.661139\n",
      "-- Epoch: 11  Batch: 13 --\n",
      "    Loss_i: 0.16624282 Loss_f: 0.1653699 rho 0.00028431186 lambda: 0.01\n",
      "    Loss_i: 0.1653699 Loss_f: 0.16671497 rho -0.00044012672 lambda: 0.01\n",
      "    Loss_i: 0.1653699 Loss_f: 0.16722067 rho -7.199737e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1653699 Loss_f: 0.16654742 rho -4.668883e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1653699 Loss_f: 0.16559769 rho -9.0494936e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1653699 Loss_f: 0.16559768 rho -9.0489e-08 lambda: 10.0\n",
      "    Train error: 25.889828\n",
      "-- Epoch: 11  Batch: 14 --\n",
      "    Loss_i: 0.17001794 Loss_f: 0.23688138 rho -0.03254424 lambda: 0.01\n",
      "    Loss_i: 0.17001794 Loss_f: 0.2285746 rho -0.0036403972 lambda: 0.099999994\n",
      "    Loss_i: 0.17001794 Loss_f: 0.1949192 rho -0.00015922303 lambda: 0.99999994\n",
      "    Loss_i: 0.17001794 Loss_f: 0.16974498 rho 1.7503433e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16974498 Loss_f: 0.1734379 rho -2.3652135e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16974498 Loss_f: 0.17343792 rho -2.3652324e-06 lambda: 10.0\n",
      "    Train error: 26.361189\n",
      "-- Epoch: 11  Batch: 15 --\n",
      "    Loss_i: 0.16798128 Loss_f: 0.20317271 rho -0.00985991 lambda: 0.01\n",
      "    Loss_i: 0.16798128 Loss_f: 0.19896878 rho -0.0011587946 lambda: 0.099999994\n",
      "    Loss_i: 0.16798128 Loss_f: 0.18072242 rho -4.9296104e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16798128 Loss_f: 0.16781513 rho 6.450688e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16781513 Loss_f: 0.16966432 rho -7.1822734e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16781513 Loss_f: 0.16966432 rho -7.182273e-07 lambda: 10.0\n",
      "    Train error: 25.989864\n",
      "-- Epoch: 11  Batch: 16 --\n",
      "    Loss_i: 0.16945396 Loss_f: 0.18870358 rho -0.0074199885 lambda: 0.01\n",
      "    Loss_i: 0.16945396 Loss_f: 0.18660837 rho -0.000890454 lambda: 0.099999994\n",
      "    Loss_i: 0.16945396 Loss_f: 0.17616388 rho -3.6080695e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16945396 Loss_f: 0.16937236 rho 4.4035563e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16937236 Loss_f: 0.17031075 rho -5.062882e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16937236 Loss_f: 0.17031075 rho -5.0628813e-07 lambda: 10.0\n",
      "    Train error: 26.41653\n",
      "-- Epoch: 11  Batch: 17 --\n",
      "    Loss_i: 0.16764788 Loss_f: 0.17811647 rho -0.0034055915 lambda: 0.01\n",
      "    Loss_i: 0.16764788 Loss_f: 0.17731673 rho -0.0003649606 lambda: 0.099999994\n",
      "    Loss_i: 0.16764788 Loss_f: 0.1714935 rho -1.4752148e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16764788 Loss_f: 0.16758654 rho 2.3572104e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16758654 Loss_f: 0.16813385 rho -2.103202e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16758654 Loss_f: 0.16813387 rho -2.1032591e-07 lambda: 10.0\n",
      "    Train error: 25.916111\n",
      "-- Epoch: 11  Batch: 18 --\n",
      "    Loss_i: 0.16979177 Loss_f: 0.175465 rho -0.001921651 lambda: 0.01\n",
      "    Loss_i: 0.16979177 Loss_f: 0.17528854 rho -0.00022563011 lambda: 0.099999994\n",
      "    Loss_i: 0.16979177 Loss_f: 0.17194876 rho -9.0455715e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16979177 Loss_f: 0.16974841 rho 1.8223947e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16974841 Loss_f: 0.17004713 rho -1.2546631e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16974841 Loss_f: 0.17004713 rho -1.2546631e-07 lambda: 10.0\n",
      "    Train error: 26.293264\n",
      "-- Epoch: 11  Batch: 19 --\n",
      "    Loss_i: 0.17213869 Loss_f: 0.172546 rho -0.00013962826 lambda: 0.01\n",
      "    Loss_i: 0.17213869 Loss_f: 0.17312405 rho -4.0280775e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17213869 Loss_f: 0.17254938 rho -1.7118338e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17213869 Loss_f: 0.1721199 rho 7.847565e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1721199 Loss_f: 0.17217623 rho -2.352594e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1721199 Loss_f: 0.17217623 rho -2.352594e-08 lambda: 10.0\n",
      "    Train error: 27.026106\n",
      "-- Epoch: 11  Batch: 20 --\n",
      "    Loss_i: 0.16780657 Loss_f: 0.16717692 rho 0.00018121077 lambda: 0.01\n",
      "    Loss_i: 0.16717692 Loss_f: 0.16718914 rho -3.4366913e-06 lambda: 0.01\n",
      "    Loss_i: 0.16717692 Loss_f: 0.16767168 rho -1.7211194e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16717692 Loss_f: 0.16748276 rho -1.0897465e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16717692 Loss_f: 0.16723138 rho -1.9452912e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16717692 Loss_f: 0.16723138 rho -1.945291e-08 lambda: 10.0\n",
      "    Train error: 25.744596\n",
      "-- Epoch: 11  Batch: 21 --\n",
      "    Loss_i: 0.16977575 Loss_f: 0.17712831 rho -0.002616104 lambda: 0.01\n",
      "    Loss_i: 0.16977575 Loss_f: 0.17679898 rho -0.00029442486 lambda: 0.099999994\n",
      "    Loss_i: 0.16977575 Loss_f: 0.17247441 rho -1.1518469e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16977575 Loss_f: 0.16965613 rho 5.115205e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16965613 Loss_f: 0.1700105 rho -1.5150827e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16965613 Loss_f: 0.1700105 rho -1.5150826e-07 lambda: 10.0\n",
      "    Train error: 26.496202\n",
      "-- Epoch: 11  Batch: 22 --\n",
      "    Loss_i: 0.16617621 Loss_f: 0.16830225 rho -0.00061114924 lambda: 0.01\n",
      "    Loss_i: 0.16617621 Loss_f: 0.16857733 rho -8.18219e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16617621 Loss_f: 0.16712543 rho -3.2957362e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16617621 Loss_f: 0.1661486 rho 9.605124e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1661486 Loss_f: 0.16627957 rho -4.554725e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1661486 Loss_f: 0.16627957 rho -4.5547242e-08 lambda: 10.0\n",
      "    Train error: 25.556301\n",
      "-- Epoch: 11  Batch: 23 --\n",
      "    Loss_i: 0.17183869 Loss_f: 0.17226848 rho -0.00014833185 lambda: 0.01\n",
      "    Loss_i: 0.17183869 Loss_f: 0.17287768 rho -4.351937e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17183869 Loss_f: 0.17227209 rho -1.8549684e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17183869 Loss_f: 0.17182256 rho 6.9158217e-09 lambda: 9.999999\n",
      "    Loss_i: 0.17182256 Loss_f: 0.17188263 rho -2.5763368e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17182256 Loss_f: 0.17188263 rho -2.5763363e-08 lambda: 10.0\n",
      "    Train error: 26.7306\n",
      "-- Epoch: 11  Batch: 24 --\n",
      "    Loss_i: 0.1713071 Loss_f: 0.17126969 rho 1.2162549e-05 lambda: 0.01\n",
      "    Loss_i: 0.17126969 Loss_f: 0.17330866 rho -0.00063072 lambda: 0.01\n",
      "    Loss_i: 0.17126969 Loss_f: 0.17357807 rho -8.646703e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17126969 Loss_f: 0.17264411 rho -5.2592204e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17126969 Loss_f: 0.17152926 rho -9.9541936e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17126969 Loss_f: 0.17152926 rho -9.954193e-08 lambda: 10.0\n",
      "    Train error: 26.618473\n",
      "-- Epoch: 11  Batch: 25 --\n",
      "    Loss_i: 0.17283526 Loss_f: 0.23234208 rho -0.021998946 lambda: 0.01\n",
      "    Loss_i: 0.17283526 Loss_f: 0.2252204 rho -0.0025024964 lambda: 0.099999994\n",
      "    Loss_i: 0.17283526 Loss_f: 0.19525829 rho -0.000110341476 lambda: 0.99999994\n",
      "    Loss_i: 0.17283526 Loss_f: 0.17247963 rho 1.7553084e-07 lambda: 9.999999\n",
      "    Loss_i: 0.17247963 Loss_f: 0.1758252 rho -1.6506609e-06 lambda: 9.999999\n",
      "    Loss_i: 0.17247963 Loss_f: 0.1758252 rho -1.6506608e-06 lambda: 10.0\n",
      "    Train error: 27.22759\n",
      "-- Epoch: 11  Batch: 26 --\n",
      "    Loss_i: 0.16800347 Loss_f: 0.19897018 rho -0.009732906 lambda: 0.01\n",
      "    Loss_i: 0.16800347 Loss_f: 0.19541813 rho -0.0010833717 lambda: 0.099999994\n",
      "    Loss_i: 0.16800347 Loss_f: 0.17900272 rho -4.4614826e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16800347 Loss_f: 0.16786958 rho 5.4450663e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16786958 Loss_f: 0.16943334 rho -6.356946e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16786958 Loss_f: 0.16943334 rho -6.356946e-07 lambda: 10.0\n",
      "    Train error: 25.99808\n",
      "-- Epoch: 11  Batch: 27 --\n",
      "    Loss_i: 0.16661315 Loss_f: 0.17666139 rho -0.0029594253 lambda: 0.01\n",
      "    Loss_i: 0.16661315 Loss_f: 0.17587002 rho -0.00036472172 lambda: 0.099999994\n",
      "    Loss_i: 0.16661315 Loss_f: 0.17024797 rho -1.482189e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16661315 Loss_f: 0.16652481 rho 3.614676e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16652481 Loss_f: 0.16703145 rho -2.0725632e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16652481 Loss_f: 0.16703144 rho -2.0725022e-07 lambda: 10.0\n",
      "    Train error: 25.73407\n",
      "-- Epoch: 11  Batch: 28 --\n",
      "    Loss_i: 0.16584824 Loss_f: 0.16848652 rho -0.00078052137 lambda: 0.01\n",
      "    Loss_i: 0.16584824 Loss_f: 0.16864963 rho -9.910263e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16584824 Loss_f: 0.16692866 rho -3.8984554e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16584824 Loss_f: 0.16581677 rho 1.13783845e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16581677 Loss_f: 0.16596496 rho -5.356937e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16581677 Loss_f: 0.16596496 rho -5.356936e-08 lambda: 10.0\n",
      "    Train error: 25.487328\n",
      "-- Epoch: 11  Batch: 29 --\n",
      "    Loss_i: 0.16782092 Loss_f: 0.1674782 rho 0.00010280387 lambda: 0.01\n",
      "    Loss_i: 0.1674782 Loss_f: 0.16867502 rho -0.00035933882 lambda: 0.01\n",
      "    Loss_i: 0.1674782 Loss_f: 0.1690176 rho -5.2196967e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1674782 Loss_f: 0.16837424 rho -3.07804e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1674782 Loss_f: 0.1676426 rho -5.654475e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1674782 Loss_f: 0.1676426 rho -5.6544735e-08 lambda: 10.0\n",
      "    Train error: 26.053753\n",
      "-- Epoch: 11  Batch: 30 --\n",
      "    Loss_i: 0.16906863 Loss_f: 0.20293352 rho -0.012103044 lambda: 0.01\n",
      "    Loss_i: 0.16906863 Loss_f: 0.19838656 rho -0.0012591551 lambda: 0.099999994\n",
      "    Loss_i: 0.16906863 Loss_f: 0.1810337 rho -5.24458e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16906863 Loss_f: 0.16886911 rho 8.7637765e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16886911 Loss_f: 0.17061049 rho -7.646756e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16886911 Loss_f: 0.17061049 rho -7.646756e-07 lambda: 10.0\n",
      "    Train error: 25.875317\n",
      "\n",
      "*** Epoch: 11 Train error: 26.46316795349121  Test error: 25.666197  Time: 706.5234654999995 sec\n",
      "\n",
      "-- Epoch: 12  Batch: 1 --\n",
      "    Loss_i: 0.16530061 Loss_f: 0.17633095 rho -0.0031394116 lambda: 0.01\n",
      "    Loss_i: 0.16530061 Loss_f: 0.17549059 rho -0.0003974392 lambda: 0.099999994\n",
      "    Loss_i: 0.16530061 Loss_f: 0.16937712 rho -1.651111e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16530061 Loss_f: 0.16518977 rho 4.5064812e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16518977 Loss_f: 0.16577071 rho -2.3612851e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16518977 Loss_f: 0.16577071 rho -2.3612851e-07 lambda: 10.0\n",
      "    Train error: 25.29976\n",
      "-- Epoch: 12  Batch: 2 --\n",
      "    Loss_i: 0.16722013 Loss_f: 0.16909531 rho -0.00051049 lambda: 0.01\n",
      "    Loss_i: 0.16722013 Loss_f: 0.16941814 rho -7.685845e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16722013 Loss_f: 0.16810842 rho -3.1970428e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16722013 Loss_f: 0.16718642 rho 1.2166919e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16718642 Loss_f: 0.16731107 rho -4.4982013e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16718642 Loss_f: 0.16731107 rho -4.498201e-08 lambda: 10.0\n",
      "    Train error: 25.71245\n",
      "-- Epoch: 12  Batch: 3 --\n",
      "    Loss_i: 0.16971129 Loss_f: 0.16940747 rho 8.8529145e-05 lambda: 0.01\n",
      "    Loss_i: 0.16940747 Loss_f: 0.1708652 rho -0.00042515824 lambda: 0.01\n",
      "    Loss_i: 0.16940747 Loss_f: 0.17122066 rho -6.087285e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16940747 Loss_f: 0.17048003 rho -3.6560475e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16940747 Loss_f: 0.16960673 rho -6.802604e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16940747 Loss_f: 0.16960675 rho -6.803113e-08 lambda: 10.0\n",
      "    Train error: 26.34713\n",
      "-- Epoch: 12  Batch: 4 --\n",
      "    Loss_i: 0.16461127 Loss_f: 0.21749848 rho -0.016106064 lambda: 0.01\n",
      "    Loss_i: 0.16461127 Loss_f: 0.21058464 rho -0.0018537538 lambda: 0.099999994\n",
      "    Loss_i: 0.16461127 Loss_f: 0.18313457 rho -7.719172e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16461127 Loss_f: 0.1642709 rho 1.4231914e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1642709 Loss_f: 0.16690227 rho -1.0990266e-06 lambda: 9.999999\n",
      "    Loss_i: 0.1642709 Loss_f: 0.16690227 rho -1.0990266e-06 lambda: 10.0\n",
      "    Train error: 25.124205\n",
      "-- Epoch: 12  Batch: 5 --\n",
      "    Loss_i: 0.1681989 Loss_f: 0.1940586 rho -0.008688316 lambda: 0.01\n",
      "    Loss_i: 0.1681989 Loss_f: 0.19070253 rho -0.0009314291 lambda: 0.099999994\n",
      "    Loss_i: 0.1681989 Loss_f: 0.17732446 rho -3.8667662e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1681989 Loss_f: 0.16801834 rho 7.668954e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16801834 Loss_f: 0.16933331 rho -5.5860187e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16801834 Loss_f: 0.16933331 rho -5.586018e-07 lambda: 10.0\n",
      "    Train error: 26.17835\n",
      "-- Epoch: 12  Batch: 6 --\n",
      "    Loss_i: 0.16238745 Loss_f: 0.17328972 rho -0.0032238064 lambda: 0.01\n",
      "    Loss_i: 0.16238745 Loss_f: 0.17240386 rho -0.00033406983 lambda: 0.099999994\n",
      "    Loss_i: 0.16238745 Loss_f: 0.16627353 rho -1.3128908e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16238745 Loss_f: 0.16230431 rho 2.8122596e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16230431 Loss_f: 0.1628343 rho -1.7919932e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16230431 Loss_f: 0.1628343 rho -1.7919932e-07 lambda: 10.0\n",
      "    Train error: 24.776718\n",
      "-- Epoch: 12  Batch: 7 --\n",
      "    Loss_i: 0.17137304 Loss_f: 0.17510854 rho -0.0010005485 lambda: 0.01\n",
      "    Loss_i: 0.17137304 Loss_f: 0.17540754 rho -0.00015903368 lambda: 0.099999994\n",
      "    Loss_i: 0.17137304 Loss_f: 0.17298277 rho -6.659386e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17137304 Loss_f: 0.17133975 rho 1.3840132e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17133975 Loss_f: 0.17156637 rho -9.418715e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17133975 Loss_f: 0.17156635 rho -9.418094e-08 lambda: 10.0\n",
      "    Train error: 26.671774\n",
      "-- Epoch: 12  Batch: 8 --\n",
      "    Loss_i: 0.17111395 Loss_f: 0.17193569 rho -0.00028544108 lambda: 0.01\n",
      "    Loss_i: 0.17111395 Loss_f: 0.17248587 rho -5.559279e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17111395 Loss_f: 0.17170171 rho -2.4220617e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17111395 Loss_f: 0.17109351 rho 8.439073e-09 lambda: 9.999999\n",
      "    Loss_i: 0.17109351 Loss_f: 0.17117661 rho -3.4304644e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17109351 Loss_f: 0.17117661 rho -3.4304644e-08 lambda: 10.0\n",
      "    Train error: 26.862207\n",
      "-- Epoch: 12  Batch: 9 --\n",
      "    Loss_i: 0.16753475 Loss_f: 0.16718714 rho 9.511803e-05 lambda: 0.01\n",
      "    Loss_i: 0.16718714 Loss_f: 0.16788092 rho -0.00019046394 lambda: 0.01\n",
      "    Loss_i: 0.16718714 Loss_f: 0.16823535 rho -3.4946694e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16718714 Loss_f: 0.1678151 rho -2.139478e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16718714 Loss_f: 0.1673043 rho -4.0006782e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16718714 Loss_f: 0.1673043 rho -4.000678e-08 lambda: 10.0\n",
      "    Train error: 26.033262\n",
      "-- Epoch: 12  Batch: 10 --\n",
      "    Loss_i: 0.16757211 Loss_f: 0.19257286 rho -0.00813123 lambda: 0.01\n",
      "    Loss_i: 0.16757211 Loss_f: 0.18974587 rho -0.00081092713 lambda: 0.099999994\n",
      "    Loss_i: 0.16757211 Loss_f: 0.17649677 rho -3.3050095e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16757211 Loss_f: 0.16739365 rho 6.616987e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16739365 Loss_f: 0.16866674 rho -4.7166466e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16739365 Loss_f: 0.16866674 rho -4.716646e-07 lambda: 10.0\n",
      "    Train error: 25.907124\n",
      "-- Epoch: 12  Batch: 11 --\n",
      "    Loss_i: 0.16691694 Loss_f: 0.1795394 rho -0.0038748076 lambda: 0.01\n",
      "    Loss_i: 0.16691694 Loss_f: 0.17841944 rho -0.00042541427 lambda: 0.099999994\n",
      "    Loss_i: 0.16691694 Loss_f: 0.17128627 rho -1.649762e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16691694 Loss_f: 0.16683556 rho 3.0789835e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16683556 Loss_f: 0.16741094 rho -2.1761613e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16683556 Loss_f: 0.16741094 rho -2.1761613e-07 lambda: 10.0\n",
      "    Train error: 25.863516\n",
      "-- Epoch: 12  Batch: 12 --\n",
      "    Loss_i: 0.17213142 Loss_f: 0.17493244 rho -0.0008568406 lambda: 0.01\n",
      "    Loss_i: 0.17213142 Loss_f: 0.17514887 rho -0.000116132025 lambda: 0.099999994\n",
      "    Loss_i: 0.17213142 Loss_f: 0.17331535 rho -4.677285e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17213142 Loss_f: 0.17209387 rho 1.4874487e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17209387 Loss_f: 0.17225528 rho -6.3913035e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17209387 Loss_f: 0.17225528 rho -6.391303e-08 lambda: 10.0\n",
      "    Train error: 26.926785\n",
      "-- Epoch: 12  Batch: 13 --\n",
      "    Loss_i: 0.16272795 Loss_f: 0.16278067 rho -1.4175631e-05 lambda: 0.01\n",
      "    Loss_i: 0.16272795 Loss_f: 0.16333038 rho -1.8740779e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16272795 Loss_f: 0.16298397 rho -8.091422e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16272795 Loss_f: 0.16271359 rho 4.547226e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16271359 Loss_f: 0.16274731 rho -1.0675153e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16271359 Loss_f: 0.16274731 rho -1.0675151e-08 lambda: 10.0\n",
      "    Train error: 25.075243\n",
      "-- Epoch: 12  Batch: 14 --\n",
      "    Loss_i: 0.16700532 Loss_f: 0.16696335 rho 1.3415132e-05 lambda: 0.01\n",
      "    Loss_i: 0.16696335 Loss_f: 0.16923936 rho -0.0006892299 lambda: 0.01\n",
      "    Loss_i: 0.16696335 Loss_f: 0.1695283 rho -9.678282e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16696335 Loss_f: 0.16853163 rho -6.0667953e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16696335 Loss_f: 0.16726622 rho -1.17458754e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16696335 Loss_f: 0.16726622 rho -1.1745874e-07 lambda: 10.0\n",
      "    Train error: 25.658726\n",
      "-- Epoch: 12  Batch: 15 --\n",
      "    Loss_i: 0.16588688 Loss_f: 0.23435639 rho -0.021706754 lambda: 0.01\n",
      "    Loss_i: 0.16588688 Loss_f: 0.22616793 rho -0.0023919297 lambda: 0.099999994\n",
      "    Loss_i: 0.16588688 Loss_f: 0.19215761 rho -0.00010693185 lambda: 0.99999994\n",
      "    Loss_i: 0.16588688 Loss_f: 0.16552418 rho 1.4801233e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16552418 Loss_f: 0.16946588 rho -1.6062289e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16552418 Loss_f: 0.16946588 rho -1.6062288e-06 lambda: 10.0\n",
      "    Train error: 25.495224\n",
      "-- Epoch: 12  Batch: 16 --\n",
      "    Loss_i: 0.16696528 Loss_f: 0.20139143 rho -0.013096224 lambda: 0.01\n",
      "    Loss_i: 0.16696528 Loss_f: 0.19735137 rho -0.0013727704 lambda: 0.099999994\n",
      "    Loss_i: 0.16696528 Loss_f: 0.17930503 rho -5.6813795e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16696528 Loss_f: 0.16675614 rho 9.647412e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16675614 Loss_f: 0.16853045 rho -8.18233e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16675614 Loss_f: 0.16853045 rho -8.182329e-07 lambda: 10.0\n",
      "    Train error: 25.77255\n",
      "-- Epoch: 12  Batch: 17 --\n",
      "    Loss_i: 0.16527596 Loss_f: 0.1814879 rho -0.0049373377 lambda: 0.01\n",
      "    Loss_i: 0.16527596 Loss_f: 0.17988922 rho -0.0005339832 lambda: 0.099999994\n",
      "    Loss_i: 0.16527596 Loss_f: 0.17108035 rho -2.1642329e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16527596 Loss_f: 0.165186 rho 3.361053e-08 lambda: 9.999999\n",
      "    Loss_i: 0.165186 Loss_f: 0.16600914 rho -3.075348e-07 lambda: 9.999999\n",
      "    Loss_i: 0.165186 Loss_f: 0.16600914 rho -3.075348e-07 lambda: 10.0\n",
      "    Train error: 25.355627\n",
      "-- Epoch: 12  Batch: 18 --\n",
      "    Loss_i: 0.16742474 Loss_f: 0.17537622 rho -0.0026635313 lambda: 0.01\n",
      "    Loss_i: 0.16742474 Loss_f: 0.17493515 rho -0.0003023917 lambda: 0.099999994\n",
      "    Loss_i: 0.16742474 Loss_f: 0.17038536 rho -1.2166089e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16742474 Loss_f: 0.16736878 rho 2.3040617e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16736878 Loss_f: 0.16778034 rho -1.6932883e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16736878 Loss_f: 0.16778034 rho -1.693288e-07 lambda: 10.0\n",
      "    Train error: 25.714996\n",
      "-- Epoch: 12  Batch: 19 --\n",
      "    Loss_i: 0.1696038 Loss_f: 0.17064403 rho -0.00032531208 lambda: 0.01\n",
      "    Loss_i: 0.1696038 Loss_f: 0.17115545 rho -5.5778735e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1696038 Loss_f: 0.17026758 rho -2.4223825e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1696038 Loss_f: 0.16958548 rho 6.69338e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16958548 Loss_f: 0.16968103 rho -3.491411e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16958548 Loss_f: 0.16968103 rho -3.4914102e-08 lambda: 10.0\n",
      "    Train error: 26.432419\n",
      "-- Epoch: 12  Batch: 20 --\n",
      "    Loss_i: 0.16525187 Loss_f: 0.16476144 rho 0.00012461109 lambda: 0.01\n",
      "    Loss_i: 0.16476144 Loss_f: 0.16506244 rho -7.44841e-05 lambda: 0.01\n",
      "    Loss_i: 0.16476144 Loss_f: 0.16546297 rho -2.1737242e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16476144 Loss_f: 0.16519077 rho -1.3647173e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16476144 Loss_f: 0.16484135 rho -2.5468427e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16476144 Loss_f: 0.16484135 rho -2.5468427e-08 lambda: 10.0\n",
      "    Train error: 25.140234\n",
      "-- Epoch: 12  Batch: 21 --\n",
      "    Loss_i: 0.16753824 Loss_f: 0.18840338 rho -0.00591677 lambda: 0.01\n",
      "    Loss_i: 0.16753824 Loss_f: 0.1864313 rho -0.0007141929 lambda: 0.099999994\n",
      "    Loss_i: 0.16753824 Loss_f: 0.17533447 rho -3.0486583e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16753824 Loss_f: 0.16742194 rho 4.56369e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16742194 Loss_f: 0.16855854 rho -4.4606855e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16742194 Loss_f: 0.16855854 rho -4.460684e-07 lambda: 10.0\n",
      "    Train error: 25.984383\n",
      "-- Epoch: 12  Batch: 22 --\n",
      "    Loss_i: 0.16383564 Loss_f: 0.17496727 rho -0.002820826 lambda: 0.01\n",
      "    Loss_i: 0.16383564 Loss_f: 0.1740178 rho -0.00032560076 lambda: 0.099999994\n",
      "    Loss_i: 0.16383564 Loss_f: 0.16784373 rho -1.3161601e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16383564 Loss_f: 0.16374822 rho 2.8785758e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16374822 Loss_f: 0.16430652 rho -1.8370856e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16374822 Loss_f: 0.16430652 rho -1.8370855e-07 lambda: 10.0\n",
      "    Train error: 24.976332\n",
      "-- Epoch: 12  Batch: 23 --\n",
      "    Loss_i: 0.16935606 Loss_f: 0.17370729 rho -0.0012624922 lambda: 0.01\n",
      "    Loss_i: 0.16935606 Loss_f: 0.17367423 rho -0.00016738307 lambda: 0.099999994\n",
      "    Loss_i: 0.16935606 Loss_f: 0.17106824 rho -6.8675513e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16935606 Loss_f: 0.16931584 rho 1.618787e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16931584 Loss_f: 0.16955437 rho -9.599538e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16931584 Loss_f: 0.16955437 rho -9.599538e-08 lambda: 10.0\n",
      "    Train error: 26.127436\n",
      "-- Epoch: 12  Batch: 24 --\n",
      "    Loss_i: 0.16874456 Loss_f: 0.17026566 rho -0.00047650488 lambda: 0.01\n",
      "    Loss_i: 0.16874456 Loss_f: 0.17056368 rho -6.682469e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16874456 Loss_f: 0.16948368 rho -2.7628023e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16874456 Loss_f: 0.16872685 rho 6.6344468e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16872685 Loss_f: 0.16883123 rho -3.9073715e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16872685 Loss_f: 0.16883123 rho -3.9073715e-08 lambda: 10.0\n",
      "    Train error: 26.073694\n",
      "-- Epoch: 12  Batch: 25 --\n",
      "    Loss_i: 0.17011426 Loss_f: 0.17003615 rho 2.140395e-05 lambda: 0.01\n",
      "    Loss_i: 0.17003615 Loss_f: 0.17167993 rho -0.00045269672 lambda: 0.01\n",
      "    Loss_i: 0.17003615 Loss_f: 0.17193907 rho -6.316166e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.17003615 Loss_f: 0.17119141 rho -3.914862e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.17003615 Loss_f: 0.17025708 rho -7.5022704e-08 lambda: 9.999999\n",
      "    Loss_i: 0.17003615 Loss_f: 0.17025708 rho -7.502269e-08 lambda: 10.0\n",
      "    Train error: 26.634016\n",
      "-- Epoch: 12  Batch: 26 --\n",
      "    Loss_i: 0.16588138 Loss_f: 0.2235668 rho -0.015293126 lambda: 0.01\n",
      "    Loss_i: 0.16588138 Loss_f: 0.21655521 rho -0.001847838 lambda: 0.099999994\n",
      "    Loss_i: 0.16588138 Loss_f: 0.18655445 rho -7.832586e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16588138 Loss_f: 0.16558488 rho 1.1277881e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16558488 Loss_f: 0.16850686 rho -1.1114158e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16558488 Loss_f: 0.16850686 rho -1.1114158e-06 lambda: 10.0\n",
      "    Train error: 25.382444\n",
      "-- Epoch: 12  Batch: 27 --\n",
      "    Loss_i: 0.16441774 Loss_f: 0.18540554 rho -0.0060049454 lambda: 0.01\n",
      "    Loss_i: 0.16441774 Loss_f: 0.1832805 rho -0.000679825 lambda: 0.099999994\n",
      "    Loss_i: 0.16441774 Loss_f: 0.17208171 rho -2.8357694e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16441774 Loss_f: 0.16426265 rho 5.7539218e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16426265 Loss_f: 0.16536425 rho -4.0858808e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16426265 Loss_f: 0.16536425 rho -4.08588e-07 lambda: 10.0\n",
      "    Train error: 25.171293\n",
      "-- Epoch: 12  Batch: 28 --\n",
      "    Loss_i: 0.16347681 Loss_f: 0.1710442 rho -0.0025157768 lambda: 0.01\n",
      "    Loss_i: 0.16347681 Loss_f: 0.17058904 rho -0.00027414996 lambda: 0.099999994\n",
      "    Loss_i: 0.16347681 Loss_f: 0.16626029 rho -1.0903124e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16347681 Loss_f: 0.16341896 rho 2.2695687e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16341896 Loss_f: 0.16380644 rho -1.5195123e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16341896 Loss_f: 0.16380644 rho -1.5195123e-07 lambda: 10.0\n",
      "    Train error: 24.875984\n",
      "-- Epoch: 12  Batch: 29 --\n",
      "    Loss_i: 0.16530651 Loss_f: 0.16639915 rho -0.00030579176 lambda: 0.01\n",
      "    Loss_i: 0.16530651 Loss_f: 0.16676122 rho -4.630438e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16530651 Loss_f: 0.16590801 rho -1.9412792e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16530651 Loss_f: 0.16528821 rho 5.9139276e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16528821 Loss_f: 0.16537325 rho -2.7485472e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16528821 Loss_f: 0.16537325 rho -2.748547e-08 lambda: 10.0\n",
      "    Train error: 25.342247\n",
      "-- Epoch: 12  Batch: 30 --\n",
      "    Loss_i: 0.1668671 Loss_f: 0.16588624 rho 0.0002753186 lambda: 0.01\n",
      "    Loss_i: 0.16588624 Loss_f: 0.16616698 rho -7.659119e-05 lambda: 0.01\n",
      "    Loss_i: 0.16588624 Loss_f: 0.16665982 rho -2.7223854e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16588624 Loss_f: 0.16636196 rho -1.7241451e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16588624 Loss_f: 0.16597177 rho -3.1092345e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16588624 Loss_f: 0.16597177 rho -3.109234e-08 lambda: 10.0\n",
      "    Train error: 25.11498\n",
      "\n",
      "*** Epoch: 12 Train error: 25.734370358784993  Test error: 25.141047  Time: 698.5154322999997 sec\n",
      "\n",
      "-- Epoch: 13  Batch: 1 --\n",
      "    Loss_i: 0.16302669 Loss_f: 0.18208769 rho -0.005450144 lambda: 0.01\n",
      "    Loss_i: 0.16302669 Loss_f: 0.18002535 rho -0.0005886511 lambda: 0.099999994\n",
      "    Loss_i: 0.16302669 Loss_f: 0.16965322 rho -2.3442053e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16302669 Loss_f: 0.1628787 rho 5.2463857e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1628787 Loss_f: 0.16377933 rho -3.1909408e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1628787 Loss_f: 0.16377933 rho -3.1909406e-07 lambda: 10.0\n",
      "    Train error: 24.745705\n",
      "-- Epoch: 13  Batch: 2 --\n",
      "    Loss_i: 0.16488749 Loss_f: 0.16969268 rho -0.0013314545 lambda: 0.01\n",
      "    Loss_i: 0.16488749 Loss_f: 0.16957015 rho -0.00014957877 lambda: 0.099999994\n",
      "    Loss_i: 0.16488749 Loss_f: 0.16672593 rho -5.9637073e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16488749 Loss_f: 0.16484009 rho 1.540011e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16484009 Loss_f: 0.16509491 rho -8.275721e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16484009 Loss_f: 0.16509491 rho -8.275721e-08 lambda: 10.0\n",
      "    Train error: 25.130234\n",
      "-- Epoch: 13  Batch: 3 --\n",
      "    Loss_i: 0.16724972 Loss_f: 0.16765086 rho -0.00010685201 lambda: 0.01\n",
      "    Loss_i: 0.16724972 Loss_f: 0.16814657 rho -2.6988015e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16724972 Loss_f: 0.16760825 rho -1.0930536e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16724972 Loss_f: 0.16722731 rho 6.841713e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16722731 Loss_f: 0.16727383 rho -1.42036995e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16722731 Loss_f: 0.16727383 rho -1.42036995e-08 lambda: 10.0\n",
      "    Train error: 25.64929\n",
      "-- Epoch: 13  Batch: 4 --\n",
      "    Loss_i: 0.16227947 Loss_f: 0.16277416 rho -0.00013225495 lambda: 0.01\n",
      "    Loss_i: 0.16227947 Loss_f: 0.16323152 rho -2.879992e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16227947 Loss_f: 0.16270414 rho -1.3017561e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16227947 Loss_f: 0.16226542 rho 4.3131116e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16226542 Loss_f: 0.16232932 rho -1.9613083e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16226542 Loss_f: 0.16232932 rho -1.961308e-08 lambda: 10.0\n",
      "    Train error: 24.624296\n",
      "-- Epoch: 13  Batch: 5 --\n",
      "    Loss_i: 0.16615146 Loss_f: 0.16717781 rho -0.00027824548 lambda: 0.01\n",
      "    Loss_i: 0.16615146 Loss_f: 0.1675782 rho -4.571127e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16615146 Loss_f: 0.16676068 rho -1.9880151e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16615146 Loss_f: 0.16613227 rho 6.2746133e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16613227 Loss_f: 0.16621983 rho -2.862627e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16613227 Loss_f: 0.16621983 rho -2.8626266e-08 lambda: 10.0\n",
      "    Train error: 25.649155\n",
      "-- Epoch: 13  Batch: 6 --\n",
      "    Loss_i: 0.16044039 Loss_f: 0.16054727 rho -2.7491322e-05 lambda: 0.01\n",
      "    Loss_i: 0.16044039 Loss_f: 0.16116364 rho -2.09656e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16044039 Loss_f: 0.16077578 rho -9.847449e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16044039 Loss_f: 0.16042867 rho 3.4432501e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16042867 Loss_f: 0.16047725 rho -1.4281085e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16042867 Loss_f: 0.16047724 rho -1.42767025e-08 lambda: 10.0\n",
      "    Train error: 24.169731\n",
      "-- Epoch: 13  Batch: 7 --\n",
      "    Loss_i: 0.16933444 Loss_f: 0.16866468 rho 0.00018323839 lambda: 0.01\n",
      "    Loss_i: 0.16866468 Loss_f: 0.16955402 rho -0.00024359146 lambda: 0.01\n",
      "    Loss_i: 0.16866468 Loss_f: 0.16998954 rho -4.264818e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16866468 Loss_f: 0.16948868 rho -2.6998441e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16866468 Loss_f: 0.16882072 rho -5.121939e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16866468 Loss_f: 0.16882072 rho -5.1219384e-08 lambda: 10.0\n",
      "    Train error: 26.061703\n",
      "-- Epoch: 13  Batch: 8 --\n",
      "    Loss_i: 0.16925156 Loss_f: 0.21154012 rho -0.013406251 lambda: 0.01\n",
      "    Loss_i: 0.16925156 Loss_f: 0.20684606 rho -0.0016491739 lambda: 0.099999994\n",
      "    Loss_i: 0.16925156 Loss_f: 0.1848219 rho -7.1028866e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16925156 Loss_f: 0.169058 rho 8.8654865e-08 lambda: 9.999999\n",
      "    Loss_i: 0.169058 Loss_f: 0.1712996 rho -1.0264675e-06 lambda: 9.999999\n",
      "    Loss_i: 0.169058 Loss_f: 0.17129959 rho -1.0264605e-06 lambda: 10.0\n",
      "    Train error: 26.249157\n",
      "-- Epoch: 13  Batch: 9 --\n",
      "    Loss_i: 0.16545582 Loss_f: 0.18478045 rho -0.0056107426 lambda: 0.01\n",
      "    Loss_i: 0.16545582 Loss_f: 0.18292096 rho -0.00063702284 lambda: 0.099999994\n",
      "    Loss_i: 0.16545582 Loss_f: 0.17267609 rho -2.7027734e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16545582 Loss_f: 0.16536285 rho 3.489274e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16536285 Loss_f: 0.16641963 rho -3.9670198e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16536285 Loss_f: 0.16641963 rho -3.9670192e-07 lambda: 10.0\n",
      "    Train error: 25.489264\n",
      "-- Epoch: 13  Batch: 10 --\n",
      "    Loss_i: 0.16560353 Loss_f: 0.17459138 rho -0.0026536526 lambda: 0.01\n",
      "    Loss_i: 0.16560353 Loss_f: 0.17395307 rho -0.00029609073 lambda: 0.099999994\n",
      "    Loss_i: 0.16560353 Loss_f: 0.1689341 rho -1.2053232e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16560353 Loss_f: 0.16554797 rho 2.0150656e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16554797 Loss_f: 0.1660234 rho -1.7226492e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16554797 Loss_f: 0.1660234 rho -1.722649e-07 lambda: 10.0\n",
      "    Train error: 25.419344\n",
      "-- Epoch: 13  Batch: 11 --\n",
      "    Loss_i: 0.16509882 Loss_f: 0.16822706 rho -0.00083577586 lambda: 0.01\n",
      "    Loss_i: 0.16509882 Loss_f: 0.16842431 rho -0.00011366185 lambda: 0.099999994\n",
      "    Loss_i: 0.16509882 Loss_f: 0.16645157 rho -4.756425e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16509882 Loss_f: 0.16505812 rho 1.4350012e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16505812 Loss_f: 0.16524947 rho -6.7458245e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16505812 Loss_f: 0.16524945 rho -6.745299e-08 lambda: 10.0\n",
      "    Train error: 25.347645\n",
      "-- Epoch: 13  Batch: 12 --\n",
      "    Loss_i: 0.1700879 Loss_f: 0.16999367 rho 2.482914e-05 lambda: 0.01\n",
      "    Loss_i: 0.16999367 Loss_f: 0.17191571 rho -0.00048875227 lambda: 0.01\n",
      "    Loss_i: 0.16999367 Loss_f: 0.17214665 rho -6.975963e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16999367 Loss_f: 0.17126189 rho -4.225077e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16999367 Loss_f: 0.17022938 rho -7.874752e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16999367 Loss_f: 0.17022938 rho -7.874752e-08 lambda: 10.0\n",
      "    Train error: 26.284586\n",
      "-- Epoch: 13  Batch: 13 --\n",
      "    Loss_i: 0.16127864 Loss_f: 0.2123671 rho -0.015961 lambda: 0.01\n",
      "    Loss_i: 0.16127864 Loss_f: 0.20623167 rho -0.0016500647 lambda: 0.099999994\n",
      "    Loss_i: 0.16127864 Loss_f: 0.18052745 rho -7.191335e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16127864 Loss_f: 0.16092284 rho 1.3316169e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16092284 Loss_f: 0.1637716 rho -1.0669391e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16092284 Loss_f: 0.1637716 rho -1.066939e-06 lambda: 10.0\n",
      "    Train error: 24.684544\n",
      "-- Epoch: 13  Batch: 14 --\n",
      "    Loss_i: 0.1653028 Loss_f: 0.1957158 rho -0.01174174 lambda: 0.01\n",
      "    Loss_i: 0.1653028 Loss_f: 0.19189434 rho -0.0012109627 lambda: 0.099999994\n",
      "    Loss_i: 0.1653028 Loss_f: 0.17584263 rho -4.8875245e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1653028 Loss_f: 0.16510002 rho 9.420296e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16510002 Loss_f: 0.1665683 rho -6.819153e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16510002 Loss_f: 0.16656831 rho -6.8192213e-07 lambda: 10.0\n",
      "    Train error: 25.222261\n",
      "-- Epoch: 13  Batch: 15 --\n",
      "    Loss_i: 0.16331492 Loss_f: 0.17547317 rho -0.003317111 lambda: 0.01\n",
      "    Loss_i: 0.16331492 Loss_f: 0.17417495 rho -0.00036019497 lambda: 0.099999994\n",
      "    Loss_i: 0.16331492 Loss_f: 0.16738042 rho -1.3781285e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16331492 Loss_f: 0.16324289 rho 2.4471527e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16324289 Loss_f: 0.16377476 rho -1.8076128e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16324289 Loss_f: 0.16377476 rho -1.8076122e-07 lambda: 10.0\n",
      "    Train error: 24.7702\n",
      "-- Epoch: 13  Batch: 16 --\n",
      "    Loss_i: 0.16504456 Loss_f: 0.16868326 rho -0.0011615036 lambda: 0.01\n",
      "    Loss_i: 0.16504456 Loss_f: 0.16872457 rho -0.00014802732 lambda: 0.099999994\n",
      "    Loss_i: 0.16504456 Loss_f: 0.16649476 rho -5.9891795e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16504456 Loss_f: 0.16501349 rho 1.28655495e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16501349 Loss_f: 0.1652146 rho -8.322879e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16501349 Loss_f: 0.1652146 rho -8.322878e-08 lambda: 10.0\n",
      "    Train error: 25.323353\n",
      "-- Epoch: 13  Batch: 17 --\n",
      "    Loss_i: 0.16306953 Loss_f: 0.16464725 rho -0.0004446362 lambda: 0.01\n",
      "    Loss_i: 0.16306953 Loss_f: 0.16498165 rho -6.3686726e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16306953 Loss_f: 0.16384736 rho -2.638679e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16306953 Loss_f: 0.16305175 rho 6.041853e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16305175 Loss_f: 0.16316217 rho -3.75188e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16305175 Loss_f: 0.16316217 rho -3.75188e-08 lambda: 10.0\n",
      "    Train error: 24.718039\n",
      "-- Epoch: 13  Batch: 18 --\n",
      "    Loss_i: 0.16523628 Loss_f: 0.16581766 rho -0.00015770429 lambda: 0.01\n",
      "    Loss_i: 0.16523628 Loss_f: 0.16622685 rho -3.207635e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16523628 Loss_f: 0.16565372 rho -1.378456e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16523628 Loss_f: 0.1652254 rho 3.5991465e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1652254 Loss_f: 0.16528466 rho -1.9607068e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1652254 Loss_f: 0.16528466 rho -1.9607066e-08 lambda: 10.0\n",
      "    Train error: 25.120874\n",
      "-- Epoch: 13  Batch: 19 --\n",
      "    Loss_i: 0.16757067 Loss_f: 0.16683736 rho 0.00020665902 lambda: 0.01\n",
      "    Loss_i: 0.16683736 Loss_f: 0.16696863 rho -3.6658923e-05 lambda: 0.01\n",
      "    Loss_i: 0.16683736 Loss_f: 0.16743216 rho -2.0246021e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16683736 Loss_f: 0.16722357 rho -1.3440116e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16683736 Loss_f: 0.16691035 rho -2.5456162e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16683736 Loss_f: 0.16691035 rho -2.5456162e-08 lambda: 10.0\n",
      "    Train error: 25.671938\n",
      "-- Epoch: 13  Batch: 20 --\n",
      "    Loss_i: 0.16303813 Loss_f: 0.17193842 rho -0.0022351316 lambda: 0.01\n",
      "    Loss_i: 0.16303813 Loss_f: 0.17116214 rho -0.00023283338 lambda: 0.099999994\n",
      "    Loss_i: 0.16303813 Loss_f: 0.16603668 rho -8.716922e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16303813 Loss_f: 0.16289715 rho 4.1042345e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16289715 Loss_f: 0.1632721 rho -1.0912477e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16289715 Loss_f: 0.1632721 rho -1.09124755e-07 lambda: 10.0\n",
      "    Train error: 24.675728\n",
      "-- Epoch: 13  Batch: 21 --\n",
      "    Loss_i: 0.16534202 Loss_f: 0.16771628 rho -0.0006520036 lambda: 0.01\n",
      "    Loss_i: 0.16534202 Loss_f: 0.16800128 rho -8.82904e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16534202 Loss_f: 0.16642402 rho -3.669059e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16534202 Loss_f: 0.16530633 rho 1.2127745e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16530633 Loss_f: 0.16546085 rho -5.2518057e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16530633 Loss_f: 0.16546085 rho -5.2518054e-08 lambda: 10.0\n",
      "    Train error: 25.35771\n",
      "-- Epoch: 13  Batch: 22 --\n",
      "    Loss_i: 0.16194415 Loss_f: 0.16273041 rho -0.00019823092 lambda: 0.01\n",
      "    Loss_i: 0.16194415 Loss_f: 0.16312899 rho -3.4358098e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16194415 Loss_f: 0.16244008 rho -1.4600208e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16194415 Loss_f: 0.16192944 rho 4.3365307e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16192944 Loss_f: 0.16199939 rho -2.0621771e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16192944 Loss_f: 0.16199939 rho -2.062177e-08 lambda: 10.0\n",
      "    Train error: 24.479355\n",
      "-- Epoch: 13  Batch: 23 --\n",
      "    Loss_i: 0.16766013 Loss_f: 0.16745904 rho 5.0478844e-05 lambda: 0.01\n",
      "    Loss_i: 0.16745904 Loss_f: 0.16956104 rho -0.00053353433 lambda: 0.01\n",
      "    Loss_i: 0.16745904 Loss_f: 0.16976562 rho -8.086734e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16745904 Loss_f: 0.16882052 rho -4.9624427e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16745904 Loss_f: 0.16771331 rho -9.30491e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16745904 Loss_f: 0.16771331 rho -9.3049096e-08 lambda: 10.0\n",
      "    Train error: 25.818483\n",
      "-- Epoch: 13  Batch: 24 --\n",
      "    Loss_i: 0.16730963 Loss_f: 0.23143037 rho -0.01843896 lambda: 0.01\n",
      "    Loss_i: 0.16730963 Loss_f: 0.22359474 rho -0.0021631932 lambda: 0.099999994\n",
      "    Loss_i: 0.16730963 Loss_f: 0.1913703 rho -9.569176e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16730963 Loss_f: 0.16688064 rho 1.7120975e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16688064 Loss_f: 0.17047673 rho -1.4343602e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16688064 Loss_f: 0.17047673 rho -1.4343602e-06 lambda: 10.0\n",
      "    Train error: 25.547848\n",
      "-- Epoch: 13  Batch: 25 --\n",
      "    Loss_i: 0.16802156 Loss_f: 0.19957225 rho -0.008806438 lambda: 0.01\n",
      "    Loss_i: 0.16802156 Loss_f: 0.19574302 rho -0.00095079024 lambda: 0.099999994\n",
      "    Loss_i: 0.16802156 Loss_f: 0.17933156 rho -3.969928e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16802156 Loss_f: 0.16787936 rho 5.003146e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16787936 Loss_f: 0.16951723 rho -5.766344e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16787936 Loss_f: 0.16951723 rho -5.766344e-07 lambda: 10.0\n",
      "    Train error: 25.998846\n",
      "-- Epoch: 13  Batch: 26 --\n",
      "    Loss_i: 0.16367495 Loss_f: 0.17851397 rho -0.004139035 lambda: 0.01\n",
      "    Loss_i: 0.16367495 Loss_f: 0.17704101 rho -0.0004523345 lambda: 0.099999994\n",
      "    Loss_i: 0.16367495 Loss_f: 0.1688772 rho -1.798915e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16367495 Loss_f: 0.16360582 rho 2.3955849e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16360582 Loss_f: 0.16432488 rho -2.4907732e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16360582 Loss_f: 0.16432486 rho -2.4907212e-07 lambda: 10.0\n",
      "    Train error: 24.92775\n",
      "-- Epoch: 13  Batch: 27 --\n",
      "    Loss_i: 0.16230968 Loss_f: 0.165993 rho -0.00097042177 lambda: 0.01\n",
      "    Loss_i: 0.16230968 Loss_f: 0.16599336 rho -0.00012182468 lambda: 0.099999994\n",
      "    Loss_i: 0.16230968 Loss_f: 0.16373451 rho -4.8355582e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16230968 Loss_f: 0.16227214 rho 1.2772312e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16227214 Loss_f: 0.16246606 rho -6.596788e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16227214 Loss_f: 0.16246606 rho -6.596788e-08 lambda: 10.0\n",
      "    Train error: 24.630894\n",
      "-- Epoch: 13  Batch: 28 --\n",
      "    Loss_i: 0.16166055 Loss_f: 0.16218583 rho -0.00013319668 lambda: 0.01\n",
      "    Loss_i: 0.16166055 Loss_f: 0.16259803 rho -2.6689333e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16166055 Loss_f: 0.16204576 rho -1.1102934e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16166055 Loss_f: 0.1616462 rho 4.1412034e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1616462 Loss_f: 0.16170117 rho -1.5861971e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1616462 Loss_f: 0.16170119 rho -1.586627e-08 lambda: 10.0\n",
      "    Train error: 24.406273\n",
      "-- Epoch: 13  Batch: 29 --\n",
      "    Loss_i: 0.16348685 Loss_f: 0.16300759 rho 0.000111116526 lambda: 0.01\n",
      "    Loss_i: 0.16300759 Loss_f: 0.16317536 rho -3.875476e-05 lambda: 0.01\n",
      "    Loss_i: 0.16300759 Loss_f: 0.16352794 rho -1.397009e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16300759 Loss_f: 0.16332251 rho -8.594305e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16300759 Loss_f: 0.16306457 rho -1.5576287e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16300759 Loss_f: 0.16306457 rho -1.5576283e-08 lambda: 10.0\n",
      "    Train error: 24.841677\n",
      "-- Epoch: 13  Batch: 30 --\n",
      "    Loss_i: 0.1644845 Loss_f: 0.17041552 rho -0.0016007875 lambda: 0.01\n",
      "    Loss_i: 0.1644845 Loss_f: 0.1702897 rho -0.00019479451 lambda: 0.099999994\n",
      "    Loss_i: 0.1644845 Loss_f: 0.16680163 rho -7.969022e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1644845 Loss_f: 0.16440566 rho 2.7182955e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16440566 Loss_f: 0.16472752 rho -1.1093878e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16440566 Loss_f: 0.16472752 rho -1.1093877e-07 lambda: 10.0\n",
      "    Train error: 24.783337\n",
      "\n",
      "*** Epoch: 13 Train error: 25.193307240804035  Test error: 24.617609  Time: 713.2129371999999 sec\n",
      "\n",
      "-- Epoch: 14  Batch: 1 --\n",
      "    Loss_i: 0.16095285 Loss_f: 0.1626882 rho -0.0004177561 lambda: 0.01\n",
      "    Loss_i: 0.16095285 Loss_f: 0.16294874 rho -5.8303896e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16095285 Loss_f: 0.16170917 rho -2.2575573e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16095285 Loss_f: 0.16091129 rho 1.2432206e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16091129 Loss_f: 0.16100721 rho -2.8696338e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16091129 Loss_f: 0.16100721 rho -2.8696332e-08 lambda: 10.0\n",
      "    Train error: 24.185917\n",
      "-- Epoch: 14  Batch: 2 --\n",
      "    Loss_i: 0.16300832 Loss_f: 0.1625843 rho 9.597588e-05 lambda: 0.01\n",
      "    Loss_i: 0.1625843 Loss_f: 0.1630514 rho -0.00010309894 lambda: 0.01\n",
      "    Loss_i: 0.1625843 Loss_f: 0.16334577 rho -2.0009446e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1625843 Loss_f: 0.16301575 rho -1.1557612e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1625843 Loss_f: 0.16266075 rho -2.0517337e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1625843 Loss_f: 0.16266075 rho -2.0517334e-08 lambda: 10.0\n",
      "    Train error: 24.50823\n",
      "-- Epoch: 14  Batch: 3 --\n",
      "    Loss_i: 0.1651308 Loss_f: 0.17435032 rho -0.0022527438 lambda: 0.01\n",
      "    Loss_i: 0.1651308 Loss_f: 0.17357993 rho -0.00026223317 lambda: 0.099999994\n",
      "    Loss_i: 0.1651308 Loss_f: 0.16844799 rho -1.05813715e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1651308 Loss_f: 0.16502956 rho 3.238357e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16502956 Loss_f: 0.16549322 rho -1.4833621e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16502956 Loss_f: 0.16549322 rho -1.4833621e-07 lambda: 10.0\n",
      "    Train error: 25.109472\n",
      "-- Epoch: 14  Batch: 4 --\n",
      "    Loss_i: 0.16020106 Loss_f: 0.16416042 rho -0.0011282776 lambda: 0.01\n",
      "    Loss_i: 0.16020106 Loss_f: 0.16411643 rho -0.00012943264 lambda: 0.099999994\n",
      "    Loss_i: 0.16020106 Loss_f: 0.16169189 rho -5.0084914e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16020106 Loss_f: 0.1601408 rho 2.0277616e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1601408 Loss_f: 0.16033813 rho -6.639724e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1601408 Loss_f: 0.16033813 rho -6.639724e-08 lambda: 10.0\n",
      "    Train error: 24.136082\n",
      "-- Epoch: 14  Batch: 5 --\n",
      "    Loss_i: 0.16386184 Loss_f: 0.16600116 rho -0.00052882236 lambda: 0.01\n",
      "    Loss_i: 0.16386184 Loss_f: 0.16614336 rho -6.824102e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16386184 Loss_f: 0.16474012 rho -2.6833027e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16386184 Loss_f: 0.1638321 rho 9.106525e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1638321 Loss_f: 0.1639501 rho -3.6130125e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1638321 Loss_f: 0.1639501 rho -3.6130125e-08 lambda: 10.0\n",
      "    Train error: 25.060987\n",
      "-- Epoch: 14  Batch: 6 --\n",
      "    Loss_i: 0.15830393 Loss_f: 0.15876108 rho -0.000108566455 lambda: 0.01\n",
      "    Loss_i: 0.15830393 Loss_f: 0.15930286 rho -2.6518066e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15830393 Loss_f: 0.15872824 rho -1.1398262e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15830393 Loss_f: 0.15828723 rho 4.492608e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15828723 Loss_f: 0.15834582 rho -1.5757195e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15828723 Loss_f: 0.15834582 rho -1.5757193e-08 lambda: 10.0\n",
      "    Train error: 23.705675\n",
      "-- Epoch: 14  Batch: 7 --\n",
      "    Loss_i: 0.16696708 Loss_f: 0.1665356 rho 0.00011063573 lambda: 0.01\n",
      "    Loss_i: 0.1665356 Loss_f: 0.1680088 rho -0.00037959017 lambda: 0.01\n",
      "    Loss_i: 0.1665356 Loss_f: 0.1683459 rho -5.3280924e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1665356 Loss_f: 0.16762213 rho -3.2440555e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1665356 Loss_f: 0.16674009 rho -6.1142295e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1665356 Loss_f: 0.16674009 rho -6.114229e-08 lambda: 10.0\n",
      "    Train error: 25.546722\n",
      "-- Epoch: 14  Batch: 8 --\n",
      "    Loss_i: 0.16723965 Loss_f: 0.21084519 rho -0.013193514 lambda: 0.01\n",
      "    Loss_i: 0.16723965 Loss_f: 0.20606473 rho -0.0015879091 lambda: 0.099999994\n",
      "    Loss_i: 0.16723965 Loss_f: 0.18369675 rho -6.976179e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16723965 Loss_f: 0.16692822 rho 1.3250035e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16692822 Loss_f: 0.16938275 rho -1.0433721e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16692822 Loss_f: 0.16938275 rho -1.043372e-06 lambda: 10.0\n",
      "    Train error: 25.722282\n",
      "-- Epoch: 14  Batch: 9 --\n",
      "    Loss_i: 0.16340657 Loss_f: 0.18273956 rho -0.005295788 lambda: 0.01\n",
      "    Loss_i: 0.16340657 Loss_f: 0.1808176 rho -0.0005842914 lambda: 0.099999994\n",
      "    Loss_i: 0.16340657 Loss_f: 0.17061655 rho -2.475296e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16340657 Loss_f: 0.16330777 rho 3.399599e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16330777 Loss_f: 0.1643684 rho -3.6502936e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16330777 Loss_f: 0.16436842 rho -3.6503442e-07 lambda: 10.0\n",
      "    Train error: 24.978907\n",
      "-- Epoch: 14  Batch: 10 --\n",
      "    Loss_i: 0.16342933 Loss_f: 0.17042981 rho -0.0019990446 lambda: 0.01\n",
      "    Loss_i: 0.16342933 Loss_f: 0.17003551 rho -0.00021797775 lambda: 0.099999994\n",
      "    Loss_i: 0.16342933 Loss_f: 0.16605468 rho -8.799434e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16342933 Loss_f: 0.1633476 rho 2.7437926e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1633476 Loss_f: 0.16371383 rho -1.2290488e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1633476 Loss_f: 0.16371383 rho -1.2290488e-07 lambda: 10.0\n",
      "    Train error: 24.875696\n",
      "-- Epoch: 14  Batch: 11 --\n",
      "    Loss_i: 0.1629433 Loss_f: 0.16495416 rho -0.0005332131 lambda: 0.01\n",
      "    Loss_i: 0.1629433 Loss_f: 0.1653076 rho -7.1438466e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1629433 Loss_f: 0.16394304 rho -3.063486e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1629433 Loss_f: 0.16292106 rho 6.826944e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16292106 Loss_f: 0.16306709 rho -4.4798764e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16292106 Loss_f: 0.16306709 rho -4.479875e-08 lambda: 10.0\n",
      "    Train error: 24.812515\n",
      "-- Epoch: 14  Batch: 12 --\n",
      "    Loss_i: 0.16775943 Loss_f: 0.1678386 rho -1.8924082e-05 lambda: 0.01\n",
      "    Loss_i: 0.16775943 Loss_f: 0.16827449 rho -1.5077694e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16775943 Loss_f: 0.16797732 rho -6.524907e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16775943 Loss_f: 0.16774857 rho 3.2605758e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16774857 Loss_f: 0.16777734 rho -8.636489e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16774857 Loss_f: 0.16777734 rho -8.636486e-09 lambda: 10.0\n",
      "    Train error: 25.714247\n",
      "-- Epoch: 14  Batch: 13 --\n",
      "    Loss_i: 0.15902857 Loss_f: 0.1580927 rho 0.00025260437 lambda: 0.01\n",
      "    Loss_i: 0.1580927 Loss_f: 0.15787825 rho 5.7174886e-05 lambda: 0.01\n",
      "    Loss_i: 0.15787825 Loss_f: 0.15837796 rho -0.00013151793 lambda: 0.01\n",
      "    Loss_i: 0.15787825 Loss_f: 0.15857507 rho -2.1144424e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15787825 Loss_f: 0.15825951 rho -1.1748679e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15787825 Loss_f: 0.1579503 rho -2.2236089e-08 lambda: 9.999999\n",
      "    Train error: 23.883036\n",
      "-- Epoch: 14  Batch: 14 --\n",
      "    Loss_i: 0.16293806 Loss_f: 0.17299202 rho -0.0035541155 lambda: 0.01\n",
      "    Loss_i: 0.16293806 Loss_f: 0.17157204 rho -0.00037374947 lambda: 0.099999994\n",
      "    Loss_i: 0.16293806 Loss_f: 0.16567954 rho -1.2139991e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16293806 Loss_f: 0.16271369 rho 9.9583986e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16271369 Loss_f: 0.16301322 rho -1.3297355e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16271369 Loss_f: 0.1630132 rho -1.3296692e-07 lambda: 10.0\n",
      "    Train error: 24.714516\n",
      "-- Epoch: 14  Batch: 15 --\n",
      "    Loss_i: 0.16100405 Loss_f: 0.1633389 rho -0.0005392492 lambda: 0.01\n",
      "    Loss_i: 0.16100405 Loss_f: 0.16337569 rho -6.984442e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16100405 Loss_f: 0.16165239 rho -1.9633535e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16100405 Loss_f: 0.16086449 rho 4.2384197e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16086449 Loss_f: 0.16090974 rho -1.374323e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16086449 Loss_f: 0.16090974 rho -1.3743227e-08 lambda: 10.0\n",
      "    Train error: 24.182873\n",
      "-- Epoch: 14  Batch: 16 --\n",
      "    Loss_i: 0.16303998 Loss_f: 0.16292225 rho 3.412194e-05 lambda: 0.01\n",
      "    Loss_i: 0.16292225 Loss_f: 0.16766506 rho -0.0013255657 lambda: 0.01\n",
      "    Loss_i: 0.16292225 Loss_f: 0.16731338 rho -0.00016372286 lambda: 0.099999994\n",
      "    Loss_i: 0.16292225 Loss_f: 0.16509487 rho -8.380522e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16292225 Loss_f: 0.16327739 rho -1.3746428e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16292225 Loss_f: 0.16327739 rho -1.3746427e-07 lambda: 10.0\n",
      "    Train error: 24.727253\n",
      "-- Epoch: 14  Batch: 17 --\n",
      "    Loss_i: 0.16147503 Loss_f: 0.20972838 rho -0.010400056 lambda: 0.01\n",
      "    Loss_i: 0.16147503 Loss_f: 0.20333803 rho -0.0015253852 lambda: 0.099999994\n",
      "    Loss_i: 0.16147503 Loss_f: 0.17913218 rho -6.9111185e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16147503 Loss_f: 0.16094714 rho 2.0816304e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16094714 Loss_f: 0.16360043 rho -1.0458064e-06 lambda: 9.999999\n",
      "    Loss_i: 0.16094714 Loss_f: 0.16360043 rho -1.0458061e-06 lambda: 10.0\n",
      "    Train error: 24.331415\n",
      "-- Epoch: 14  Batch: 18 --\n",
      "    Loss_i: 0.16323732 Loss_f: 0.19093287 rho -0.0072712176 lambda: 0.01\n",
      "    Loss_i: 0.16323732 Loss_f: 0.18727057 rho -0.0008891575 lambda: 0.099999994\n",
      "    Loss_i: 0.16323732 Loss_f: 0.17251496 rho -3.5788908e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16323732 Loss_f: 0.16294968 rho 1.11432655e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16294968 Loss_f: 0.1642217 rho -4.9260177e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16294968 Loss_f: 0.1642217 rho -4.926017e-07 lambda: 10.0\n",
      "    Train error: 24.542582\n",
      "-- Epoch: 14  Batch: 19 --\n",
      "    Loss_i: 0.16504177 Loss_f: 0.17270575 rho -0.0020933344 lambda: 0.01\n",
      "    Loss_i: 0.16504177 Loss_f: 0.17202495 rho -0.00023608455 lambda: 0.099999994\n",
      "    Loss_i: 0.16504177 Loss_f: 0.16771238 rho -9.248543e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16504177 Loss_f: 0.16491307 rho 4.4679272e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16491307 Loss_f: 0.16528662 rho -1.2968816e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16491307 Loss_f: 0.16528662 rho -1.2968815e-07 lambda: 10.0\n",
      "    Train error: 25.237164\n",
      "-- Epoch: 14  Batch: 20 --\n",
      "    Loss_i: 0.16078666 Loss_f: 0.16199568 rho -0.00028130886 lambda: 0.01\n",
      "    Loss_i: 0.16078666 Loss_f: 0.16224137 rho -3.990624e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16078666 Loss_f: 0.1612478 rho -1.288093e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16078666 Loss_f: 0.16071828 rho 1.9135463e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16071828 Loss_f: 0.16076657 rho -1.351435e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16071828 Loss_f: 0.16076659 rho -1.3518519e-08 lambda: 10.0\n",
      "    Train error: 24.170673\n",
      "-- Epoch: 14  Batch: 21 --\n",
      "    Loss_i: 0.16295102 Loss_f: 0.16352998 rho -0.00015611823 lambda: 0.01\n",
      "    Loss_i: 0.16295102 Loss_f: 0.1640318 rho -3.3989687e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16295102 Loss_f: 0.16333644 rho -1.2326044e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16295102 Loss_f: 0.16290732 rho 1.40009755e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16290732 Loss_f: 0.16295145 rho -1.4139147e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16290732 Loss_f: 0.16295145 rho -1.4139146e-08 lambda: 10.0\n",
      "    Train error: 24.793139\n",
      "-- Epoch: 14  Batch: 22 --\n",
      "    Loss_i: 0.1596736 Loss_f: 0.15978688 rho -2.5442487e-05 lambda: 0.01\n",
      "    Loss_i: 0.1596736 Loss_f: 0.16023338 rho -1.4340165e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1596736 Loss_f: 0.1598701 rho -5.105685e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1596736 Loss_f: 0.1596448 rho 7.494793e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1596448 Loss_f: 0.1596665 rho -5.6454112e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1596448 Loss_f: 0.1596665 rho -5.6454104e-09 lambda: 10.0\n",
      "    Train error: 23.903292\n",
      "-- Epoch: 14  Batch: 23 --\n",
      "    Loss_i: 0.16491647 Loss_f: 0.16477622 rho 3.090971e-05 lambda: 0.01\n",
      "    Loss_i: 0.16477622 Loss_f: 0.16699159 rho -0.0004936873 lambda: 0.01\n",
      "    Loss_i: 0.16477622 Loss_f: 0.16710116 rho -6.746131e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16477622 Loss_f: 0.1660753 rho -3.886885e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16477622 Loss_f: 0.16500932 rho -6.99616e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16477622 Loss_f: 0.16500932 rho -6.99616e-08 lambda: 10.0\n",
      "    Train error: 25.141706\n",
      "-- Epoch: 14  Batch: 24 --\n",
      "    Loss_i: 0.16489987 Loss_f: 0.20978786 rho -0.011564634 lambda: 0.01\n",
      "    Loss_i: 0.16489987 Loss_f: 0.2037891 rho -0.0013250782 lambda: 0.099999994\n",
      "    Loss_i: 0.16489987 Loss_f: 0.18092348 rho -5.6417164e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16489987 Loss_f: 0.16457091 rho 1.1620944e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16457091 Loss_f: 0.16694103 rho -8.3671284e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16457091 Loss_f: 0.16694105 rho -8.367181e-07 lambda: 10.0\n",
      "    Train error: 25.033796\n",
      "-- Epoch: 14  Batch: 25 --\n",
      "    Loss_i: 0.16571805 Loss_f: 0.18666328 rho -0.005439445 lambda: 0.01\n",
      "    Loss_i: 0.16571805 Loss_f: 0.18382414 rho -0.0006081075 lambda: 0.099999994\n",
      "    Loss_i: 0.16571805 Loss_f: 0.17253731 rho -2.3594992e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16571805 Loss_f: 0.16556163 rho 5.4285255e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16556163 Loss_f: 0.16647625 rho -3.1741592e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16556163 Loss_f: 0.16647625 rho -3.1741592e-07 lambda: 10.0\n",
      "    Train error: 25.427519\n",
      "-- Epoch: 14  Batch: 26 --\n",
      "    Loss_i: 0.161223 Loss_f: 0.16656102 rho -0.0011753126 lambda: 0.01\n",
      "    Loss_i: 0.161223 Loss_f: 0.16629852 rho -0.00014470809 lambda: 0.099999994\n",
      "    Loss_i: 0.161223 Loss_f: 0.16315414 rho -5.6731865e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.161223 Loss_f: 0.16115844 rho 1.9021394e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16115844 Loss_f: 0.16141863 rho -7.664751e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16115844 Loss_f: 0.16141863 rho -7.6647495e-08 lambda: 10.0\n",
      "    Train error: 24.364918\n",
      "-- Epoch: 14  Batch: 27 --\n",
      "    Loss_i: 0.15993443 Loss_f: 0.16057368 rho -0.00015244169 lambda: 0.01\n",
      "    Loss_i: 0.15993443 Loss_f: 0.1608919 rho -2.6912785e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15993443 Loss_f: 0.1602496 rho -9.02013e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15993443 Loss_f: 0.15990393 rho 8.745618e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15990393 Loss_f: 0.15993887 rho -1.001921e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15990393 Loss_f: 0.15993887 rho -1.00192095e-08 lambda: 10.0\n",
      "    Train error: 24.059612\n",
      "-- Epoch: 14  Batch: 28 --\n",
      "    Loss_i: 0.15931933 Loss_f: 0.15907258 rho 5.5680615e-05 lambda: 0.01\n",
      "    Loss_i: 0.15907258 Loss_f: 0.16064148 rho -0.00034206183 lambda: 0.01\n",
      "    Loss_i: 0.15907258 Loss_f: 0.16082774 rho -4.4614815e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15907258 Loss_f: 0.1600574 rho -2.5455522e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15907258 Loss_f: 0.15925045 rho -4.6054762e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15907258 Loss_f: 0.15925045 rho -4.605476e-08 lambda: 10.0\n",
      "    Train error: 23.68739\n",
      "-- Epoch: 14  Batch: 29 --\n",
      "    Loss_i: 0.1611984 Loss_f: 0.19893782 rho -0.0071869744 lambda: 0.01\n",
      "    Loss_i: 0.1611984 Loss_f: 0.19460866 rho -0.0009786107 lambda: 0.099999994\n",
      "    Loss_i: 0.1611984 Loss_f: 0.17505114 rho -4.2883126e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1611984 Loss_f: 0.16095959 rho 7.435326e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16095959 Loss_f: 0.16296926 rho -6.257383e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16095959 Loss_f: 0.16296926 rho -6.257382e-07 lambda: 10.0\n",
      "    Train error: 24.341377\n",
      "-- Epoch: 14  Batch: 30 --\n",
      "    Loss_i: 0.16229133 Loss_f: 0.17475043 rho -0.0029519403 lambda: 0.01\n",
      "    Loss_i: 0.16229133 Loss_f: 0.17378353 rho -0.00037752703 lambda: 0.099999994\n",
      "    Loss_i: 0.16229133 Loss_f: 0.16682914 rho -1.550637e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16229133 Loss_f: 0.16215952 rho 4.522523e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16215952 Loss_f: 0.16278528 rho -2.1456891e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16215952 Loss_f: 0.16278528 rho -2.1456891e-07 lambda: 10.0\n",
      "    Train error: 24.204275\n",
      "\n",
      "*** Epoch: 14 Train error: 24.63677558898926  Test error: 23.956993  Time: 717.3376916000016 sec\n",
      "\n",
      "-- Epoch: 15  Batch: 1 --\n",
      "    Loss_i: 0.15862457 Loss_f: 0.16128442 rho -0.0005932103 lambda: 0.01\n",
      "    Loss_i: 0.15862457 Loss_f: 0.16140805 rho -7.6781354e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15862457 Loss_f: 0.15971126 rho -3.070297e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15862457 Loss_f: 0.15858775 rho 1.0428589e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15858775 Loss_f: 0.15873547 rho -4.183236e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15858775 Loss_f: 0.15873547 rho -4.1832358e-08 lambda: 10.0\n",
      "    Train error: 23.598473\n",
      "-- Epoch: 15  Batch: 2 --\n",
      "    Loss_i: 0.16053647 Loss_f: 0.16031197 rho 4.7166526e-05 lambda: 0.01\n",
      "    Loss_i: 0.16031197 Loss_f: 0.16144305 rho -0.00023172151 lambda: 0.01\n",
      "    Loss_i: 0.16031197 Loss_f: 0.16165218 rho -3.237569e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16031197 Loss_f: 0.16104397 rho -1.8005768e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16031197 Loss_f: 0.16043839 rho -3.115384e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16031197 Loss_f: 0.16043839 rho -3.115384e-08 lambda: 10.0\n",
      "    Train error: 23.951824\n",
      "-- Epoch: 15  Batch: 3 --\n",
      "    Loss_i: 0.16291082 Loss_f: 0.18029536 rho -0.0041144723 lambda: 0.01\n",
      "    Loss_i: 0.16291082 Loss_f: 0.17823167 rho -0.00047897873 lambda: 0.099999994\n",
      "    Loss_i: 0.16291082 Loss_f: 0.16861135 rho -1.8412606e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16291082 Loss_f: 0.16271567 rho 6.324136e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16271567 Loss_f: 0.16346654 rho -2.4328833e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16271567 Loss_f: 0.16346654 rho -2.4328833e-07 lambda: 10.0\n",
      "    Train error: 24.557995\n",
      "-- Epoch: 15  Batch: 4 --\n",
      "    Loss_i: 0.15788144 Loss_f: 0.16376242 rho -0.001541227 lambda: 0.01\n",
      "    Loss_i: 0.15788144 Loss_f: 0.16338126 rho -0.00016694122 lambda: 0.099999994\n",
      "    Loss_i: 0.15788144 Loss_f: 0.15993331 rho -6.328383e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15788144 Loss_f: 0.15779254 rho 2.7462757e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15779254 Loss_f: 0.15806523 rho -8.4224915e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15779254 Loss_f: 0.15806523 rho -8.422491e-08 lambda: 10.0\n",
      "    Train error: 23.559105\n",
      "-- Epoch: 15  Batch: 5 --\n",
      "    Loss_i: 0.16147566 Loss_f: 0.16433816 rho -0.00065860694 lambda: 0.01\n",
      "    Loss_i: 0.16147566 Loss_f: 0.16436894 rho -8.217213e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16147566 Loss_f: 0.16260272 rho -3.2778032e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16147566 Loss_f: 0.1614366 rho 1.1385836e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1614366 Loss_f: 0.16159326 rho -4.5671243e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1614366 Loss_f: 0.16159326 rho -4.5671232e-08 lambda: 10.0\n",
      "    Train error: 24.469091\n",
      "-- Epoch: 15  Batch: 6 --\n",
      "    Loss_i: 0.15585573 Loss_f: 0.15690847 rho -0.000246368 lambda: 0.01\n",
      "    Loss_i: 0.15585573 Loss_f: 0.15731333 rho -3.857024e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15585573 Loss_f: 0.15638916 rho -1.430231e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15585573 Loss_f: 0.15582958 rho 7.021018e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15582958 Loss_f: 0.15589502 rho -1.756934e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15582958 Loss_f: 0.15589502 rho -1.756934e-08 lambda: 10.0\n",
      "    Train error: 23.140915\n",
      "-- Epoch: 15  Batch: 7 --\n",
      "    Loss_i: 0.16458674 Loss_f: 0.16416834 rho 9.634529e-05 lambda: 0.01\n",
      "    Loss_i: 0.16416834 Loss_f: 0.16596934 rho -0.0004171817 lambda: 0.01\n",
      "    Loss_i: 0.16416834 Loss_f: 0.16625592 rho -5.673565e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16416834 Loss_f: 0.16538228 rho -3.3573845e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16416834 Loss_f: 0.16439304 rho -6.225349e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16416834 Loss_f: 0.16439302 rho -6.2249356e-08 lambda: 10.0\n",
      "    Train error: 24.939964\n",
      "-- Epoch: 15  Batch: 8 --\n",
      "    Loss_i: 0.16490506 Loss_f: 0.20899083 rho -0.014437387 lambda: 0.01\n",
      "    Loss_i: 0.16490506 Loss_f: 0.20367932 rho -0.0015832079 lambda: 0.099999994\n",
      "    Loss_i: 0.16490506 Loss_f: 0.1811354 rho -6.7947876e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16490506 Loss_f: 0.1645764 rho 1.3794163e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1645764 Loss_f: 0.16696437 rho -1.0016458e-06 lambda: 9.999999\n",
      "    Loss_i: 0.1645764 Loss_f: 0.16696437 rho -1.0016457e-06 lambda: 10.0\n",
      "    Train error: 25.12244\n",
      "-- Epoch: 15  Batch: 9 --\n",
      "    Loss_i: 0.16105558 Loss_f: 0.17932259 rho -0.0036667404 lambda: 0.01\n",
      "    Loss_i: 0.16105558 Loss_f: 0.17732972 rho -0.00048518018 lambda: 0.099999994\n",
      "    Loss_i: 0.16105558 Loss_f: 0.16737683 rho -1.9806577e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16105558 Loss_f: 0.16093715 rho 3.729966e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16093715 Loss_f: 0.16180092 rho -2.720312e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16093715 Loss_f: 0.16180092 rho -2.7203112e-07 lambda: 10.0\n",
      "    Train error: 24.369522\n",
      "-- Epoch: 15  Batch: 10 --\n",
      "    Loss_i: 0.16093901 Loss_f: 0.16615814 rho -0.0012782941 lambda: 0.01\n",
      "    Loss_i: 0.16093901 Loss_f: 0.16590422 rho -0.000143324 lambda: 0.099999994\n",
      "    Loss_i: 0.16093901 Loss_f: 0.16286343 rho -5.655955e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16093901 Loss_f: 0.1608593 rho 2.3468647e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1608593 Loss_f: 0.16112185 rho -7.7293336e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1608593 Loss_f: 0.16112185 rho -7.729333e-08 lambda: 10.0\n",
      "    Train error: 24.244696\n",
      "-- Epoch: 15  Batch: 11 --\n",
      "    Loss_i: 0.1604886 Loss_f: 0.16201329 rho -0.00037632577 lambda: 0.01\n",
      "    Loss_i: 0.1604886 Loss_f: 0.16239278 rho -5.4106455e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1604886 Loss_f: 0.16127667 rho -2.273636e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1604886 Loss_f: 0.16046306 rho 7.380033e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16046306 Loss_f: 0.1605764 rho -3.2744403e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16046306 Loss_f: 0.1605764 rho -3.2744403e-08 lambda: 10.0\n",
      "    Train error: 24.185\n",
      "-- Epoch: 15  Batch: 12 --\n",
      "    Loss_i: 0.16533636 Loss_f: 0.16536543 rho -6.6351654e-06 lambda: 0.01\n",
      "    Loss_i: 0.16533636 Loss_f: 0.16578795 rho -1.2543288e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16533636 Loss_f: 0.16551177 rho -4.980358e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16533636 Loss_f: 0.16531688 rho 5.5417937e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16531688 Loss_f: 0.16533737 rho -5.830025e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16531688 Loss_f: 0.16533737 rho -5.830025e-09 lambda: 10.0\n",
      "    Train error: 25.133768\n",
      "-- Epoch: 15  Batch: 13 --\n",
      "    Loss_i: 0.15637209 Loss_f: 0.15566656 rho 0.00015821794 lambda: 0.01\n",
      "    Loss_i: 0.15566656 Loss_f: 0.15581238 rho -3.234253e-05 lambda: 0.01\n",
      "    Loss_i: 0.15566656 Loss_f: 0.15616722 rho -1.3074883e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15566656 Loss_f: 0.15592453 rho -6.8585865e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15566656 Loss_f: 0.15570584 rho -1.046208e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15566656 Loss_f: 0.15570584 rho -1.0462079e-08 lambda: 10.0\n",
      "    Train error: 23.262966\n",
      "-- Epoch: 15  Batch: 14 --\n",
      "    Loss_i: 0.16051617 Loss_f: 0.16139702 rho -0.00023777926 lambda: 0.01\n",
      "    Loss_i: 0.16051617 Loss_f: 0.16169949 rho -3.882826e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16051617 Loss_f: 0.16086984 rho -1.1860482e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16051617 Loss_f: 0.16044657 rho 2.339371e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16044657 Loss_f: 0.16047949 rho -1.1063251e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16044657 Loss_f: 0.16047949 rho -1.106325e-08 lambda: 10.0\n",
      "    Train error: 24.117249\n",
      "-- Epoch: 15  Batch: 15 --\n",
      "    Loss_i: 0.1587074 Loss_f: 0.15858072 rho 2.6577152e-05 lambda: 0.01\n",
      "    Loss_i: 0.15858072 Loss_f: 0.16074249 rho -0.00045299245 lambda: 0.01\n",
      "    Loss_i: 0.15858072 Loss_f: 0.16081129 rho -5.296296e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15858072 Loss_f: 0.1597612 rho -2.8407758e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15858072 Loss_f: 0.1587841 rho -4.9009895e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15858072 Loss_f: 0.1587841 rho -4.900989e-08 lambda: 10.0\n",
      "    Train error: 23.660805\n",
      "-- Epoch: 15  Batch: 16 --\n",
      "    Loss_i: 0.16086747 Loss_f: 0.18474378 rho -0.0077550104 lambda: 0.01\n",
      "    Loss_i: 0.16086747 Loss_f: 0.18189551 rho -0.00082892936 lambda: 0.099999994\n",
      "    Loss_i: 0.16086747 Loss_f: 0.16903308 rho -3.2891832e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16086747 Loss_f: 0.1605953 rho 1.0987234e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1605953 Loss_f: 0.16171604 rho -4.520337e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1605953 Loss_f: 0.16171606 rho -4.5203967e-07 lambda: 10.0\n",
      "    Train error: 24.187101\n",
      "-- Epoch: 15  Batch: 17 --\n",
      "    Loss_i: 0.1585954 Loss_f: 0.16825306 rho -0.002270778 lambda: 0.01\n",
      "    Loss_i: 0.1585954 Loss_f: 0.16744298 rho -0.00025554796 lambda: 0.099999994\n",
      "    Loss_i: 0.1585954 Loss_f: 0.16211355 rho -1.0399136e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1585954 Loss_f: 0.15845826 rho 4.0630187e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15845826 Loss_f: 0.15896294 rho -1.4949725e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15845826 Loss_f: 0.15896294 rho -1.4949723e-07 lambda: 10.0\n",
      "    Train error: 23.59082\n",
      "-- Epoch: 15  Batch: 18 --\n",
      "    Loss_i: 0.16088536 Loss_f: 0.16609521 rho -0.0013465012 lambda: 0.01\n",
      "    Loss_i: 0.16088536 Loss_f: 0.16589998 rho -0.0001519562 lambda: 0.099999994\n",
      "    Loss_i: 0.16088536 Loss_f: 0.16286884 rho -6.1159476e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16088536 Loss_f: 0.16081023 rho 2.3207159e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16081023 Loss_f: 0.16108757 rho -8.5642874e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16081023 Loss_f: 0.16108757 rho -8.564286e-08 lambda: 10.0\n",
      "    Train error: 24.012188\n",
      "-- Epoch: 15  Batch: 19 --\n",
      "    Loss_i: 0.16258338 Loss_f: 0.16350758 rho -0.00021771448 lambda: 0.01\n",
      "    Loss_i: 0.16258338 Loss_f: 0.1638476 rho -3.6404785e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16258338 Loss_f: 0.1630602 rho -1.4043077e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16258338 Loss_f: 0.16254479 rho 1.1392382e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16254479 Loss_f: 0.16260429 rho -1.756166e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16254479 Loss_f: 0.16260429 rho -1.756166e-08 lambda: 10.0\n",
      "    Train error: 24.56748\n",
      "-- Epoch: 15  Batch: 20 --\n",
      "    Loss_i: 0.15864766 Loss_f: 0.15833883 rho 6.0867747e-05 lambda: 0.01\n",
      "    Loss_i: 0.15833883 Loss_f: 0.15897712 rho -0.00012342041 lambda: 0.01\n",
      "    Loss_i: 0.15833883 Loss_f: 0.15921521 rho -1.9928555e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15833883 Loss_f: 0.15882693 rho -1.1298107e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15833883 Loss_f: 0.15842448 rho -1.9861432e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15833883 Loss_f: 0.15842448 rho -1.986143e-08 lambda: 10.0\n",
      "    Train error: 23.558207\n",
      "-- Epoch: 15  Batch: 21 --\n",
      "    Loss_i: 0.16092528 Loss_f: 0.1757079 rho -0.003577168 lambda: 0.01\n",
      "    Loss_i: 0.16092528 Loss_f: 0.17412922 rho -0.0004263314 lambda: 0.099999994\n",
      "    Loss_i: 0.16092528 Loss_f: 0.16591918 rho -1.6682103e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16092528 Loss_f: 0.16076733 rho 5.2947033e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16076733 Loss_f: 0.1614406 rho -2.256735e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16076733 Loss_f: 0.1614406 rho -2.2567347e-07 lambda: 10.0\n",
      "    Train error: 24.27866\n",
      "-- Epoch: 15  Batch: 22 --\n",
      "    Loss_i: 0.1575309 Loss_f: 0.16432099 rho -0.0014644014 lambda: 0.01\n",
      "    Loss_i: 0.1575309 Loss_f: 0.16382492 rho -0.0001692279 lambda: 0.099999994\n",
      "    Loss_i: 0.1575309 Loss_f: 0.15986511 rho -6.434759e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1575309 Loss_f: 0.15745215 rho 2.1764949e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15745215 Loss_f: 0.15775865 rho -8.466647e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15745215 Loss_f: 0.15775864 rho -8.466234e-08 lambda: 10.0\n",
      "    Train error: 23.393965\n",
      "-- Epoch: 15  Batch: 23 --\n",
      "    Loss_i: 0.16288584 Loss_f: 0.1655362 rho -0.00064796326 lambda: 0.01\n",
      "    Loss_i: 0.16288584 Loss_f: 0.16564815 rho -8.735436e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16288584 Loss_f: 0.16394445 rho -3.4489424e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16288584 Loss_f: 0.1628465 rho 1.2855531e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1628465 Loss_f: 0.16298735 rho -4.6019554e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1628465 Loss_f: 0.16298735 rho -4.6019547e-08 lambda: 10.0\n",
      "    Train error: 24.47638\n",
      "-- Epoch: 15  Batch: 24 --\n",
      "    Loss_i: 0.1623405 Loss_f: 0.16322921 rho -0.00020409239 lambda: 0.01\n",
      "    Loss_i: 0.1623405 Loss_f: 0.16350687 rho -3.1303553e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1623405 Loss_f: 0.16281004 rho -1.2817925e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1623405 Loss_f: 0.16232361 rho 4.620917e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16232361 Loss_f: 0.16238879 rho -1.7821023e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16232361 Loss_f: 0.16238879 rho -1.7821023e-08 lambda: 10.0\n",
      "    Train error: 24.44626\n",
      "-- Epoch: 15  Batch: 25 --\n",
      "    Loss_i: 0.1634654 Loss_f: 0.1634253 rho 8.388085e-06 lambda: 0.01\n",
      "    Loss_i: 0.1634253 Loss_f: 0.16507345 rho -0.00034734313 lambda: 0.01\n",
      "    Loss_i: 0.1634253 Loss_f: 0.16520165 rho -4.4418408e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1634253 Loss_f: 0.16444148 rho -2.589306e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1634253 Loss_f: 0.16361351 rho -4.805014e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1634253 Loss_f: 0.16361351 rho -4.8050133e-08 lambda: 10.0\n",
      "    Train error: 24.8865\n",
      "-- Epoch: 15  Batch: 26 --\n",
      "    Loss_i: 0.15957414 Loss_f: 0.19589464 rho -0.0076175947 lambda: 0.01\n",
      "    Loss_i: 0.15957414 Loss_f: 0.19174466 rho -0.00095069956 lambda: 0.099999994\n",
      "    Loss_i: 0.15957414 Loss_f: 0.17240296 rho -3.9528437e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15957414 Loss_f: 0.15924174 rho 1.0285851e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15924174 Loss_f: 0.16101016 rho -5.4708505e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15924174 Loss_f: 0.16101016 rho -5.4708494e-07 lambda: 10.0\n",
      "    Train error: 23.770605\n",
      "-- Epoch: 15  Batch: 27 --\n",
      "    Loss_i: 0.1581602 Loss_f: 0.16858141 rho -0.002467847 lambda: 0.01\n",
      "    Loss_i: 0.1581602 Loss_f: 0.16770272 rho -0.00026742107 lambda: 0.099999994\n",
      "    Loss_i: 0.1581602 Loss_f: 0.16189349 rho -1.0657698e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1581602 Loss_f: 0.15805607 rho 2.9782182e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15805607 Loss_f: 0.1585733 rho -1.4788368e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15805607 Loss_f: 0.1585733 rho -1.4788368e-07 lambda: 10.0\n",
      "    Train error: 23.562227\n",
      "-- Epoch: 15  Batch: 28 --\n",
      "    Loss_i: 0.15699793 Loss_f: 0.16015139 rho -0.00061250775 lambda: 0.01\n",
      "    Loss_i: 0.15699793 Loss_f: 0.16012402 rho -7.62376e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15699793 Loss_f: 0.15820903 rho -3.0310255e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15699793 Loss_f: 0.15695484 rho 1.0813627e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15695484 Loss_f: 0.1571191 rho -4.1213895e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15695484 Loss_f: 0.1571191 rho -4.121389e-08 lambda: 10.0\n",
      "    Train error: 23.212208\n",
      "-- Epoch: 15  Batch: 29 --\n",
      "    Loss_i: 0.15911935 Loss_f: 0.15943164 rho -6.3287465e-05 lambda: 0.01\n",
      "    Loss_i: 0.15911935 Loss_f: 0.15980943 rho -1.671678e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15911935 Loss_f: 0.15939541 rho -6.820665e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15911935 Loss_f: 0.15910333 rho 3.965699e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15910333 Loss_f: 0.15914017 rho -9.119957e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15910333 Loss_f: 0.15914017 rho -9.119955e-09 lambda: 10.0\n",
      "    Train error: 23.757154\n",
      "-- Epoch: 15  Batch: 30 --\n",
      "    Loss_i: 0.16031003 Loss_f: 0.15950347 rho 0.00017351037 lambda: 0.01\n",
      "    Loss_i: 0.15950347 Loss_f: 0.15978786 rho -6.021994e-05 lambda: 0.01\n",
      "    Loss_i: 0.15950347 Loss_f: 0.16016677 rho -1.7945133e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15950347 Loss_f: 0.15987496 rho -1.033739e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15950347 Loss_f: 0.15956667 rho -1.7635946e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15950347 Loss_f: 0.15956667 rho -1.7635946e-08 lambda: 10.0\n",
      "    Train error: 23.560543\n",
      "\n",
      "*** Epoch: 15 Train error: 24.05247033437093  Test error: 23.56323  Time: 711.0840571999997 sec\n",
      "\n",
      "-- Epoch: 16  Batch: 1 --\n",
      "    Loss_i: 0.15660478 Loss_f: 0.16063607 rho -0.00092718506 lambda: 0.01\n",
      "    Loss_i: 0.15660478 Loss_f: 0.16037703 rho -0.000102401784 lambda: 0.099999994\n",
      "    Loss_i: 0.15660478 Loss_f: 0.15778053 rho -3.2502924e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15660478 Loss_f: 0.15647388 rho 3.6255074e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15647388 Loss_f: 0.15658532 rho -3.087048e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15647388 Loss_f: 0.15658532 rho -3.0870478e-08 lambda: 10.0\n",
      "    Train error: 23.08157\n",
      "-- Epoch: 16  Batch: 2 --\n",
      "    Loss_i: 0.15849663 Loss_f: 0.1583103 rho 3.762158e-05 lambda: 0.01\n",
      "    Loss_i: 0.1583103 Loss_f: 0.16010986 rho -0.00035274762 lambda: 0.01\n",
      "    Loss_i: 0.1583103 Loss_f: 0.16019645 rho -4.3314194e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1583103 Loss_f: 0.15931624 rho -2.3503992e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1583103 Loss_f: 0.15847903 rho -3.949526e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1583103 Loss_f: 0.15847903 rho -3.949525e-08 lambda: 10.0\n",
      "    Train error: 23.36243\n",
      "-- Epoch: 16  Batch: 3 --\n",
      "    Loss_i: 0.16080922 Loss_f: 0.17028117 rho -0.001953341 lambda: 0.01\n",
      "    Loss_i: 0.16080922 Loss_f: 0.16936745 rho -0.00022618828 lambda: 0.099999994\n",
      "    Loss_i: 0.16080922 Loss_f: 0.16401327 rho -8.713441e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16080922 Loss_f: 0.16063869 rho 4.651028e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16063869 Loss_f: 0.16106598 rho -1.1655499e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16063869 Loss_f: 0.16106598 rho -1.1655498e-07 lambda: 10.0\n",
      "    Train error: 23.94977\n",
      "-- Epoch: 16  Batch: 4 --\n",
      "    Loss_i: 0.1559941 Loss_f: 0.16179648 rho -0.0013536671 lambda: 0.01\n",
      "    Loss_i: 0.1559941 Loss_f: 0.16125447 rho -0.00014348805 lambda: 0.099999994\n",
      "    Loss_i: 0.1559941 Loss_f: 0.15782139 rho -5.0701187e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1559941 Loss_f: 0.15587567 rho 3.2918337e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15587567 Loss_f: 0.15609694 rho -6.150739e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15587567 Loss_f: 0.15609694 rho -6.150738e-08 lambda: 10.0\n",
      "    Train error: 23.030565\n",
      "-- Epoch: 16  Batch: 5 --\n",
      "    Loss_i: 0.1595029 Loss_f: 0.1632295 rho -0.00076953456 lambda: 0.01\n",
      "    Loss_i: 0.1595029 Loss_f: 0.16309242 rho -9.479871e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1595029 Loss_f: 0.16084604 rho -3.6490092e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1595029 Loss_f: 0.1594437 rho 1.6126096e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1594437 Loss_f: 0.15962508 rho -4.9426376e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1594437 Loss_f: 0.15962508 rho -4.9426376e-08 lambda: 10.0\n",
      "    Train error: 23.923851\n",
      "-- Epoch: 16  Batch: 6 --\n",
      "    Loss_i: 0.15376054 Loss_f: 0.1550744 rho -0.00024932518 lambda: 0.01\n",
      "    Loss_i: 0.15376054 Loss_f: 0.15540147 rho -3.8926384e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15376054 Loss_f: 0.15438849 rho -1.5278403e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15376054 Loss_f: 0.153716 rho 1.0864611e-08 lambda: 9.999999\n",
      "    Loss_i: 0.153716 Loss_f: 0.15379447 rho -1.9141815e-08 lambda: 9.999999\n",
      "    Loss_i: 0.153716 Loss_f: 0.15379447 rho -1.9141815e-08 lambda: 10.0\n",
      "    Train error: 22.565329\n",
      "-- Epoch: 16  Batch: 7 --\n",
      "    Loss_i: 0.16218252 Loss_f: 0.16224256 rho -1.257495e-05 lambda: 0.01\n",
      "    Loss_i: 0.16218252 Loss_f: 0.16277075 rho -1.5020014e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16218252 Loss_f: 0.16241825 rho -6.153871e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16218252 Loss_f: 0.16216512 rho 4.5539275e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16216512 Loss_f: 0.16219482 rho -7.769944e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16216512 Loss_f: 0.16219482 rho -7.769944e-09 lambda: 10.0\n",
      "    Train error: 24.28822\n",
      "-- Epoch: 16  Batch: 8 --\n",
      "    Loss_i: 0.16255943 Loss_f: 0.16192605 rho 0.00013026515 lambda: 0.01\n",
      "    Loss_i: 0.16192605 Loss_f: 0.16216312 rho -4.810092e-05 lambda: 0.01\n",
      "    Loss_i: 0.16192605 Loss_f: 0.16248834 rho -1.5644995e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16192605 Loss_f: 0.16222169 rho -8.542942e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.16192605 Loss_f: 0.16197473 rho -1.4121899e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16192605 Loss_f: 0.16197473 rho -1.4121898e-08 lambda: 10.0\n",
      "    Train error: 24.562483\n",
      "-- Epoch: 16  Batch: 9 --\n",
      "    Loss_i: 0.15902567 Loss_f: 0.16139379 rho -0.00048132002 lambda: 0.01\n",
      "    Loss_i: 0.15902567 Loss_f: 0.1613291 rho -6.255481e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15902567 Loss_f: 0.15975718 rho -2.0556831e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15902567 Loss_f: 0.15893885 rho 2.44815e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15893885 Loss_f: 0.1590103 rho -2.0148752e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15893885 Loss_f: 0.1590103 rho -2.014875e-08 lambda: 10.0\n",
      "    Train error: 23.869379\n",
      "-- Epoch: 16  Batch: 10 --\n",
      "    Loss_i: 0.158765 Loss_f: 0.15913725 rho -7.980123e-05 lambda: 0.01\n",
      "    Loss_i: 0.158765 Loss_f: 0.15950614 rho -1.8273477e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.158765 Loss_f: 0.15901811 rho -6.3358004e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.158765 Loss_f: 0.15873459 rho 7.624577e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15873459 Loss_f: 0.15876094 rho -6.604694e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15873459 Loss_f: 0.15876094 rho -6.6046932e-09 lambda: 10.0\n",
      "    Train error: 23.643915\n",
      "-- Epoch: 16  Batch: 11 --\n",
      "    Loss_i: 0.15839544 Loss_f: 0.15872325 rho -6.82095e-05 lambda: 0.01\n",
      "    Loss_i: 0.15839544 Loss_f: 0.15913248 rho -1.802945e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15839544 Loss_f: 0.15866368 rho -6.6788596e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15839544 Loss_f: 0.15836968 rho 6.42655e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15836968 Loss_f: 0.15840001 rho -7.567931e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15836968 Loss_f: 0.15840001 rho -7.56793e-09 lambda: 10.0\n",
      "    Train error: 23.68011\n",
      "-- Epoch: 16  Batch: 12 --\n",
      "    Loss_i: 0.16329797 Loss_f: 0.16296217 rho 6.792734e-05 lambda: 0.01\n",
      "    Loss_i: 0.16296217 Loss_f: 0.16373032 rho -0.00015210327 lambda: 0.01\n",
      "    Loss_i: 0.16296217 Loss_f: 0.16398728 rho -2.6126654e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16296217 Loss_f: 0.1635425 rho -1.5227819e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16296217 Loss_f: 0.16306596 rho -2.731447e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16296217 Loss_f: 0.16306596 rho -2.731447e-08 lambda: 10.0\n",
      "    Train error: 24.532942\n",
      "-- Epoch: 16  Batch: 13 --\n",
      "    Loss_i: 0.15430625 Loss_f: 0.16358352 rho -0.0018962683 lambda: 0.01\n",
      "    Loss_i: 0.15430625 Loss_f: 0.16268644 rho -0.0002044872 lambda: 0.099999994\n",
      "    Loss_i: 0.15430625 Loss_f: 0.15737297 rho -7.631067e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15430625 Loss_f: 0.1540754 rho 5.7557013e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1540754 Loss_f: 0.1544544 rho -9.451045e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1540754 Loss_f: 0.1544544 rho -9.451043e-08 lambda: 10.0\n",
      "    Train error: 22.849468\n",
      "-- Epoch: 16  Batch: 14 --\n",
      "    Loss_i: 0.1586778 Loss_f: 0.16186382 rho -0.00074696634 lambda: 0.01\n",
      "    Loss_i: 0.1586778 Loss_f: 0.16187334 rho -0.00010050117 lambda: 0.099999994\n",
      "    Loss_i: 0.1586778 Loss_f: 0.15983817 rho -3.7784273e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1586778 Loss_f: 0.15862432 rho 1.74762e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15862432 Loss_f: 0.15876596 rho -4.6283063e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15862432 Loss_f: 0.15876596 rho -4.628306e-08 lambda: 10.0\n",
      "    Train error: 23.624315\n",
      "-- Epoch: 16  Batch: 15 --\n",
      "    Loss_i: 0.1567038 Loss_f: 0.15709215 rho -7.331821e-05 lambda: 0.01\n",
      "    Loss_i: 0.1567038 Loss_f: 0.15744673 rho -1.6263735e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1567038 Loss_f: 0.15700936 rho -6.797672e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1567038 Loss_f: 0.15668955 rho 3.174255e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15668955 Loss_f: 0.15673123 rho -9.28767e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15668955 Loss_f: 0.15673123 rho -9.287669e-09 lambda: 10.0\n",
      "    Train error: 23.080164\n",
      "-- Epoch: 16  Batch: 16 --\n",
      "    Loss_i: 0.15873468 Loss_f: 0.1583725 rho 8.0615464e-05 lambda: 0.01\n",
      "    Loss_i: 0.1583725 Loss_f: 0.15914652 rho -0.0001684542 lambda: 0.01\n",
      "    Loss_i: 0.1583725 Loss_f: 0.15937385 rho -2.7169703e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1583725 Loss_f: 0.15895827 rho -1.6295741e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1583725 Loss_f: 0.15848035 rho -3.007666e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1583725 Loss_f: 0.15848033 rho -3.00725e-08 lambda: 10.0\n",
      "    Train error: 23.660286\n",
      "-- Epoch: 16  Batch: 17 --\n",
      "    Loss_i: 0.15679869 Loss_f: 0.17481247 rho -0.0041222563 lambda: 0.01\n",
      "    Loss_i: 0.15679869 Loss_f: 0.17278284 rho -0.00042974344 lambda: 0.099999994\n",
      "    Loss_i: 0.15679869 Loss_f: 0.16305178 rho -1.7111031e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15679869 Loss_f: 0.15663758 rho 4.4165297e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15663758 Loss_f: 0.15751101 rho -2.3938932e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15663758 Loss_f: 0.157511 rho -2.3938517e-07 lambda: 10.0\n",
      "    Train error: 23.196808\n",
      "-- Epoch: 16  Batch: 18 --\n",
      "    Loss_i: 0.1588292 Loss_f: 0.16597193 rho -0.0015756264 lambda: 0.01\n",
      "    Loss_i: 0.1588292 Loss_f: 0.16540962 rho -0.0001776406 lambda: 0.099999994\n",
      "    Loss_i: 0.1588292 Loss_f: 0.16138548 rho -7.058705e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1588292 Loss_f: 0.15875779 rho 1.976281e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15875779 Loss_f: 0.15911032 rho -9.7529195e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15875779 Loss_f: 0.15911032 rho -9.7529195e-08 lambda: 10.0\n",
      "    Train error: 23.530012\n",
      "-- Epoch: 16  Batch: 19 --\n",
      "    Loss_i: 0.16102038 Loss_f: 0.16173795 rho -0.00016664821 lambda: 0.01\n",
      "    Loss_i: 0.16102038 Loss_f: 0.1621962 rho -3.1733623e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16102038 Loss_f: 0.16149718 rho -1.3079917e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16102038 Loss_f: 0.16098441 rho 9.884376e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16098441 Loss_f: 0.16104706 rho -1.7214997e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16098441 Loss_f: 0.16104707 rho -1.7219088e-08 lambda: 10.0\n",
      "    Train error: 24.205853\n",
      "-- Epoch: 16  Batch: 20 --\n",
      "    Loss_i: 0.15690628 Loss_f: 0.15646759 rho 7.997449e-05 lambda: 0.01\n",
      "    Loss_i: 0.15646759 Loss_f: 0.15677224 rho -5.4464104e-05 lambda: 0.01\n",
      "    Loss_i: 0.15646759 Loss_f: 0.15705259 rho -1.3296366e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15646759 Loss_f: 0.15680884 rho -7.972531e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15646759 Loss_f: 0.15652786 rho -1.4121258e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15646759 Loss_f: 0.15652786 rho -1.4121258e-08 lambda: 10.0\n",
      "    Train error: 23.060253\n",
      "-- Epoch: 16  Batch: 21 --\n",
      "    Loss_i: 0.15883747 Loss_f: 0.16890988 rho -0.0023171608 lambda: 0.01\n",
      "    Loss_i: 0.15883747 Loss_f: 0.1679987 rho -0.00025416518 lambda: 0.099999994\n",
      "    Loss_i: 0.15883747 Loss_f: 0.16235194 rho -9.955446e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15883747 Loss_f: 0.15872212 rho 3.274406e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15872212 Loss_f: 0.15919547 rho -1.3438009e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15872212 Loss_f: 0.15919547 rho -1.3438009e-07 lambda: 10.0\n",
      "    Train error: 23.729464\n",
      "-- Epoch: 16  Batch: 22 --\n",
      "    Loss_i: 0.15571935 Loss_f: 0.16010599 rho -0.0008632444 lambda: 0.01\n",
      "    Loss_i: 0.15571935 Loss_f: 0.15988646 rho -0.00010223341 lambda: 0.099999994\n",
      "    Loss_i: 0.15571935 Loss_f: 0.15725617 rho -3.865695e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15571935 Loss_f: 0.15566278 rho 1.42681005e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15566278 Loss_f: 0.1558604 rho -4.9837826e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15566278 Loss_f: 0.15586038 rho -4.9834064e-08 lambda: 10.0\n",
      "    Train error: 22.927105\n",
      "-- Epoch: 16  Batch: 23 --\n",
      "    Loss_i: 0.16093767 Loss_f: 0.16259629 rho -0.00036423453 lambda: 0.01\n",
      "    Loss_i: 0.16093767 Loss_f: 0.16279015 rho -5.0098584e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16093767 Loss_f: 0.16162878 rho -1.913351e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16093767 Loss_f: 0.16090582 rho 8.836881e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16090582 Loss_f: 0.16099234 rho -2.4007052e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16090582 Loss_f: 0.16099234 rho -2.4007049e-08 lambda: 10.0\n",
      "    Train error: 23.97531\n",
      "-- Epoch: 16  Batch: 24 --\n",
      "    Loss_i: 0.1605471 Loss_f: 0.16100135 rho -9.616047e-05 lambda: 0.01\n",
      "    Loss_i: 0.1605471 Loss_f: 0.16130233 rho -1.9625872e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1605471 Loss_f: 0.16085514 rho -8.1915994e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1605471 Loss_f: 0.16053578 rho 3.0145308e-09 lambda: 9.999999\n",
      "    Loss_i: 0.16053578 Loss_f: 0.1605785 rho -1.1386376e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16053578 Loss_f: 0.1605785 rho -1.1386375e-08 lambda: 10.0\n",
      "    Train error: 23.994951\n",
      "-- Epoch: 16  Batch: 25 --\n",
      "    Loss_i: 0.16152178 Loss_f: 0.16126959 rho 4.960606e-05 lambda: 0.01\n",
      "    Loss_i: 0.16126959 Loss_f: 0.16177446 rho -9.968008e-05 lambda: 0.01\n",
      "    Loss_i: 0.16126959 Loss_f: 0.16200313 rho -1.6799931e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16126959 Loss_f: 0.16170312 rho -1.0090479e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16126959 Loss_f: 0.16134986 rho -1.8713795e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16126959 Loss_f: 0.16134986 rho -1.8713791e-08 lambda: 10.0\n",
      "    Train error: 24.304108\n",
      "-- Epoch: 16  Batch: 26 --\n",
      "    Loss_i: 0.15751587 Loss_f: 0.17430688 rho -0.0034498323 lambda: 0.01\n",
      "    Loss_i: 0.15751587 Loss_f: 0.17261775 rho -0.00041072158 lambda: 0.099999994\n",
      "    Loss_i: 0.15751587 Loss_f: 0.16348137 rho -1.6766986e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15751587 Loss_f: 0.15736866 rho 4.1514173e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15736866 Loss_f: 0.15820318 rho -2.3517475e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15736866 Loss_f: 0.15820317 rho -2.3517056e-07 lambda: 10.0\n",
      "    Train error: 23.331398\n",
      "-- Epoch: 16  Batch: 27 --\n",
      "    Loss_i: 0.15626232 Loss_f: 0.16031699 rho -0.00078915147 lambda: 0.01\n",
      "    Loss_i: 0.15626232 Loss_f: 0.16025323 rho -0.00010412004 lambda: 0.099999994\n",
      "    Loss_i: 0.15626232 Loss_f: 0.1578857 rho -4.3845657e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15626232 Loss_f: 0.15622768 rho 9.390393e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15622768 Loss_f: 0.15646146 rho -6.335323e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15622768 Loss_f: 0.15646146 rho -6.335322e-08 lambda: 10.0\n",
      "    Train error: 23.098719\n",
      "-- Epoch: 16  Batch: 28 --\n",
      "    Loss_i: 0.15523726 Loss_f: 0.15623765 rho -0.00019920056 lambda: 0.01\n",
      "    Loss_i: 0.15523726 Loss_f: 0.1564699 rho -2.8817987e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15523726 Loss_f: 0.15572159 rho -1.1523887e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15523726 Loss_f: 0.15522231 rho 3.5624323e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15522231 Loss_f: 0.15528859 rho -1.579767e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15522231 Loss_f: 0.15528859 rho -1.579767e-08 lambda: 10.0\n",
      "    Train error: 22.790699\n",
      "-- Epoch: 16  Batch: 29 --\n",
      "    Loss_i: 0.15729477 Loss_f: 0.15700403 rho 5.5707693e-05 lambda: 0.01\n",
      "    Loss_i: 0.15700403 Loss_f: 0.15747099 rho -8.951643e-05 lambda: 0.01\n",
      "    Loss_i: 0.15700403 Loss_f: 0.15773055 rho -1.5728141e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15700403 Loss_f: 0.15745209 rho -9.826975e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15700403 Loss_f: 0.15708916 rho -1.8695381e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15700403 Loss_f: 0.15708916 rho -1.869538e-08 lambda: 10.0\n",
      "    Train error: 23.339262\n",
      "-- Epoch: 16  Batch: 30 --\n",
      "    Loss_i: 0.15828186 Loss_f: 0.17045777 rho -0.0025547463 lambda: 0.01\n",
      "    Loss_i: 0.15828186 Loss_f: 0.16942419 rho -0.00031791948 lambda: 0.099999994\n",
      "    Loss_i: 0.15828186 Loss_f: 0.16268009 rho -1.3017736e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15828186 Loss_f: 0.15815705 rho 3.707994e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15815705 Loss_f: 0.1587604 rho -1.7913891e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15815705 Loss_f: 0.1587604 rho -1.7913887e-07 lambda: 10.0\n",
      "    Train error: 23.183012\n",
      "\n",
      "*** Epoch: 16 Train error: 23.54572499593099  Test error: 23.002968  Time: 720.1524393 sec\n",
      "\n",
      "-- Epoch: 17  Batch: 1 --\n",
      "    Loss_i: 0.15474892 Loss_f: 0.15827158 rho -0.0006705792 lambda: 0.01\n",
      "    Loss_i: 0.15474892 Loss_f: 0.15823217 rho -8.412439e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15474892 Loss_f: 0.15611453 rho -3.389182e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15474892 Loss_f: 0.15470211 rho 1.1648096e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15470211 Loss_f: 0.1548904 rho -4.6867886e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15470211 Loss_f: 0.1548904 rho -4.6867882e-08 lambda: 10.0\n",
      "    Train error: 22.62046\n",
      "-- Epoch: 17  Batch: 2 --\n",
      "    Loss_i: 0.15665299 Loss_f: 0.15654767 rho 1.6975386e-05 lambda: 0.01\n",
      "    Loss_i: 0.15654767 Loss_f: 0.15766868 rho -0.00017639267 lambda: 0.01\n",
      "    Loss_i: 0.15654767 Loss_f: 0.15783419 rho -2.5234058e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15654767 Loss_f: 0.15731017 rho -1.5333968e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15654767 Loss_f: 0.15669076 rho -2.884939e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15654767 Loss_f: 0.15669076 rho -2.884939e-08 lambda: 10.0\n",
      "    Train error: 22.939537\n",
      "-- Epoch: 17  Batch: 3 --\n",
      "    Loss_i: 0.15901674 Loss_f: 0.18425825 rho -0.0056825774 lambda: 0.01\n",
      "    Loss_i: 0.15901674 Loss_f: 0.18125747 rho -0.00059137173 lambda: 0.099999994\n",
      "    Loss_i: 0.15901674 Loss_f: 0.16782989 rho -2.3865963e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15901674 Loss_f: 0.15882686 rho 5.151594e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15882686 Loss_f: 0.16006514 rho -3.3591917e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15882686 Loss_f: 0.16006514 rho -3.359191e-07 lambda: 10.0\n",
      "    Train error: 23.560339\n",
      "-- Epoch: 17  Batch: 4 --\n",
      "    Loss_i: 0.15422839 Loss_f: 0.16341665 rho -0.0021685245 lambda: 0.01\n",
      "    Loss_i: 0.15422839 Loss_f: 0.16258438 rho -0.00022778925 lambda: 0.099999994\n",
      "    Loss_i: 0.15422839 Loss_f: 0.15738177 rho -8.731705e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15422839 Loss_f: 0.15412661 rho 2.8225887e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15412661 Loss_f: 0.15454315 rho -1.1545276e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15412661 Loss_f: 0.15454315 rho -1.1545274e-07 lambda: 10.0\n",
      "    Train error: 22.628498\n",
      "-- Epoch: 17  Batch: 5 --\n",
      "    Loss_i: 0.15767875 Loss_f: 0.1614602 rho -0.00070944056 lambda: 0.01\n",
      "    Loss_i: 0.15767875 Loss_f: 0.16130352 rho -9.2228685e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15767875 Loss_f: 0.1590576 rho -3.637937e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15767875 Loss_f: 0.15764232 rho 9.6481605e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15764232 Loss_f: 0.15782718 rho -4.8962836e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15764232 Loss_f: 0.15782718 rho -4.8962836e-08 lambda: 10.0\n",
      "    Train error: 23.493765\n",
      "-- Epoch: 17  Batch: 6 --\n",
      "    Loss_i: 0.15206197 Loss_f: 0.15259007 rho -9.6328775e-05 lambda: 0.01\n",
      "    Loss_i: 0.15206197 Loss_f: 0.1530138 rho -2.0554524e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15206197 Loss_f: 0.15246092 rho -8.776609e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15206197 Loss_f: 0.15204602 rho 3.5142114e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15204602 Loss_f: 0.1521008 rho -1.2071719e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15204602 Loss_f: 0.1521008 rho -1.2071718e-08 lambda: 10.0\n",
      "    Train error: 22.152594\n",
      "-- Epoch: 17  Batch: 7 --\n",
      "    Loss_i: 0.16055782 Loss_f: 0.16017526 rho 7.0608774e-05 lambda: 0.01\n",
      "    Loss_i: 0.16017526 Loss_f: 0.16120352 rho -0.0001901821 lambda: 0.01\n",
      "    Loss_i: 0.16017526 Loss_f: 0.16149627 rho -2.9518706e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16017526 Loss_f: 0.16094957 rho -1.7670288e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16017526 Loss_f: 0.16031614 rho -3.2217308e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16017526 Loss_f: 0.16031614 rho -3.2217308e-08 lambda: 10.0\n",
      "    Train error: 23.870953\n",
      "-- Epoch: 17  Batch: 8 --\n",
      "    Loss_i: 0.16084781 Loss_f: 0.18810953 rho -0.007272321 lambda: 0.01\n",
      "    Loss_i: 0.16084781 Loss_f: 0.18511865 rho -0.0007928805 lambda: 0.099999994\n",
      "    Loss_i: 0.16084781 Loss_f: 0.17068614 rho -3.287842e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.16084781 Loss_f: 0.16060409 rho 8.163683e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16060409 Loss_f: 0.16199 rho -4.6387711e-07 lambda: 9.999999\n",
      "    Loss_i: 0.16060409 Loss_f: 0.16199 rho -4.6387694e-07 lambda: 10.0\n",
      "    Train error: 24.136564\n",
      "-- Epoch: 17  Batch: 9 --\n",
      "    Loss_i: 0.15738624 Loss_f: 0.16515215 rho -0.0017968733 lambda: 0.01\n",
      "    Loss_i: 0.15738624 Loss_f: 0.16452184 rho -0.00019461464 lambda: 0.099999994\n",
      "    Loss_i: 0.15738624 Loss_f: 0.16016167 rho -7.707406e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15738624 Loss_f: 0.15731487 rho 1.9857511e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15731487 Loss_f: 0.15769461 rho -1.05637966e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15731487 Loss_f: 0.15769461 rho -1.0563796e-07 lambda: 10.0\n",
      "    Train error: 23.431147\n",
      "-- Epoch: 17  Batch: 10 --\n",
      "    Loss_i: 0.15710588 Loss_f: 0.15909594 rho -0.00041259755 lambda: 0.01\n",
      "    Loss_i: 0.15710588 Loss_f: 0.15925951 rho -5.208279e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15710588 Loss_f: 0.1579658 rho -2.1147907e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15710588 Loss_f: 0.15707596 rho 7.3710766e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15707596 Loss_f: 0.15719505 rho -2.9337059e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15707596 Loss_f: 0.15719506 rho -2.9340724e-08 lambda: 10.0\n",
      "    Train error: 23.273376\n",
      "-- Epoch: 17  Batch: 11 --\n",
      "    Loss_i: 0.15676838 Loss_f: 0.15767302 rho -0.00017562792 lambda: 0.01\n",
      "    Loss_i: 0.15676838 Loss_f: 0.15803517 rho -2.8592038e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15676838 Loss_f: 0.15731688 rho -1.2584393e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15676838 Loss_f: 0.15675312 rho 3.5066827e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15675312 Loss_f: 0.15683441 rho -1.8681954e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15675312 Loss_f: 0.15683441 rho -1.8681954e-08 lambda: 10.0\n",
      "    Train error: 23.237307\n",
      "-- Epoch: 17  Batch: 12 --\n",
      "    Loss_i: 0.16144556 Loss_f: 0.16136418 rho 1.5955204e-05 lambda: 0.01\n",
      "    Loss_i: 0.16136418 Loss_f: 0.16266367 rho -0.0002489312 lambda: 0.01\n",
      "    Loss_i: 0.16136418 Loss_f: 0.16283977 rho -3.386679e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.16136418 Loss_f: 0.16222839 rho -2.0235675e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.16136418 Loss_f: 0.16152526 rho -3.779411e-08 lambda: 9.999999\n",
      "    Loss_i: 0.16136418 Loss_f: 0.16152526 rho -3.7794106e-08 lambda: 10.0\n",
      "    Train error: 24.125235\n",
      "-- Epoch: 17  Batch: 13 --\n",
      "    Loss_i: 0.15290123 Loss_f: 0.1780509 rho -0.005576594 lambda: 0.01\n",
      "    Loss_i: 0.15290123 Loss_f: 0.17482351 rho -0.00058358384 lambda: 0.099999994\n",
      "    Loss_i: 0.15290123 Loss_f: 0.16102833 rho -2.2077553e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15290123 Loss_f: 0.15258484 rho 8.6126526e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15258484 Loss_f: 0.15359434 rho -2.747385e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15258484 Loss_f: 0.15359434 rho -2.747385e-07 lambda: 10.0\n",
      "    Train error: 22.509514\n",
      "-- Epoch: 17  Batch: 14 --\n",
      "    Loss_i: 0.1572083 Loss_f: 0.16665788 rho -0.0025755346 lambda: 0.01\n",
      "    Loss_i: 0.1572083 Loss_f: 0.16579764 rho -0.00028013616 lambda: 0.099999994\n",
      "    Loss_i: 0.1572083 Loss_f: 0.16034731 rho -1.044305e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1572083 Loss_f: 0.15710938 rho 3.297332e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15710938 Loss_f: 0.15750483 rho -1.3179341e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15710938 Loss_f: 0.15750483 rho -1.317934e-07 lambda: 10.0\n",
      "    Train error: 23.256227\n",
      "-- Epoch: 17  Batch: 15 --\n",
      "    Loss_i: 0.15515804 Loss_f: 0.1567906 rho -0.00030635338 lambda: 0.01\n",
      "    Loss_i: 0.15515804 Loss_f: 0.15697227 rho -4.0491384e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15515804 Loss_f: 0.15586951 rho -1.6185643e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15515804 Loss_f: 0.15512997 rho 6.399012e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15512997 Loss_f: 0.15522611 rho -2.191417e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15512997 Loss_f: 0.15522611 rho -2.1914168e-08 lambda: 10.0\n",
      "    Train error: 22.696152\n",
      "-- Epoch: 17  Batch: 16 --\n",
      "    Loss_i: 0.15714103 Loss_f: 0.15691467 rho 4.666874e-05 lambda: 0.01\n",
      "    Loss_i: 0.15691467 Loss_f: 0.15809594 rho -0.00023689358 lambda: 0.01\n",
      "    Loss_i: 0.15691467 Loss_f: 0.15825537 rho -3.387062e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15691467 Loss_f: 0.1576898 rho -2.010458e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15691467 Loss_f: 0.15705803 rho -3.728391e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15691467 Loss_f: 0.15705803 rho -3.7283904e-08 lambda: 10.0\n",
      "    Train error: 23.234207\n",
      "-- Epoch: 17  Batch: 17 --\n",
      "    Loss_i: 0.1552659 Loss_f: 0.18352033 rho -0.004846187 lambda: 0.01\n",
      "    Loss_i: 0.1552659 Loss_f: 0.18042931 rho -0.00061132316 lambda: 0.099999994\n",
      "    Loss_i: 0.1552659 Loss_f: 0.16546497 rho -2.5854342e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1552659 Loss_f: 0.15502213 rho 6.206419e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15502213 Loss_f: 0.15647072 rho -3.688324e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15502213 Loss_f: 0.15647072 rho -3.688324e-07 lambda: 10.0\n",
      "    Train error: 22.776867\n",
      "-- Epoch: 17  Batch: 18 --\n",
      "    Loss_i: 0.15727164 Loss_f: 0.16991974 rho -0.0025078212 lambda: 0.01\n",
      "    Loss_i: 0.15727164 Loss_f: 0.16850244 rho -0.00029982175 lambda: 0.099999994\n",
      "    Loss_i: 0.15727164 Loss_f: 0.16154638 rho -1.1821542e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15727164 Loss_f: 0.157177 rho 2.6265631e-08 lambda: 9.999999\n",
      "    Loss_i: 0.157177 Loss_f: 0.15775809 rho -1.6116783e-07 lambda: 9.999999\n",
      "    Loss_i: 0.157177 Loss_f: 0.15775809 rho -1.6116779e-07 lambda: 10.0\n",
      "    Train error: 23.102694\n",
      "-- Epoch: 17  Batch: 19 --\n",
      "    Loss_i: 0.1593705 Loss_f: 0.16165091 rho -0.0004958225 lambda: 0.01\n",
      "    Loss_i: 0.1593705 Loss_f: 0.16187833 rho -6.814813e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1593705 Loss_f: 0.16037697 rho -2.8050597e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1593705 Loss_f: 0.15933345 rho 1.0350874e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15933345 Loss_f: 0.15947379 rho -3.9207826e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15933345 Loss_f: 0.15947379 rho -3.9207823e-08 lambda: 10.0\n",
      "    Train error: 23.775585\n",
      "-- Epoch: 17  Batch: 20 --\n",
      "    Loss_i: 0.15528053 Loss_f: 0.15519145 rho 1.5514659e-05 lambda: 0.01\n",
      "    Loss_i: 0.15519145 Loss_f: 0.15652192 rho -0.00022551768 lambda: 0.01\n",
      "    Loss_i: 0.15519145 Loss_f: 0.15667325 rho -3.001493e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15519145 Loss_f: 0.15608428 rho -1.8444637e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15519145 Loss_f: 0.15536101 rho -3.509851e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15519145 Loss_f: 0.15536101 rho -3.5098505e-08 lambda: 10.0\n",
      "    Train error: 22.699808\n",
      "-- Epoch: 17  Batch: 21 --\n",
      "    Loss_i: 0.15756482 Loss_f: 0.1989751 rho -0.007924826 lambda: 0.01\n",
      "    Loss_i: 0.15756482 Loss_f: 0.19412468 rho -0.001026636 lambda: 0.099999994\n",
      "    Loss_i: 0.15756482 Loss_f: 0.17279923 rho -4.487697e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15756482 Loss_f: 0.15724549 rho 9.4531046e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15724549 Loss_f: 0.15947932 rho -6.6116627e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15724549 Loss_f: 0.15947932 rho -6.611662e-07 lambda: 10.0\n",
      "    Train error: 23.423449\n",
      "-- Epoch: 17  Batch: 22 --\n",
      "    Loss_i: 0.15434599 Loss_f: 0.17503443 rho -0.003369116 lambda: 0.01\n",
      "    Loss_i: 0.15434599 Loss_f: 0.17267066 rho -0.00045436475 lambda: 0.099999994\n",
      "    Loss_i: 0.15434599 Loss_f: 0.16142236 rho -1.8513507e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15434599 Loss_f: 0.1542023 rho 3.780174e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1542023 Loss_f: 0.15516625 rho -2.5354365e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1542023 Loss_f: 0.15516625 rho -2.535436e-07 lambda: 10.0\n",
      "    Train error: 22.552162\n",
      "-- Epoch: 17  Batch: 23 --\n",
      "    Loss_i: 0.15947457 Loss_f: 0.16783181 rho -0.0018855399 lambda: 0.01\n",
      "    Loss_i: 0.15947457 Loss_f: 0.16713563 rho -0.0002208931 lambda: 0.099999994\n",
      "    Loss_i: 0.15947457 Loss_f: 0.16243486 rho -8.779525e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15947457 Loss_f: 0.1594042 rho 2.09279e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1594042 Loss_f: 0.15980878 rho -1.2032439e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1594042 Loss_f: 0.15980877 rho -1.2031994e-07 lambda: 10.0\n",
      "    Train error: 23.624004\n",
      "-- Epoch: 17  Batch: 24 --\n",
      "    Loss_i: 0.15904379 Loss_f: 0.16153751 rho -0.00053206255 lambda: 0.01\n",
      "    Loss_i: 0.15904379 Loss_f: 0.16154696 rho -6.9221016e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15904379 Loss_f: 0.15998806 rho -2.690899e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15904379 Loss_f: 0.15900843 rho 1.0107539e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15900843 Loss_f: 0.15913215 rho -3.535784e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15900843 Loss_f: 0.15913215 rho -3.5357832e-08 lambda: 10.0\n",
      "    Train error: 23.608992\n",
      "-- Epoch: 17  Batch: 25 --\n",
      "    Loss_i: 0.15990698 Loss_f: 0.159859 rho 7.792461e-06 lambda: 0.01\n",
      "    Loss_i: 0.159859 Loss_f: 0.16116141 rho -0.00021252083 lambda: 0.01\n",
      "    Loss_i: 0.159859 Loss_f: 0.16129392 rho -3.0199915e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.159859 Loss_f: 0.16068701 rho -1.7946767e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.159859 Loss_f: 0.1600119 rho -3.32397e-08 lambda: 9.999999\n",
      "    Loss_i: 0.159859 Loss_f: 0.1600119 rho -3.3239697e-08 lambda: 10.0\n",
      "    Train error: 23.966562\n",
      "-- Epoch: 17  Batch: 26 --\n",
      "    Loss_i: 0.15620768 Loss_f: 0.18839729 rho -0.007055561 lambda: 0.01\n",
      "    Loss_i: 0.15620768 Loss_f: 0.18472339 rho -0.000736017 lambda: 0.099999994\n",
      "    Loss_i: 0.15620768 Loss_f: 0.16773984 rho -3.03037e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15620768 Loss_f: 0.15595154 rho 6.74282e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15595154 Loss_f: 0.15759079 rho -4.3125573e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15595154 Loss_f: 0.15759079 rho -4.3125567e-07 lambda: 10.0\n",
      "    Train error: 22.937954\n",
      "-- Epoch: 17  Batch: 27 --\n",
      "    Loss_i: 0.1548751 Loss_f: 0.16458437 rho -0.0020771895 lambda: 0.01\n",
      "    Loss_i: 0.1548751 Loss_f: 0.1637959 rho -0.00022886679 lambda: 0.099999994\n",
      "    Loss_i: 0.1548751 Loss_f: 0.1584223 rho -9.28546e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1548751 Loss_f: 0.15478918 rho 2.2536952e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15478918 Loss_f: 0.15528601 rho -1.3030086e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15478918 Loss_f: 0.15528603 rho -1.3030476e-07 lambda: 10.0\n",
      "    Train error: 22.740088\n",
      "-- Epoch: 17  Batch: 28 --\n",
      "    Loss_i: 0.15376477 Loss_f: 0.15664655 rho -0.00054366374 lambda: 0.01\n",
      "    Loss_i: 0.15376477 Loss_f: 0.15663984 rho -6.234393e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15376477 Loss_f: 0.15487774 rho -2.449997e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15376477 Loss_f: 0.15372734 rho 8.252435e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15372734 Loss_f: 0.15387684 rho -3.2958663e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15372734 Loss_f: 0.15387686 rho -3.2961943e-08 lambda: 10.0\n",
      "    Train error: 22.398235\n",
      "-- Epoch: 17  Batch: 29 --\n",
      "    Loss_i: 0.15574028 Loss_f: 0.1559648 rho -3.8767594e-05 lambda: 0.01\n",
      "    Loss_i: 0.15574028 Loss_f: 0.15628484 rho -1.1730048e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15574028 Loss_f: 0.15596141 rho -4.8841844e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15574028 Loss_f: 0.15572736 rho 2.8607572e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15572736 Loss_f: 0.15575674 rho -6.50674e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15572736 Loss_f: 0.15575673 rho -6.50344e-09 lambda: 10.0\n",
      "    Train error: 22.908873\n",
      "-- Epoch: 17  Batch: 30 --\n",
      "    Loss_i: 0.1568107 Loss_f: 0.155995 rho 0.0001810134 lambda: 0.01\n",
      "    Loss_i: 0.155995 Loss_f: 0.15595476 rho 8.835033e-06 lambda: 0.01\n",
      "    Loss_i: 0.15595476 Loss_f: 0.15669948 rho -0.00016175973 lambda: 0.01\n",
      "    Loss_i: 0.15595476 Loss_f: 0.15679315 rho -2.1815038e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15595476 Loss_f: 0.15640722 rho -1.2010869e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15595476 Loss_f: 0.15604153 rho -2.308023e-08 lambda: 9.999999\n",
      "    Train error: 22.685581\n",
      "\n",
      "*** Epoch: 17 Train error: 23.145557594299316  Test error: 23.009401  Time: 710.1934021999987 sec\n",
      "\n",
      "-- Epoch: 18  Batch: 1 --\n",
      "    Loss_i: 0.15381551 Loss_f: 0.15945779 rho -0.001163487 lambda: 0.01\n",
      "    Loss_i: 0.15381551 Loss_f: 0.15858898 rho -0.00013094224 lambda: 0.099999994\n",
      "    Loss_i: 0.15381551 Loss_f: 0.15475337 rho -2.6605487e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15381551 Loss_f: 0.15339945 rho 1.1843178e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15339945 Loss_f: 0.15336344 rho 1.0250927e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15336344 Loss_f: 0.15370934 rho -9.843837e-08 lambda: 9.999999\n",
      "    Train error: 22.361618\n",
      "-- Epoch: 18  Batch: 2 --\n",
      "    Loss_i: 0.15536581 Loss_f: 0.16401596 rho -0.0016374887 lambda: 0.01\n",
      "    Loss_i: 0.15536581 Loss_f: 0.16329384 rho -0.0001968008 lambda: 0.099999994\n",
      "    Loss_i: 0.15536581 Loss_f: 0.15838987 rho -7.747947e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15536581 Loss_f: 0.15525636 rho 2.8132375e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15525636 Loss_f: 0.15566346 rho -1.0463982e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15525636 Loss_f: 0.15566346 rho -1.0463982e-07 lambda: 10.0\n",
      "    Train error: 22.73203\n",
      "-- Epoch: 18  Batch: 3 --\n",
      "    Loss_i: 0.15758675 Loss_f: 0.1581992 rho -0.00012624031 lambda: 0.01\n",
      "    Loss_i: 0.15758675 Loss_f: 0.15865368 rho -2.5944306e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15758675 Loss_f: 0.15793894 rho -8.7208974e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15758675 Loss_f: 0.15753202 rho 1.3577571e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15753202 Loss_f: 0.15756212 rho -7.467859e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15753202 Loss_f: 0.15756214 rho -7.471556e-09 lambda: 10.0\n",
      "    Train error: 23.19637\n",
      "-- Epoch: 18  Batch: 4 --\n",
      "    Loss_i: 0.15303762 Loss_f: 0.15300025 rho 7.3906326e-06 lambda: 0.01\n",
      "    Loss_i: 0.15300025 Loss_f: 0.15638676 rho -0.0006428464 lambda: 0.01\n",
      "    Loss_i: 0.15300025 Loss_f: 0.15623844 rho -7.500509e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15300025 Loss_f: 0.15485352 rho -4.3893324e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15300025 Loss_f: 0.15334058 rho -8.078591e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15300025 Loss_f: 0.15334058 rho -8.07859e-08 lambda: 10.0\n",
      "    Train error: 22.18391\n",
      "-- Epoch: 18  Batch: 5 --\n",
      "    Loss_i: 0.15699254 Loss_f: 0.23429912 rho -0.018170426 lambda: 0.01\n",
      "    Loss_i: 0.15699254 Loss_f: 0.22495544 rho -0.0022007516 lambda: 0.099999994\n",
      "    Loss_i: 0.15699254 Loss_f: 0.18725969 rho -0.00010185706 lambda: 0.99999994\n",
      "    Loss_i: 0.15699254 Loss_f: 0.15638234 rho 2.061587e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15638234 Loss_f: 0.16102938 rho -1.5704188e-06 lambda: 9.999999\n",
      "    Loss_i: 0.15638234 Loss_f: 0.16102938 rho -1.5704187e-06 lambda: 10.0\n",
      "    Train error: 23.258726\n",
      "-- Epoch: 18  Batch: 6 --\n",
      "    Loss_i: 0.1505177 Loss_f: 0.18900713 rho -0.0073398976 lambda: 0.01\n",
      "    Loss_i: 0.1505177 Loss_f: 0.18446176 rho -0.0007672162 lambda: 0.099999994\n",
      "    Loss_i: 0.1505177 Loss_f: 0.16439244 rho -3.1952106e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1505177 Loss_f: 0.15026756 rho 5.7714896e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15026756 Loss_f: 0.15227269 rho -4.6212767e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15026756 Loss_f: 0.15227269 rho -4.6212753e-07 lambda: 10.0\n",
      "    Train error: 21.64619\n",
      "-- Epoch: 18  Batch: 7 --\n",
      "    Loss_i: 0.1586793 Loss_f: 0.17499956 rho -0.0025073164 lambda: 0.01\n",
      "    Loss_i: 0.1586793 Loss_f: 0.17322971 rho -0.0003645134 lambda: 0.099999994\n",
      "    Loss_i: 0.1586793 Loss_f: 0.16426998 rho -1.4948335e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1586793 Loss_f: 0.15855144 rho 3.4420676e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15855144 Loss_f: 0.15930714 rho -2.033754e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15855144 Loss_f: 0.15930714 rho -2.0337538e-07 lambda: 10.0\n",
      "    Train error: 23.407766\n",
      "-- Epoch: 18  Batch: 8 --\n",
      "    Loss_i: 0.15902536 Loss_f: 0.1626003 rho -0.00074405735 lambda: 0.01\n",
      "    Loss_i: 0.15902536 Loss_f: 0.16262427 rho -0.000100071775 lambda: 0.099999994\n",
      "    Loss_i: 0.15902536 Loss_f: 0.16042574 rho -4.0292953e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15902536 Loss_f: 0.15898402 rho 1.1934983e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15898402 Loss_f: 0.15917476 rho -5.5059154e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15898402 Loss_f: 0.15917476 rho -5.5059143e-08 lambda: 10.0\n",
      "    Train error: 23.769732\n",
      "-- Epoch: 18  Batch: 9 --\n",
      "    Loss_i: 0.15572618 Loss_f: 0.15578654 rho -1.0528341e-05 lambda: 0.01\n",
      "    Loss_i: 0.15572618 Loss_f: 0.15611988 rho -8.746061e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15572618 Loss_f: 0.15588537 rho -3.6358716e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15572618 Loss_f: 0.15571782 rho 1.9147055e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15571782 Loss_f: 0.15573812 rho -4.648834e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15571782 Loss_f: 0.15573812 rho -4.6488333e-09 lambda: 10.0\n",
      "    Train error: 22.99877\n",
      "-- Epoch: 18  Batch: 10 --\n",
      "    Loss_i: 0.15545358 Loss_f: 0.1549337 rho 9.915817e-05 lambda: 0.01\n",
      "    Loss_i: 0.1549337 Loss_f: 0.15509939 rho -3.111218e-05 lambda: 0.01\n",
      "    Loss_i: 0.1549337 Loss_f: 0.1553861 rho -1.0007967e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1549337 Loss_f: 0.1551988 rho -5.970686e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1549337 Loss_f: 0.15498058 rho -1.0577797e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1549337 Loss_f: 0.15498058 rho -1.0577797e-08 lambda: 10.0\n",
      "    Train error: 22.742683\n",
      "-- Epoch: 18  Batch: 11 --\n",
      "    Loss_i: 0.15487118 Loss_f: 0.16126183 rho -0.0012499702 lambda: 0.01\n",
      "    Loss_i: 0.15487118 Loss_f: 0.16077948 rho -0.00014005022 lambda: 0.099999994\n",
      "    Loss_i: 0.15487118 Loss_f: 0.15703665 rho -5.2441437e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15487118 Loss_f: 0.15476513 rho 2.5738398e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15476513 Loss_f: 0.15503788 rho -6.619144e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15476513 Loss_f: 0.15503788 rho -6.619143e-08 lambda: 10.0\n",
      "    Train error: 22.765614\n",
      "-- Epoch: 18  Batch: 12 --\n",
      "    Loss_i: 0.15954576 Loss_f: 0.16062063 rho -0.00022596291 lambda: 0.01\n",
      "    Loss_i: 0.15954576 Loss_f: 0.16085665 rho -3.2141084e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15954576 Loss_f: 0.16001043 rho -1.1585571e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15954576 Loss_f: 0.1595097 rho 9.006375e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1595097 Loss_f: 0.1595608 rho -1.27616735e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1595097 Loss_f: 0.1595608 rho -1.2761673e-08 lambda: 10.0\n",
      "    Train error: 23.694601\n",
      "-- Epoch: 18  Batch: 13 --\n",
      "    Loss_i: 0.15081759 Loss_f: 0.1504897 rho 5.31923e-05 lambda: 0.01\n",
      "    Loss_i: 0.1504897 Loss_f: 0.15097256 rho -7.806829e-05 lambda: 0.01\n",
      "    Loss_i: 0.1504897 Loss_f: 0.15120123 rho -1.4252594e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1504897 Loss_f: 0.15088683 rho -8.149602e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1504897 Loss_f: 0.15056124 rho -1.4716997e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1504897 Loss_f: 0.15056124 rho -1.4716995e-08 lambda: 10.0\n",
      "    Train error: 21.982468\n",
      "-- Epoch: 18  Batch: 14 --\n",
      "    Loss_i: 0.15535949 Loss_f: 0.17131504 rho -0.0037633874 lambda: 0.01\n",
      "    Loss_i: 0.15535949 Loss_f: 0.16973196 rho -0.0004175408 lambda: 0.099999994\n",
      "    Loss_i: 0.15535949 Loss_f: 0.16108707 rho -1.703409e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15535949 Loss_f: 0.15521549 rho 4.2929575e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15521549 Loss_f: 0.15602297 rho -2.4056104e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15521549 Loss_f: 0.15602297 rho -2.40561e-07 lambda: 10.0\n",
      "    Train error: 22.758488\n",
      "-- Epoch: 18  Batch: 15 --\n",
      "    Loss_i: 0.15316397 Loss_f: 0.1585994 rho -0.00088102743 lambda: 0.01\n",
      "    Loss_i: 0.15316397 Loss_f: 0.15821993 rho -0.000108105836 lambda: 0.099999994\n",
      "    Loss_i: 0.15316397 Loss_f: 0.15509649 rho -4.2682955e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15316397 Loss_f: 0.15311526 rho 1.0794448e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15311526 Loss_f: 0.15337352 rho -5.7225005e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15311526 Loss_f: 0.15337352 rho -5.7224998e-08 lambda: 10.0\n",
      "    Train error: 22.19964\n",
      "-- Epoch: 18  Batch: 16 --\n",
      "    Loss_i: 0.1553121 Loss_f: 0.15593487 rho -0.0001023522 lambda: 0.01\n",
      "    Loss_i: 0.1553121 Loss_f: 0.1562557 rho -2.332988e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1553121 Loss_f: 0.15566954 rho -9.3067024e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1553121 Loss_f: 0.15529026 rho 5.7182965e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15529026 Loss_f: 0.15533352 rho -1.1322856e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15529026 Loss_f: 0.15533352 rho -1.1322855e-08 lambda: 10.0\n",
      "    Train error: 22.884995\n",
      "-- Epoch: 18  Batch: 17 --\n",
      "    Loss_i: 0.15324037 Loss_f: 0.1530568 rho 3.1367683e-05 lambda: 0.01\n",
      "    Loss_i: 0.1530568 Loss_f: 0.15385312 rho -0.00013668808 lambda: 0.01\n",
      "    Loss_i: 0.1530568 Loss_f: 0.15407903 rho -2.0357831e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1530568 Loss_f: 0.1536744 rho -1.2499653e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1530568 Loss_f: 0.15317316 rho -2.3589484e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1530568 Loss_f: 0.15317316 rho -2.358948e-08 lambda: 10.0\n",
      "    Train error: 22.324802\n",
      "-- Epoch: 18  Batch: 18 --\n",
      "    Loss_i: 0.15563676 Loss_f: 0.18393463 rho -0.005909927 lambda: 0.01\n",
      "    Loss_i: 0.15563676 Loss_f: 0.1802131 rho -0.0006116534 lambda: 0.099999994\n",
      "    Loss_i: 0.15563676 Loss_f: 0.16513896 rho -2.4111165e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15563676 Loss_f: 0.15544805 rho 4.7977146e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15544805 Loss_f: 0.15674485 rho -3.2940397e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15544805 Loss_f: 0.15674485 rho -3.2940392e-07 lambda: 10.0\n",
      "    Train error: 22.664099\n",
      "-- Epoch: 18  Batch: 19 --\n",
      "    Loss_i: 0.15728211 Loss_f: 0.16403422 rho -0.0011237221 lambda: 0.01\n",
      "    Loss_i: 0.15728211 Loss_f: 0.16353607 rho -0.00015188435 lambda: 0.099999994\n",
      "    Loss_i: 0.15728211 Loss_f: 0.15975855 rho -6.303825e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15728211 Loss_f: 0.15722074 rho 1.5699436e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15722074 Loss_f: 0.15756816 rho -8.886413e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15722074 Loss_f: 0.15756816 rho -8.886411e-08 lambda: 10.0\n",
      "    Train error: 23.249298\n",
      "-- Epoch: 18  Batch: 20 --\n",
      "    Loss_i: 0.15330541 Loss_f: 0.15450536 rho -0.0001700946 lambda: 0.01\n",
      "    Loss_i: 0.15330541 Loss_f: 0.1546665 rho -2.7106951e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15330541 Loss_f: 0.15381983 rho -1.0677372e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15330541 Loss_f: 0.15328595 rho 4.0564747e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15328595 Loss_f: 0.15335256 rho -1.3883773e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15328595 Loss_f: 0.15335256 rho -1.3883773e-08 lambda: 10.0\n",
      "    Train error: 22.304352\n",
      "-- Epoch: 18  Batch: 21 --\n",
      "    Loss_i: 0.15546978 Loss_f: 0.15592891 rho -8.8134744e-05 lambda: 0.01\n",
      "    Loss_i: 0.15546978 Loss_f: 0.15628253 rho -1.8793671e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15546978 Loss_f: 0.15580708 rho -7.962511e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15546978 Loss_f: 0.15545486 rho 3.5285208e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15545486 Loss_f: 0.15550183 rho -1.1109885e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15545486 Loss_f: 0.15550183 rho -1.1109885e-08 lambda: 10.0\n",
      "    Train error: 22.890417\n",
      "-- Epoch: 18  Batch: 22 --\n",
      "    Loss_i: 0.1524555 Loss_f: 0.15261428 rho -2.2645638e-05 lambda: 0.01\n",
      "    Loss_i: 0.1524555 Loss_f: 0.15291259 rho -8.946317e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1524555 Loss_f: 0.15264614 rho -3.8756676e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1524555 Loss_f: 0.1524489 rho 1.3471834e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1524489 Loss_f: 0.1524748 rho -5.2884475e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1524489 Loss_f: 0.1524748 rho -5.2884475e-09 lambda: 10.0\n",
      "    Train error: 22.058641\n",
      "-- Epoch: 18  Batch: 23 --\n",
      "    Loss_i: 0.15779491 Loss_f: 0.15765755 rho 2.6931195e-05 lambda: 0.01\n",
      "    Loss_i: 0.15765755 Loss_f: 0.15868424 rho -0.00020369307 lambda: 0.01\n",
      "    Loss_i: 0.15765755 Loss_f: 0.15887544 rho -2.8307848e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15765755 Loss_f: 0.15838656 rho -1.7240444e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15765755 Loss_f: 0.15779412 rho -3.2353896e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15765755 Loss_f: 0.15779412 rho -3.2353896e-08 lambda: 10.0\n",
      "    Train error: 23.281202\n",
      "-- Epoch: 18  Batch: 24 --\n",
      "    Loss_i: 0.15759349 Loss_f: 0.19382279 rho -0.007940618 lambda: 0.01\n",
      "    Loss_i: 0.15759349 Loss_f: 0.18943352 rho -0.0008288996 lambda: 0.099999994\n",
      "    Loss_i: 0.15759349 Loss_f: 0.17046125 rho -3.4140066e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15759349 Loss_f: 0.1573416 rho 6.695803e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1573416 Loss_f: 0.15915872 rho -4.825152e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1573416 Loss_f: 0.1591587 rho -4.825112e-07 lambda: 10.0\n",
      "    Train error: 23.166683\n",
      "-- Epoch: 18  Batch: 25 --\n",
      "    Loss_i: 0.1582033 Loss_f: 0.1712574 rho -0.0023411862 lambda: 0.01\n",
      "    Loss_i: 0.1582033 Loss_f: 0.1698851 rho -0.0002767152 lambda: 0.099999994\n",
      "    Loss_i: 0.1582033 Loss_f: 0.16282922 rho -1.1320907e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1582033 Loss_f: 0.1581092 rho 2.3105681e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1581092 Loss_f: 0.15876167 rho -1.6018933e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1581092 Loss_f: 0.15876167 rho -1.6018932e-07 lambda: 10.0\n",
      "    Train error: 23.470264\n",
      "-- Epoch: 18  Batch: 26 --\n",
      "    Loss_i: 0.15418784 Loss_f: 0.15770328 rho -0.00067997054 lambda: 0.01\n",
      "    Loss_i: 0.15418784 Loss_f: 0.15756144 rho -7.993297e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15418784 Loss_f: 0.15546918 rho -3.105832e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15418784 Loss_f: 0.1541531 rho 8.442366e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1541531 Loss_f: 0.15432437 rho -4.1606825e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1541531 Loss_f: 0.15432437 rho -4.160682e-08 lambda: 10.0\n",
      "    Train error: 22.549778\n",
      "-- Epoch: 18  Batch: 27 --\n",
      "    Loss_i: 0.15302637 Loss_f: 0.1529972 rho 4.779357e-06 lambda: 0.01\n",
      "    Loss_i: 0.1529972 Loss_f: 0.15423074 rho -0.0002035817 lambda: 0.01\n",
      "    Loss_i: 0.1529972 Loss_f: 0.15437078 rho -2.7499102e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1529972 Loss_f: 0.15380952 rho -1.6616611e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1529972 Loss_f: 0.15315081 rho -3.14918e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1529972 Loss_f: 0.15315081 rho -3.149179e-08 lambda: 10.0\n",
      "    Train error: 22.415981\n",
      "-- Epoch: 18  Batch: 28 --\n",
      "    Loss_i: 0.152314 Loss_f: 0.18636799 rho -0.0065622637 lambda: 0.01\n",
      "    Loss_i: 0.152314 Loss_f: 0.1823487 rho -0.0006681775 lambda: 0.099999994\n",
      "    Loss_i: 0.152314 Loss_f: 0.16448641 rho -2.7504635e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.152314 Loss_f: 0.15206124 rho 5.7205057e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15206124 Loss_f: 0.1538008 rho -3.9333835e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15206124 Loss_f: 0.15380082 rho -3.9334174e-07 lambda: 10.0\n",
      "    Train error: 21.986006\n",
      "-- Epoch: 18  Batch: 29 --\n",
      "    Loss_i: 0.15407296 Loss_f: 0.16632463 rho -0.0023220188 lambda: 0.01\n",
      "    Loss_i: 0.15407296 Loss_f: 0.16508874 rho -0.00026327686 lambda: 0.099999994\n",
      "    Loss_i: 0.15407296 Loss_f: 0.15831769 rho -1.0416799e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15407296 Loss_f: 0.1539661 rho 2.6293577e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1539661 Loss_f: 0.15453772 rho -1.4064386e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1539661 Loss_f: 0.15453772 rho -1.4064386e-07 lambda: 10.0\n",
      "    Train error: 22.529652\n",
      "-- Epoch: 18  Batch: 30 --\n",
      "    Loss_i: 0.15454711 Loss_f: 0.15577532 rho -0.00024517433 lambda: 0.01\n",
      "    Loss_i: 0.15454711 Loss_f: 0.15606222 rho -3.604575e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15454711 Loss_f: 0.15515147 rho -1.4659502e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15454711 Loss_f: 0.15451975 rho 6.649141e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15451975 Loss_f: 0.15460078 rho -1.969383e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15451975 Loss_f: 0.15460078 rho -1.9693827e-08 lambda: 10.0\n",
      "    Train error: 22.268995\n",
      "\n",
      "*** Epoch: 18 Train error: 22.724792416890462  Test error: 22.229187  Time: 709.3017591000007 sec\n",
      "\n",
      "-- Epoch: 19  Batch: 1 --\n",
      "    Loss_i: 0.151468 Loss_f: 0.15114604 rho 5.3562435e-05 lambda: 0.01\n",
      "    Loss_i: 0.15114604 Loss_f: 0.1514011 rho -4.235587e-05 lambda: 0.01\n",
      "    Loss_i: 0.15114604 Loss_f: 0.15164085 rho -9.1796355e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15114604 Loss_f: 0.15144621 rho -5.634738e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15114604 Loss_f: 0.1512023 rho -1.05748725e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15114604 Loss_f: 0.1512023 rho -1.05748725e-08 lambda: 10.0\n",
      "    Train error: 21.765308\n",
      "-- Epoch: 19  Batch: 2 --\n",
      "    Loss_i: 0.1533761 Loss_f: 0.16166537 rho -0.0013069097 lambda: 0.01\n",
      "    Loss_i: 0.1533761 Loss_f: 0.16088177 rho -0.00014874968 lambda: 0.099999994\n",
      "    Loss_i: 0.1533761 Loss_f: 0.15616006 rho -5.662867e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1533761 Loss_f: 0.15329245 rho 1.7061362e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15329245 Loss_f: 0.15364976 rho -7.2869526e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15329245 Loss_f: 0.15364975 rho -7.286648e-08 lambda: 10.0\n",
      "    Train error: 22.197456\n",
      "-- Epoch: 19  Batch: 3 --\n",
      "    Loss_i: 0.155455 Loss_f: 0.1557071 rho -4.2458934e-05 lambda: 0.01\n",
      "    Loss_i: 0.155455 Loss_f: 0.15602012 rho -1.0956645e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.155455 Loss_f: 0.15567075 rho -4.247253e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.155455 Loss_f: 0.1554374 rho 3.469659e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1554374 Loss_f: 0.1554633 rho -5.1061346e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1554374 Loss_f: 0.1554633 rho -5.106134e-09 lambda: 10.0\n",
      "    Train error: 22.638214\n",
      "-- Epoch: 19  Batch: 4 --\n",
      "    Loss_i: 0.15088393 Loss_f: 0.15072998 rho 2.607063e-05 lambda: 0.01\n",
      "    Loss_i: 0.15072998 Loss_f: 0.15157889 rho -0.00014061948 lambda: 0.01\n",
      "    Loss_i: 0.15072998 Loss_f: 0.15175863 rho -2.0245005e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15072998 Loss_f: 0.15135705 rho -1.2578206e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15072998 Loss_f: 0.15084913 rho -2.3945764e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15072998 Loss_f: 0.15084913 rho -2.3945763e-08 lambda: 10.0\n",
      "    Train error: 21.709015\n",
      "-- Epoch: 19  Batch: 5 --\n",
      "    Loss_i: 0.15468056 Loss_f: 0.18578516 rho -0.0060292436 lambda: 0.01\n",
      "    Loss_i: 0.15468056 Loss_f: 0.1821002 rho -0.00069459283 lambda: 0.099999994\n",
      "    Loss_i: 0.15468056 Loss_f: 0.16608939 rho -2.9815701e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15468056 Loss_f: 0.15447403 rho 5.4145776e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15447403 Loss_f: 0.15615657 rho -4.4114643e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15447403 Loss_f: 0.15615657 rho -4.4114643e-07 lambda: 10.0\n",
      "    Train error: 22.746124\n",
      "-- Epoch: 19  Batch: 6 --\n",
      "    Loss_i: 0.14866155 Loss_f: 0.16183259 rho -0.0023339977 lambda: 0.01\n",
      "    Loss_i: 0.14866155 Loss_f: 0.1604936 rho -0.00024190734 lambda: 0.099999994\n",
      "    Loss_i: 0.14866155 Loss_f: 0.15322188 rho -9.469203e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14866155 Loss_f: 0.1485583 rho 2.1472742e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1485583 Loss_f: 0.14917387 rho -1.2797429e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1485583 Loss_f: 0.14917387 rho -1.2797429e-07 lambda: 10.0\n",
      "    Train error: 21.281935\n",
      "-- Epoch: 19  Batch: 7 --\n",
      "    Loss_i: 0.15704635 Loss_f: 0.16040531 rho -0.0005851925 lambda: 0.01\n",
      "    Loss_i: 0.15704635 Loss_f: 0.16041584 rho -7.703815e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15704635 Loss_f: 0.1583046 rho -2.9695486e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15704635 Loss_f: 0.15700793 rho 9.095522e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15700793 Loss_f: 0.15716892 rho -3.8119357e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15700793 Loss_f: 0.15716892 rho -3.811935e-08 lambda: 10.0\n",
      "    Train error: 23.009823\n",
      "-- Epoch: 19  Batch: 8 --\n",
      "    Loss_i: 0.15744607 Loss_f: 0.1573407 rho 1.912673e-05 lambda: 0.01\n",
      "    Loss_i: 0.1573407 Loss_f: 0.15900835 rho -0.0002925816 lambda: 0.01\n",
      "    Loss_i: 0.1573407 Loss_f: 0.15916233 rho -4.2560994e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1573407 Loss_f: 0.15840718 rho -2.5772422e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1573407 Loss_f: 0.15754066 rho -4.8488303e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1573407 Loss_f: 0.15754066 rho -4.8488296e-08 lambda: 10.0\n",
      "    Train error: 23.286074\n",
      "-- Epoch: 19  Batch: 9 --\n",
      "    Loss_i: 0.15452011 Loss_f: 0.20416883 rho -0.009688644 lambda: 0.01\n",
      "    Loss_i: 0.15452011 Loss_f: 0.19855548 rho -0.0012159004 lambda: 0.099999994\n",
      "    Loss_i: 0.15452011 Loss_f: 0.17348453 rho -5.4631288e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15452011 Loss_f: 0.15430054 rho 6.352672e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15430054 Loss_f: 0.1571703 rho -8.3050156e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15430054 Loss_f: 0.15717028 rho -8.304971e-07 lambda: 10.0\n",
      "    Train error: 22.768408\n",
      "-- Epoch: 19  Batch: 10 --\n",
      "    Loss_i: 0.15379389 Loss_f: 0.17879842 rho -0.005046159 lambda: 0.01\n",
      "    Loss_i: 0.15379389 Loss_f: 0.17583494 rho -0.0005467442 lambda: 0.099999994\n",
      "    Loss_i: 0.15379389 Loss_f: 0.1625906 rho -2.2332651e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15379389 Loss_f: 0.15363109 rho 4.1426798e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15363109 Loss_f: 0.15487719 rho -3.1674597e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15363109 Loss_f: 0.15487719 rho -3.1674594e-07 lambda: 10.0\n",
      "    Train error: 22.404446\n",
      "-- Epoch: 19  Batch: 11 --\n",
      "    Loss_i: 0.15348294 Loss_f: 0.16522886 rho -0.0023904752 lambda: 0.01\n",
      "    Loss_i: 0.15348294 Loss_f: 0.16416295 rho -0.0002542723 lambda: 0.099999994\n",
      "    Loss_i: 0.15348294 Loss_f: 0.15769513 rho -1.0201762e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15348294 Loss_f: 0.15339899 rho 2.036834e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15339899 Loss_f: 0.15398805 rho -1.4287123e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15339899 Loss_f: 0.15398805 rho -1.4287122e-07 lambda: 10.0\n",
      "    Train error: 22.445778\n",
      "-- Epoch: 19  Batch: 12 --\n",
      "    Loss_i: 0.1580589 Loss_f: 0.16178067 rho -0.0007287191 lambda: 0.01\n",
      "    Loss_i: 0.1580589 Loss_f: 0.16163017 rho -8.2361614e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1580589 Loss_f: 0.15946528 rho -3.3021745e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1580589 Loss_f: 0.15802458 rho 8.072287e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15802458 Loss_f: 0.15822065 rho -4.6116924e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15802458 Loss_f: 0.15822065 rho -4.611692e-08 lambda: 10.0\n",
      "    Train error: 23.333334\n",
      "-- Epoch: 19  Batch: 13 --\n",
      "    Loss_i: 0.14940542 Loss_f: 0.14985488 rho -7.02593e-05 lambda: 0.01\n",
      "    Loss_i: 0.14940542 Loss_f: 0.15014493 rho -1.4119703e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14940542 Loss_f: 0.1497018 rho -5.7870653e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14940542 Loss_f: 0.14939229 rho 2.5691178e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14939229 Loss_f: 0.14943238 rho -7.844926e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14939229 Loss_f: 0.14943238 rho -7.844925e-09 lambda: 10.0\n",
      "    Train error: 21.68378\n",
      "-- Epoch: 19  Batch: 14 --\n",
      "    Loss_i: 0.15400305 Loss_f: 0.15403818 rho -6.65569e-06 lambda: 0.01\n",
      "    Loss_i: 0.15400305 Loss_f: 0.15444213 rho -1.0847426e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15400305 Loss_f: 0.15419638 rho -4.9257534e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15400305 Loss_f: 0.15399243 rho 2.7155045e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15399243 Loss_f: 0.1540196 rho -6.942609e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15399243 Loss_f: 0.1540196 rho -6.942609e-09 lambda: 10.0\n",
      "    Train error: 22.412798\n",
      "-- Epoch: 19  Batch: 15 --\n",
      "    Loss_i: 0.15201728 Loss_f: 0.15157045 rho 7.5057374e-05 lambda: 0.01\n",
      "    Loss_i: 0.15157045 Loss_f: 0.1515992 rho -4.815925e-06 lambda: 0.01\n",
      "    Loss_i: 0.15157045 Loss_f: 0.15187229 rho -5.724965e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15157045 Loss_f: 0.15175799 rho -3.6045074e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15157045 Loss_f: 0.15160476 rho -6.602064e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15157045 Loss_f: 0.15160476 rho -6.602064e-09 lambda: 10.0\n",
      "    Train error: 21.840649\n",
      "-- Epoch: 19  Batch: 16 --\n",
      "    Loss_i: 0.15397882 Loss_f: 0.15897377 rho -0.000676075 lambda: 0.01\n",
      "    Loss_i: 0.15397882 Loss_f: 0.15866011 rho -0.000110899244 lambda: 0.099999994\n",
      "    Loss_i: 0.15397882 Loss_f: 0.1557342 rho -4.4957474e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15397882 Loss_f: 0.15392649 rho 1.3512772e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15392649 Loss_f: 0.15415566 rho -5.9174347e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15392649 Loss_f: 0.15415566 rho -5.917434e-08 lambda: 10.0\n",
      "    Train error: 22.547165\n",
      "-- Epoch: 19  Batch: 17 --\n",
      "    Loss_i: 0.15178926 Loss_f: 0.15263382 rho -0.00012649923 lambda: 0.01\n",
      "    Loss_i: 0.15178926 Loss_f: 0.15286864 rho -2.052248e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15178926 Loss_f: 0.15221995 rho -8.4154647e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15178926 Loss_f: 0.15177487 rho 2.8204377e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15177487 Loss_f: 0.15183312 rho -1.1414721e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15177487 Loss_f: 0.1518331 rho -1.1411799e-08 lambda: 10.0\n",
      "    Train error: 21.883808\n",
      "-- Epoch: 19  Batch: 18 --\n",
      "    Loss_i: 0.1541688 Loss_f: 0.1542477 rho -1.2277792e-05 lambda: 0.01\n",
      "    Loss_i: 0.1541688 Loss_f: 0.1545625 rho -7.736187e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1541688 Loss_f: 0.15433703 rho -3.3949698e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1541688 Loss_f: 0.15415868 rho 2.0473179e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15415868 Loss_f: 0.15418136 rho -4.589384e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15415868 Loss_f: 0.15418136 rho -4.589384e-09 lambda: 10.0\n",
      "    Train error: 22.32345\n",
      "-- Epoch: 19  Batch: 19 --\n",
      "    Loss_i: 0.15599997 Loss_f: 0.15547667 rho 9.821824e-05 lambda: 0.01\n",
      "    Loss_i: 0.15547667 Loss_f: 0.15551591 rho -7.292005e-06 lambda: 0.01\n",
      "    Loss_i: 0.15547667 Loss_f: 0.15582347 rho -7.382626e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15547667 Loss_f: 0.15569362 rho -4.6865327e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15547667 Loss_f: 0.15551634 rho -8.581625e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15547667 Loss_f: 0.15551634 rho -8.581624e-09 lambda: 10.0\n",
      "    Train error: 22.745037\n",
      "-- Epoch: 19  Batch: 20 --\n",
      "    Loss_i: 0.15198655 Loss_f: 0.15425426 rho -0.0003443847 lambda: 0.01\n",
      "    Loss_i: 0.15198655 Loss_f: 0.1541642 rho -4.1507814e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15198655 Loss_f: 0.15261021 rho -1.2198708e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15198655 Loss_f: 0.15187909 rho 2.1075678e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15187909 Loss_f: 0.15191954 rho -7.933592e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15187909 Loss_f: 0.15191954 rho -7.933591e-09 lambda: 10.0\n",
      "    Train error: 21.956102\n",
      "-- Epoch: 19  Batch: 21 --\n",
      "    Loss_i: 0.15419361 Loss_f: 0.15411432 rho 1.3680694e-05 lambda: 0.01\n",
      "    Loss_i: 0.15411432 Loss_f: 0.15584563 rho -0.00030169226 lambda: 0.01\n",
      "    Loss_i: 0.15411432 Loss_f: 0.1560127 rho -3.913008e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15411432 Loss_f: 0.15522045 rho -2.3224636e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15411432 Loss_f: 0.15431888 rho -4.3030926e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15411432 Loss_f: 0.15431888 rho -4.3030923e-08 lambda: 10.0\n",
      "    Train error: 22.677767\n",
      "-- Epoch: 19  Batch: 22 --\n",
      "    Loss_i: 0.1515134 Loss_f: 0.18958291 rho -0.005654501 lambda: 0.01\n",
      "    Loss_i: 0.1515134 Loss_f: 0.18452036 rho -0.0007352496 lambda: 0.099999994\n",
      "    Loss_i: 0.1515134 Loss_f: 0.16402571 rho -2.9338002e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1515134 Loss_f: 0.15108193 rho 1.0170143e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15108193 Loss_f: 0.15274684 rho -3.9226776e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15108193 Loss_f: 0.15274683 rho -3.922642e-07 lambda: 10.0\n",
      "    Train error: 21.743284\n",
      "-- Epoch: 19  Batch: 23 --\n",
      "    Loss_i: 0.1563577 Loss_f: 0.17003272 rho -0.002728208 lambda: 0.01\n",
      "    Loss_i: 0.1563577 Loss_f: 0.16865888 rho -0.00031510676 lambda: 0.099999994\n",
      "    Loss_i: 0.1563577 Loss_f: 0.16122605 rho -1.28352485e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1563577 Loss_f: 0.15623693 rho 3.1935045e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15623693 Loss_f: 0.1569149 rho -1.7923291e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15623693 Loss_f: 0.1569149 rho -1.792329e-07 lambda: 10.0\n",
      "    Train error: 22.826109\n",
      "-- Epoch: 19  Batch: 24 --\n",
      "    Loss_i: 0.15579942 Loss_f: 0.16079056 rho -0.00089056615 lambda: 0.01\n",
      "    Loss_i: 0.15579942 Loss_f: 0.16038954 rho -0.00010923311 lambda: 0.099999994\n",
      "    Loss_i: 0.15579942 Loss_f: 0.1575313 rho -4.2637344e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15579942 Loss_f: 0.15574135 rho 1.4345738e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15574135 Loss_f: 0.15597238 rho -5.7065623e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15574135 Loss_f: 0.15597238 rho -5.706562e-08 lambda: 10.0\n",
      "    Train error: 22.784824\n",
      "-- Epoch: 19  Batch: 25 --\n",
      "    Loss_i: 0.15660745 Loss_f: 0.15746349 rho -0.00014096613 lambda: 0.01\n",
      "    Loss_i: 0.15660745 Loss_f: 0.15762696 rho -2.0680995e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15660745 Loss_f: 0.15699348 rho -8.0165853e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15660745 Loss_f: 0.15658987 rho 3.660184e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15658987 Loss_f: 0.15664001 rho -1.0438825e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15658987 Loss_f: 0.15664001 rho -1.0438824e-08 lambda: 10.0\n",
      "    Train error: 23.034788\n",
      "-- Epoch: 19  Batch: 26 --\n",
      "    Loss_i: 0.15277594 Loss_f: 0.15257934 rho 2.9215169e-05 lambda: 0.01\n",
      "    Loss_i: 0.15257934 Loss_f: 0.1532414 rho -9.6227275e-05 lambda: 0.01\n",
      "    Loss_i: 0.15257934 Loss_f: 0.15343577 rho -1.6254e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15257934 Loss_f: 0.1530773 rho -9.748909e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15257934 Loss_f: 0.15266964 rho -1.7734546e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15257934 Loss_f: 0.15266964 rho -1.7734543e-08 lambda: 10.0\n",
      "    Train error: 22.08097\n",
      "-- Epoch: 19  Batch: 27 --\n",
      "    Loss_i: 0.15175848 Loss_f: 0.16590312 rho -0.0026466476 lambda: 0.01\n",
      "    Loss_i: 0.15175848 Loss_f: 0.16385551 rho -0.00028781316 lambda: 0.099999994\n",
      "    Loss_i: 0.15175848 Loss_f: 0.15606013 rho -1.0520176e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15175848 Loss_f: 0.15157473 rho 4.5062983e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15157473 Loss_f: 0.1521134 rho -1.3213094e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15157473 Loss_f: 0.1521134 rho -1.3213092e-07 lambda: 10.0\n",
      "    Train error: 21.939678\n",
      "-- Epoch: 19  Batch: 28 --\n",
      "    Loss_i: 0.15052609 Loss_f: 0.15274589 rho -0.00035621919 lambda: 0.01\n",
      "    Loss_i: 0.15052609 Loss_f: 0.15276411 rho -4.467229e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15052609 Loss_f: 0.15136854 rho -1.7236189e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15052609 Loss_f: 0.1504829 rho 8.860357e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1504829 Loss_f: 0.15059102 rho -2.217415e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1504829 Loss_f: 0.15059102 rho -2.217415e-08 lambda: 10.0\n",
      "    Train error: 21.601345\n",
      "-- Epoch: 19  Batch: 29 --\n",
      "    Loss_i: 0.15261391 Loss_f: 0.15257007 rho 7.155666e-06 lambda: 0.01\n",
      "    Loss_i: 0.15257007 Loss_f: 0.15384178 rho -0.00020848961 lambda: 0.01\n",
      "    Loss_i: 0.15257007 Loss_f: 0.15397814 rho -2.7102475e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15257007 Loss_f: 0.15337066 rho -1.5682776e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15257007 Loss_f: 0.15271556 rho -2.855144e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15257007 Loss_f: 0.15271556 rho -2.8551439e-08 lambda: 10.0\n",
      "    Train error: 22.273746\n",
      "-- Epoch: 19  Batch: 30 --\n",
      "    Loss_i: 0.15351756 Loss_f: 0.1817558 rho -0.005171485 lambda: 0.01\n",
      "    Loss_i: 0.15351756 Loss_f: 0.17847331 rho -0.00063662266 lambda: 0.099999994\n",
      "    Loss_i: 0.15351756 Loss_f: 0.1632049 rho -2.5723233e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15351756 Loss_f: 0.15327057 rho 6.5852895e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15327057 Loss_f: 0.15457012 rho -3.4623122e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15327057 Loss_f: 0.15457012 rho -3.462312e-07 lambda: 10.0\n",
      "    Train error: 21.927095\n",
      "\n",
      "*** Epoch: 19 Train error: 22.32894376118978  Test error: 21.799026  Time: 709.1818254999998 sec\n",
      "\n",
      "-- Epoch: 20  Batch: 1 --\n",
      "    Loss_i: 0.14994274 Loss_f: 0.15690708 rho -0.0012119836 lambda: 0.01\n",
      "    Loss_i: 0.14994274 Loss_f: 0.15623723 rho -0.00013189644 lambda: 0.099999994\n",
      "    Loss_i: 0.14994274 Loss_f: 0.15223719 rho -4.9080186e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14994274 Loss_f: 0.14986743 rho 1.6143165e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14986743 Loss_f: 0.15015881 rho -6.246262e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14986743 Loss_f: 0.15015881 rho -6.246261e-08 lambda: 10.0\n",
      "    Train error: 21.402325\n",
      "-- Epoch: 20  Batch: 2 --\n",
      "    Loss_i: 0.15201117 Loss_f: 0.15218529 rho -2.5298837e-05 lambda: 0.01\n",
      "    Loss_i: 0.15201117 Loss_f: 0.15248626 rho -8.638821e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15201117 Loss_f: 0.15218854 rho -3.308362e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15201117 Loss_f: 0.15199786 rho 2.4884548e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15199786 Loss_f: 0.1520184 rho -3.8397916e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15199786 Loss_f: 0.1520184 rho -3.8397916e-09 lambda: 10.0\n",
      "    Train error: 21.8095\n",
      "-- Epoch: 20  Batch: 3 --\n",
      "    Loss_i: 0.15405256 Loss_f: 0.1536677 rho 6.269784e-05 lambda: 0.01\n",
      "    Loss_i: 0.1536677 Loss_f: 0.15387256 rho -3.2779615e-05 lambda: 0.01\n",
      "    Loss_i: 0.1536677 Loss_f: 0.1541276 rho -8.471956e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1536677 Loss_f: 0.1539427 rho -5.143751e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1536677 Loss_f: 0.15371798 rho -9.418431e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1536677 Loss_f: 0.15371798 rho -9.418431e-09 lambda: 10.0\n",
      "    Train error: 22.112274\n",
      "-- Epoch: 20  Batch: 4 --\n",
      "    Loss_i: 0.14937453 Loss_f: 0.15507324 rho -0.0008214019 lambda: 0.01\n",
      "    Loss_i: 0.14937453 Loss_f: 0.15455955 rho -0.00010407041 lambda: 0.099999994\n",
      "    Loss_i: 0.14937453 Loss_f: 0.15131609 rho -4.056185e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14937453 Loss_f: 0.14930397 rho 1.4800776e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14930397 Loss_f: 0.14956515 rho -5.4792743e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14930397 Loss_f: 0.14956515 rho -5.479273e-08 lambda: 10.0\n",
      "    Train error: 21.463633\n",
      "-- Epoch: 20  Batch: 5 --\n",
      "    Loss_i: 0.15293516 Loss_f: 0.15350647 rho -9.1714006e-05 lambda: 0.01\n",
      "    Loss_i: 0.15293516 Loss_f: 0.15374869 rho -1.5649097e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15293516 Loss_f: 0.15322956 rho -5.777689e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15293516 Loss_f: 0.15291327 rho 4.304615e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15291327 Loss_f: 0.15294735 rho -6.7010872e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15291327 Loss_f: 0.15294735 rho -6.701087e-09 lambda: 10.0\n",
      "    Train error: 22.231747\n",
      "-- Epoch: 20  Batch: 6 --\n",
      "    Loss_i: 0.14715911 Loss_f: 0.14712608 rho 4.85968e-06 lambda: 0.01\n",
      "    Loss_i: 0.14712608 Loss_f: 0.14861491 rho -0.00022030354 lambda: 0.01\n",
      "    Loss_i: 0.14712608 Loss_f: 0.14875536 rho -2.7652171e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14712608 Loss_f: 0.1480793 rho -1.6419561e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14712608 Loss_f: 0.14730321 rho -3.055666e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14712608 Loss_f: 0.14730321 rho -3.055666e-08 lambda: 10.0\n",
      "    Train error: 21.097548\n",
      "-- Epoch: 20  Batch: 7 --\n",
      "    Loss_i: 0.15599531 Loss_f: 0.19929682 rho -0.0068236818 lambda: 0.01\n",
      "    Loss_i: 0.15599531 Loss_f: 0.1939639 rho -0.0009433681 lambda: 0.099999994\n",
      "    Loss_i: 0.15599531 Loss_f: 0.17085224 rho -3.917252e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15599531 Loss_f: 0.1556547 rho 9.0360025e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1556547 Loss_f: 0.15768988 rho -5.395299e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1556547 Loss_f: 0.15768988 rho -5.395297e-07 lambda: 10.0\n",
      "    Train error: 22.558422\n",
      "-- Epoch: 20  Batch: 8 --\n",
      "    Loss_i: 0.15602478 Loss_f: 0.17406087 rho -0.0022741242 lambda: 0.01\n",
      "    Loss_i: 0.15602478 Loss_f: 0.17208187 rho -0.00041166358 lambda: 0.099999994\n",
      "    Loss_i: 0.15602478 Loss_f: 0.16220833 rho -1.7679964e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15602478 Loss_f: 0.15586388 rho 4.654154e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15586388 Loss_f: 0.15669215 rho -2.394979e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15586388 Loss_f: 0.15669215 rho -2.394979e-07 lambda: 10.0\n",
      "    Train error: 23.072989\n",
      "-- Epoch: 20  Batch: 9 --\n",
      "    Loss_i: 0.1528456 Loss_f: 0.1602225 rho -0.0016307359 lambda: 0.01\n",
      "    Loss_i: 0.1528456 Loss_f: 0.15956597 rho -0.00017606487 lambda: 0.099999994\n",
      "    Loss_i: 0.1528456 Loss_f: 0.15538324 rho -6.773692e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1528456 Loss_f: 0.15275761 rho 2.3531838e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15275761 Loss_f: 0.15308985 rho -8.884363e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15275761 Loss_f: 0.15308985 rho -8.884362e-08 lambda: 10.0\n",
      "    Train error: 22.2273\n",
      "-- Epoch: 20  Batch: 10 --\n",
      "    Loss_i: 0.15215452 Loss_f: 0.15419349 rho -0.00036051217 lambda: 0.01\n",
      "    Loss_i: 0.15215452 Loss_f: 0.15423763 rho -4.3054573e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15215452 Loss_f: 0.15292764 rho -1.6253739e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15215452 Loss_f: 0.15212402 rho 6.4238157e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15212402 Loss_f: 0.15222175 rho -2.0583933e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15212402 Loss_f: 0.15222175 rho -2.058393e-08 lambda: 10.0\n",
      "    Train error: 22.080889\n",
      "-- Epoch: 20  Batch: 11 --\n",
      "    Loss_i: 0.15189874 Loss_f: 0.15178086 rho 1.7995355e-05 lambda: 0.01\n",
      "    Loss_i: 0.15178086 Loss_f: 0.1527914 rho -0.00015062772 lambda: 0.01\n",
      "    Loss_i: 0.15178086 Loss_f: 0.15296982 rho -2.118175e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15178086 Loss_f: 0.15249291 rho -1.2937979e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15178086 Loss_f: 0.1519142 rho -2.4275392e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15178086 Loss_f: 0.1519142 rho -2.4275387e-08 lambda: 10.0\n",
      "    Train error: 21.848043\n",
      "-- Epoch: 20  Batch: 12 --\n",
      "    Loss_i: 0.15691626 Loss_f: 0.18392602 rho -0.0051675425 lambda: 0.01\n",
      "    Loss_i: 0.15691626 Loss_f: 0.18075737 rho -0.0006235509 lambda: 0.099999994\n",
      "    Loss_i: 0.15691626 Loss_f: 0.16665405 rho -2.6439067e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15691626 Loss_f: 0.15666547 rho 6.835145e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15666547 Loss_f: 0.1580767 rho -3.846653e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15666547 Loss_f: 0.1580767 rho -3.8466527e-07 lambda: 10.0\n",
      "    Train error: 23.066864\n",
      "-- Epoch: 20  Batch: 13 --\n",
      "    Loss_i: 0.14801466 Loss_f: 0.16159748 rho -0.0023817706 lambda: 0.01\n",
      "    Loss_i: 0.14801466 Loss_f: 0.16021559 rho -0.00024848402 lambda: 0.099999994\n",
      "    Loss_i: 0.14801466 Loss_f: 0.15285945 rho -1.0028797e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14801466 Loss_f: 0.147912 rho 2.128761e-08 lambda: 9.999999\n",
      "    Loss_i: 0.147912 Loss_f: 0.1485934 rho -1.4122008e-07 lambda: 9.999999\n",
      "    Loss_i: 0.147912 Loss_f: 0.14859341 rho -1.4122315e-07 lambda: 10.0\n",
      "    Train error: 21.251448\n",
      "-- Epoch: 20  Batch: 14 --\n",
      "    Loss_i: 0.15244837 Loss_f: 0.15568164 rho -0.0006654061 lambda: 0.01\n",
      "    Loss_i: 0.15244837 Loss_f: 0.15565632 rho -8.4627005e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15244837 Loss_f: 0.15370816 rho -3.4197546e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15244837 Loss_f: 0.15240604 rho 1.152524e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15240604 Loss_f: 0.15257747 rho -4.666945e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15240604 Loss_f: 0.15257747 rho -4.666944e-08 lambda: 10.0\n",
      "    Train error: 22.136099\n",
      "-- Epoch: 20  Batch: 15 --\n",
      "    Loss_i: 0.15038797 Loss_f: 0.15100344 rho -9.475536e-05 lambda: 0.01\n",
      "    Loss_i: 0.15038797 Loss_f: 0.15124823 rho -1.521088e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15038797 Loss_f: 0.15074366 rho -6.384023e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15038797 Loss_f: 0.15037452 rho 2.418716e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15037452 Loss_f: 0.15042399 rho -8.892072e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15037452 Loss_f: 0.15042399 rho -8.892071e-09 lambda: 10.0\n",
      "    Train error: 21.472805\n",
      "-- Epoch: 20  Batch: 16 --\n",
      "    Loss_i: 0.15268639 Loss_f: 0.15294899 rho -4.7428726e-05 lambda: 0.01\n",
      "    Loss_i: 0.15268639 Loss_f: 0.15331061 rho -1.4678805e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15268639 Loss_f: 0.15295008 rho -6.393851e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15268639 Loss_f: 0.15267386 rho 3.0481608e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15267386 Loss_f: 0.15271075 rho -8.973994e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15267386 Loss_f: 0.15271077 rho -8.977619e-09 lambda: 10.0\n",
      "    Train error: 22.274208\n",
      "-- Epoch: 20  Batch: 17 --\n",
      "    Loss_i: 0.1505016 Loss_f: 0.15040907 rho 1.4645612e-05 lambda: 0.01\n",
      "    Loss_i: 0.15040907 Loss_f: 0.15143113 rho -0.00015801485 lambda: 0.01\n",
      "    Loss_i: 0.15040907 Loss_f: 0.15159488 rho -2.1453576e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15040907 Loss_f: 0.15113658 rho -1.3389914e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15040907 Loss_f: 0.15054914 rho -2.5825155e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15040907 Loss_f: 0.15054914 rho -2.5825155e-08 lambda: 10.0\n",
      "    Train error: 21.426868\n",
      "-- Epoch: 20  Batch: 18 --\n",
      "    Loss_i: 0.15304647 Loss_f: 0.19399889 rho -0.0076297126 lambda: 0.01\n",
      "    Loss_i: 0.15304647 Loss_f: 0.18878508 rho -0.0008327825 lambda: 0.099999994\n",
      "    Loss_i: 0.15304647 Loss_f: 0.1676828 rho -3.498278e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15304647 Loss_f: 0.15285194 rho 4.6616286e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15285194 Loss_f: 0.1549623 rho -5.060543e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15285194 Loss_f: 0.15496229 rho -5.0605064e-07 lambda: 10.0\n",
      "    Train error: 22.149937\n",
      "-- Epoch: 20  Batch: 19 --\n",
      "    Loss_i: 0.15457523 Loss_f: 0.17848305 rho -0.0042648814 lambda: 0.01\n",
      "    Loss_i: 0.15457523 Loss_f: 0.17563625 rho -0.0005073233 lambda: 0.099999994\n",
      "    Loss_i: 0.15457523 Loss_f: 0.16297421 rho -2.0966181e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15457523 Loss_f: 0.15441057 rho 4.1252925e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15441057 Loss_f: 0.15559563 rho -2.9670068e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15441057 Loss_f: 0.15559563 rho -2.9670068e-07 lambda: 10.0\n",
      "    Train error: 22.441526\n",
      "-- Epoch: 20  Batch: 20 --\n",
      "    Loss_i: 0.1507731 Loss_f: 0.1631845 rho -0.0021521552 lambda: 0.01\n",
      "    Loss_i: 0.1507731 Loss_f: 0.16188781 rho -0.0002263278 lambda: 0.099999994\n",
      "    Loss_i: 0.1507731 Loss_f: 0.15527673 rho -9.333416e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1507731 Loss_f: 0.15070044 rho 1.508454e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15070044 Loss_f: 0.1513551 rho -1.3591178e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15070044 Loss_f: 0.15135512 rho -1.3591485e-07 lambda: 10.0\n",
      "    Train error: 21.730274\n",
      "-- Epoch: 20  Batch: 21 --\n",
      "    Loss_i: 0.15273218 Loss_f: 0.15626395 rho -0.00065985613 lambda: 0.01\n",
      "    Loss_i: 0.15273218 Loss_f: 0.15615956 rho -7.540825e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15273218 Loss_f: 0.15402682 rho -2.8999343e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15273218 Loss_f: 0.15269764 rho 7.750999e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15269764 Loss_f: 0.1528674 rho -3.808942e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15269764 Loss_f: 0.1528674 rho -3.8089418e-08 lambda: 10.0\n",
      "    Train error: 22.144949\n",
      "-- Epoch: 20  Batch: 22 --\n",
      "    Loss_i: 0.14974786 Loss_f: 0.14991479 rho -2.581365e-05 lambda: 0.01\n",
      "    Loss_i: 0.14974786 Loss_f: 0.15017812 rho -7.865028e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14974786 Loss_f: 0.14992562 rho -3.3096134e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14974786 Loss_f: 0.1497396 rho 1.5426668e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1497396 Loss_f: 0.14976329 rho -4.419446e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1497396 Loss_f: 0.14976327 rho -4.4166666e-09 lambda: 10.0\n",
      "    Train error: 21.485073\n",
      "-- Epoch: 20  Batch: 23 --\n",
      "    Loss_i: 0.15510702 Loss_f: 0.15468168 rho 7.4980926e-05 lambda: 0.01\n",
      "    Loss_i: 0.15468168 Loss_f: 0.15466543 rho 2.8273726e-06 lambda: 0.01\n",
      "    Loss_i: 0.15466543 Loss_f: 0.155069 rho -6.922654e-05 lambda: 0.01\n",
      "    Loss_i: 0.15466543 Loss_f: 0.15519832 rho -1.0882174e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15466543 Loss_f: 0.15499593 rho -6.8802973e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15466543 Loss_f: 0.15473308 rho -1.4110607e-08 lambda: 9.999999\n",
      "    Train error: 22.336702\n",
      "-- Epoch: 20  Batch: 24 --\n",
      "    Loss_i: 0.15492424 Loss_f: 0.17795159 rho -0.0042528105 lambda: 0.01\n",
      "    Loss_i: 0.15492424 Loss_f: 0.17529829 rho -0.00049012183 lambda: 0.099999994\n",
      "    Loss_i: 0.15492424 Loss_f: 0.16308096 rho -2.023414e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15492424 Loss_f: 0.15468475 rho 5.95959e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15468475 Loss_f: 0.1558206 rho -2.8269463e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15468475 Loss_f: 0.1558206 rho -2.8269457e-07 lambda: 10.0\n",
      "    Train error: 22.656149\n",
      "-- Epoch: 20  Batch: 25 --\n",
      "    Loss_i: 0.15560687 Loss_f: 0.16602734 rho -0.0020042264 lambda: 0.01\n",
      "    Loss_i: 0.15560687 Loss_f: 0.16501191 rho -0.00021194952 lambda: 0.099999994\n",
      "    Loss_i: 0.15560687 Loss_f: 0.15928875 rho -8.442338e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15560687 Loss_f: 0.15549123 rho 2.6560347e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15549123 Loss_f: 0.15600269 rho -1.1744795e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15549123 Loss_f: 0.15600269 rho -1.1744795e-07 lambda: 10.0\n",
      "    Train error: 22.7487\n",
      "-- Epoch: 20  Batch: 26 --\n",
      "    Loss_i: 0.1514803 Loss_f: 0.1570438 rho -0.0007759226 lambda: 0.01\n",
      "    Loss_i: 0.1514803 Loss_f: 0.15673898 rho -0.00010736601 lambda: 0.099999994\n",
      "    Loss_i: 0.1514803 Loss_f: 0.15353703 rho -4.4035037e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1514803 Loss_f: 0.15142159 rho 1.26314985e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15142159 Loss_f: 0.15170634 rho -6.1272345e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15142159 Loss_f: 0.15170634 rho -6.127234e-08 lambda: 10.0\n",
      "    Train error: 21.94624\n",
      "-- Epoch: 20  Batch: 27 --\n",
      "    Loss_i: 0.1503462 Loss_f: 0.15298794 rho -0.00044620474 lambda: 0.01\n",
      "    Loss_i: 0.1503462 Loss_f: 0.15298885 rho -5.1080606e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1503462 Loss_f: 0.15139519 rho -2.0573182e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1503462 Loss_f: 0.15030831 rho 7.442802e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15030831 Loss_f: 0.1504537 rho -2.8551753e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15030831 Loss_f: 0.15045372 rho -2.8554679e-08 lambda: 10.0\n",
      "    Train error: 21.578796\n",
      "-- Epoch: 20  Batch: 28 --\n",
      "    Loss_i: 0.14947902 Loss_f: 0.14983901 rho -4.8857757e-05 lambda: 0.01\n",
      "    Loss_i: 0.14947902 Loss_f: 0.15019247 rho -1.3035071e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14947902 Loss_f: 0.14978318 rho -5.7564654e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14947902 Loss_f: 0.14946596 rho 2.4793334e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14946596 Loss_f: 0.14950836 rho -8.052398e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14946596 Loss_f: 0.14950836 rho -8.052398e-09 lambda: 10.0\n",
      "    Train error: 21.418333\n",
      "-- Epoch: 20  Batch: 29 --\n",
      "    Loss_i: 0.15130848 Loss_f: 0.15134947 rho -5.3676777e-06 lambda: 0.01\n",
      "    Loss_i: 0.15130848 Loss_f: 0.15165915 rho -6.2928398e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15130848 Loss_f: 0.15145703 rho -2.7683217e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15130848 Loss_f: 0.15129845 rho 1.8760922e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15129845 Loss_f: 0.15131778 rho -3.615439e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15129845 Loss_f: 0.15131778 rho -3.6154386e-09 lambda: 10.0\n",
      "    Train error: 21.723055\n",
      "-- Epoch: 20  Batch: 30 --\n",
      "    Loss_i: 0.15208873 Loss_f: 0.15222828 rho -1.7513874e-05 lambda: 0.01\n",
      "    Loss_i: 0.15208873 Loss_f: 0.15264006 rho -1.0656292e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15208873 Loss_f: 0.1523225 rho -4.776349e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15208873 Loss_f: 0.15207663 rho 2.4864006e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15207663 Loss_f: 0.1521084 rho -6.5286763e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15207663 Loss_f: 0.1521084 rho -6.528676e-09 lambda: 10.0\n",
      "    Train error: 21.71103\n",
      "\n",
      "*** Epoch: 20 Train error: 21.986790784200032  Test error: 21.629503  Time: 705.6087255000002 sec\n",
      "\n",
      "-- Epoch: 21  Batch: 1 --\n",
      "    Loss_i: 0.14891009 Loss_f: 0.1491651 rho -3.710662e-05 lambda: 0.01\n",
      "    Loss_i: 0.14891009 Loss_f: 0.14948611 rho -9.922931e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14891009 Loss_f: 0.14913908 rho -4.0185662e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14891009 Loss_f: 0.14889517 rho 2.62259e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14889517 Loss_f: 0.1489245 rho -5.1556523e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14889517 Loss_f: 0.1489245 rho -5.1556523e-09 lambda: 10.0\n",
      "    Train error: 21.063063\n",
      "-- Epoch: 21  Batch: 2 --\n",
      "    Loss_i: 0.15100966 Loss_f: 0.15177615 rho -0.00011680286 lambda: 0.01\n",
      "    Loss_i: 0.15100966 Loss_f: 0.15202902 rho -1.8544688e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15100966 Loss_f: 0.1514378 rho -7.942901e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15100966 Loss_f: 0.15099387 rho 2.9361524e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15099387 Loss_f: 0.15105456 rho -1.12819505e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15099387 Loss_f: 0.15105456 rho -1.1281949e-08 lambda: 10.0\n",
      "    Train error: 21.676989\n",
      "-- Epoch: 21  Batch: 3 --\n",
      "    Loss_i: 0.15312418 Loss_f: 0.15428157 rho -0.00019019186 lambda: 0.01\n",
      "    Loss_i: 0.15312418 Loss_f: 0.15458149 rho -2.9362196e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15312418 Loss_f: 0.15371805 rho -1.224229e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15312418 Loss_f: 0.15310244 rho 4.4921253e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15310244 Loss_f: 0.15318349 rho -1.6743076e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15310244 Loss_f: 0.15318349 rho -1.6743074e-08 lambda: 10.0\n",
      "    Train error: 22.019636\n",
      "-- Epoch: 21  Batch: 4 --\n",
      "    Loss_i: 0.14863348 Loss_f: 0.14844611 rho 2.8907074e-05 lambda: 0.01\n",
      "    Loss_i: 0.14844611 Loss_f: 0.14944322 rho -0.0001545485 lambda: 0.01\n",
      "    Loss_i: 0.14844611 Loss_f: 0.14961259 rho -2.102528e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14844611 Loss_f: 0.1491619 rho -1.3115515e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14844611 Loss_f: 0.14858288 rho -2.5100734e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14844611 Loss_f: 0.14858288 rho -2.5100732e-08 lambda: 10.0\n",
      "    Train error: 21.34698\n",
      "-- Epoch: 21  Batch: 5 --\n",
      "    Loss_i: 0.15225004 Loss_f: 0.18758486 rho -0.0052521196 lambda: 0.01\n",
      "    Loss_i: 0.15225004 Loss_f: 0.18317904 rho -0.00064663636 lambda: 0.099999994\n",
      "    Loss_i: 0.15225004 Loss_f: 0.16461067 rho -2.6937734e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15225004 Loss_f: 0.15210803 rho 3.1079782e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15210803 Loss_f: 0.15386467 rho -3.8442002e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15210803 Loss_f: 0.15386465 rho -3.8441672e-07 lambda: 10.0\n",
      "    Train error: 21.997828\n",
      "-- Epoch: 21  Batch: 6 --\n",
      "    Loss_i: 0.14631419 Loss_f: 0.16160734 rho -0.0023944844 lambda: 0.01\n",
      "    Loss_i: 0.14631419 Loss_f: 0.16012889 rho -0.0002736183 lambda: 0.099999994\n",
      "    Loss_i: 0.14631419 Loss_f: 0.15189345 rho -1.13512615e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14631419 Loss_f: 0.14620417 rho 2.24442e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14620417 Loss_f: 0.14700374 rho -1.6309968e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14620417 Loss_f: 0.14700377 rho -1.6310577e-07 lambda: 10.0\n",
      "    Train error: 20.778648\n",
      "-- Epoch: 21  Batch: 7 --\n",
      "    Loss_i: 0.15468773 Loss_f: 0.162166 rho -0.0012991085 lambda: 0.01\n",
      "    Loss_i: 0.15468773 Loss_f: 0.16166522 rho -0.00015590996 lambda: 0.099999994\n",
      "    Loss_i: 0.15468773 Loss_f: 0.1573746 rho -6.1806786e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15468773 Loss_f: 0.15461904 rho 1.5848634e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15461904 Loss_f: 0.15498085 rho -8.346051e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15461904 Loss_f: 0.15498085 rho -8.3460506e-08 lambda: 10.0\n",
      "    Train error: 22.345871\n",
      "-- Epoch: 21  Batch: 8 --\n",
      "    Loss_i: 0.15492725 Loss_f: 0.15761806 rho -0.0005345153 lambda: 0.01\n",
      "    Loss_i: 0.15492725 Loss_f: 0.1577156 rho -6.615631e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15492725 Loss_f: 0.15600218 rho -2.6009238e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15492725 Loss_f: 0.1548855 rho 1.0122773e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1548855 Loss_f: 0.15502742 rho -3.4400035e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1548855 Loss_f: 0.15502742 rho -3.4400035e-08 lambda: 10.0\n",
      "    Train error: 22.804285\n",
      "-- Epoch: 21  Batch: 9 --\n",
      "    Loss_i: 0.15180762 Loss_f: 0.15321466 rho -0.00025811826 lambda: 0.01\n",
      "    Loss_i: 0.15180762 Loss_f: 0.15333411 rho -3.3490793e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15180762 Loss_f: 0.15239875 rho -1.3228456e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15180762 Loss_f: 0.15178743 rho 4.5274677e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15178743 Loss_f: 0.15186617 rho -1.7653104e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15178743 Loss_f: 0.15186617 rho -1.76531e-08 lambda: 10.0\n",
      "    Train error: 21.989422\n",
      "-- Epoch: 21  Batch: 10 --\n",
      "    Loss_i: 0.1512878 Loss_f: 0.1514068 rho -1.8243996e-05 lambda: 0.01\n",
      "    Loss_i: 0.1512878 Loss_f: 0.15173101 rho -8.318082e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1512878 Loss_f: 0.15147738 rho -3.6396406e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1512878 Loss_f: 0.15127847 rho 1.7949007e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15127847 Loss_f: 0.15130495 rho -5.095044e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15127847 Loss_f: 0.15130495 rho -5.095044e-09 lambda: 10.0\n",
      "    Train error: 21.870226\n",
      "-- Epoch: 21  Batch: 11 --\n",
      "    Loss_i: 0.15104684 Loss_f: 0.15052907 rho 8.6535714e-05 lambda: 0.01\n",
      "    Loss_i: 0.15052907 Loss_f: 0.15048955 rho 6.5177987e-06 lambda: 0.01\n",
      "    Loss_i: 0.15048955 Loss_f: 0.15093394 rho -7.232009e-05 lambda: 0.01\n",
      "    Loss_i: 0.15048955 Loss_f: 0.1510454 rho -1.0447829e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15048955 Loss_f: 0.15081033 rho -6.12436e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15048955 Loss_f: 0.15055175 rho -1.189361e-08 lambda: 9.999999\n",
      "    Train error: 21.568335\n",
      "-- Epoch: 21  Batch: 12 --\n",
      "    Loss_i: 0.15609927 Loss_f: 0.16331415 rho -0.0015227188 lambda: 0.01\n",
      "    Loss_i: 0.15609927 Loss_f: 0.16244464 rho -0.00016434053 lambda: 0.099999994\n",
      "    Loss_i: 0.15609927 Loss_f: 0.15793462 rho -4.8639054e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15609927 Loss_f: 0.15576251 rho 8.9455334e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15576251 Loss_f: 0.1558823 rho -3.18218e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15576251 Loss_f: 0.15588228 rho -3.1817837e-08 lambda: 10.0\n",
      "    Train error: 22.803032\n",
      "-- Epoch: 21  Batch: 13 --\n",
      "    Loss_i: 0.14708544 Loss_f: 0.14829831 rho -0.00019009491 lambda: 0.01\n",
      "    Loss_i: 0.14708544 Loss_f: 0.1484673 rho -2.5826086e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14708544 Loss_f: 0.14744994 rho -6.945895e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14708544 Loss_f: 0.14699718 rho 1.6851907e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14699718 Loss_f: 0.14701413 rho -3.2345322e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14699718 Loss_f: 0.14701413 rho -3.2345315e-09 lambda: 10.0\n",
      "    Train error: 21.039396\n",
      "-- Epoch: 21  Batch: 14 --\n",
      "    Loss_i: 0.15142 Loss_f: 0.15130134 rho 2.148688e-05 lambda: 0.01\n",
      "    Loss_i: 0.15130134 Loss_f: 0.1531284 rho -0.00033330033 lambda: 0.01\n",
      "    Loss_i: 0.15130134 Loss_f: 0.15317412 rho -4.1196858e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15130134 Loss_f: 0.15228616 rho -2.2119093e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15130134 Loss_f: 0.15147015 rho -3.7995903e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15130134 Loss_f: 0.15147015 rho -3.7995903e-08 lambda: 10.0\n",
      "    Train error: 21.968971\n",
      "-- Epoch: 21  Batch: 15 --\n",
      "    Loss_i: 0.14973937 Loss_f: 0.17553623 rho -0.0045547523 lambda: 0.01\n",
      "    Loss_i: 0.14973937 Loss_f: 0.17203568 rho -0.00047494937 lambda: 0.099999994\n",
      "    Loss_i: 0.14973937 Loss_f: 0.15802072 rho -1.801259e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14973937 Loss_f: 0.14940171 rho 7.359895e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14940171 Loss_f: 0.15047075 rho -2.3303086e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14940171 Loss_f: 0.15047075 rho -2.3303085e-07 lambda: 10.0\n",
      "    Train error: 21.23546\n",
      "-- Epoch: 21  Batch: 16 --\n",
      "    Loss_i: 0.1516303 Loss_f: 0.15971558 rho -0.0015926549 lambda: 0.01\n",
      "    Loss_i: 0.1516303 Loss_f: 0.15882115 rho -0.00018790297 lambda: 0.099999994\n",
      "    Loss_i: 0.1516303 Loss_f: 0.15420513 rho -6.955384e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1516303 Loss_f: 0.15148945 rho 3.8175507e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15148945 Loss_f: 0.15181433 rho -8.807893e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15148945 Loss_f: 0.15181433 rho -8.8078906e-08 lambda: 10.0\n",
      "    Train error: 22.00991\n",
      "-- Epoch: 21  Batch: 17 --\n",
      "    Loss_i: 0.14935188 Loss_f: 0.15201658 rho -0.0004408367 lambda: 0.01\n",
      "    Loss_i: 0.14935188 Loss_f: 0.1519225 rho -5.11048e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14935188 Loss_f: 0.15025097 rho -1.824222e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14935188 Loss_f: 0.14930405 rho 9.725096e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14930405 Loss_f: 0.14941542 rho -2.2643333e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14930405 Loss_f: 0.14941542 rho -2.264333e-08 lambda: 10.0\n",
      "    Train error: 21.227585\n",
      "-- Epoch: 21  Batch: 18 --\n",
      "    Loss_i: 0.15180835 Loss_f: 0.15267476 rho -0.00014497749 lambda: 0.01\n",
      "    Loss_i: 0.15180835 Loss_f: 0.15291907 rho -2.3097324e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15180835 Loss_f: 0.15220958 rho -8.551104e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15180835 Loss_f: 0.15177943 rho 6.1795626e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15177943 Loss_f: 0.15182742 rho -1.0255326e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15177943 Loss_f: 0.15182742 rho -1.0255326e-08 lambda: 10.0\n",
      "    Train error: 21.813894\n",
      "-- Epoch: 21  Batch: 19 --\n",
      "    Loss_i: 0.15334162 Loss_f: 0.15455554 rho -0.00021873027 lambda: 0.01\n",
      "    Loss_i: 0.15334162 Loss_f: 0.15471789 rho -2.8847817e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15334162 Loss_f: 0.15381846 rho -1.0160837e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15334162 Loss_f: 0.15331157 rho 6.4151555e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15331157 Loss_f: 0.15336873 rho -1.2201003e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15331157 Loss_f: 0.15336873 rho -1.2201002e-08 lambda: 10.0\n",
      "    Train error: 22.190477\n",
      "-- Epoch: 21  Batch: 20 --\n",
      "    Loss_i: 0.14968441 Loss_f: 0.15048622 rho -0.00011940516 lambda: 0.01\n",
      "    Loss_i: 0.14968441 Loss_f: 0.15068482 rho -1.7460012e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14968441 Loss_f: 0.15006675 rho -6.789609e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14968441 Loss_f: 0.14966123 rho 4.1246975e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14966123 Loss_f: 0.14970998 rho -8.673589e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14966123 Loss_f: 0.14970998 rho -8.673588e-09 lambda: 10.0\n",
      "    Train error: 21.449371\n",
      "-- Epoch: 21  Batch: 21 --\n",
      "    Loss_i: 0.15187483 Loss_f: 0.15130068 rho 9.825935e-05 lambda: 0.01\n",
      "    Loss_i: 0.15130068 Loss_f: 0.15139991 rho -1.6805585e-05 lambda: 0.01\n",
      "    Loss_i: 0.15130068 Loss_f: 0.15167974 rho -7.4352156e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15130068 Loss_f: 0.15151154 rho -4.2023336e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15130068 Loss_f: 0.15133564 rho -6.9784774e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15130068 Loss_f: 0.15133564 rho -6.978477e-09 lambda: 10.0\n",
      "    Train error: 21.805622\n",
      "-- Epoch: 21  Batch: 22 --\n",
      "    Loss_i: 0.14869587 Loss_f: 0.14967375 rho -0.00014770392 lambda: 0.01\n",
      "    Loss_i: 0.14869587 Loss_f: 0.14978266 rho -2.1002343e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14869587 Loss_f: 0.14899077 rho -5.8626966e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14869587 Loss_f: 0.14862634 rho 1.3862652e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14862634 Loss_f: 0.14864141 rho -3.003548e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14862634 Loss_f: 0.14864142 rho -3.0065184e-09 lambda: 10.0\n",
      "    Train error: 21.211477\n",
      "-- Epoch: 21  Batch: 23 --\n",
      "    Loss_i: 0.15370543 Loss_f: 0.15357018 rho 1.9739859e-05 lambda: 0.01\n",
      "    Loss_i: 0.15357018 Loss_f: 0.15478542 rho -0.00017475768 lambda: 0.01\n",
      "    Loss_i: 0.15357018 Loss_f: 0.15486602 rho -2.4388928e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15357018 Loss_f: 0.15425393 rho -1.3278842e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15357018 Loss_f: 0.15368709 rho -2.277793e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15357018 Loss_f: 0.15368709 rho -2.277793e-08 lambda: 10.0\n",
      "    Train error: 22.071207\n",
      "-- Epoch: 21  Batch: 24 --\n",
      "    Loss_i: 0.15359208 Loss_f: 0.1619735 rho -0.0012695033 lambda: 0.01\n",
      "    Loss_i: 0.15359208 Loss_f: 0.16097054 rho -0.00017107379 lambda: 0.099999994\n",
      "    Loss_i: 0.15359208 Loss_f: 0.15612783 rho -6.208799e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15359208 Loss_f: 0.15339568 rho 4.8359126e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15339568 Loss_f: 0.15369408 rho -7.3483704e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15339568 Loss_f: 0.15369406 rho -7.348003e-08 lambda: 10.0\n",
      "    Train error: 22.270924\n",
      "-- Epoch: 21  Batch: 25 --\n",
      "    Loss_i: 0.15432315 Loss_f: 0.15683773 rho -0.0003883862 lambda: 0.01\n",
      "    Loss_i: 0.15432315 Loss_f: 0.15664577 rho -4.7148507e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15432315 Loss_f: 0.15501791 rho -1.4561142e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15432315 Loss_f: 0.15424342 rho 1.6762618e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15424342 Loss_f: 0.15430927 rho -1.38457334e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15424342 Loss_f: 0.15430927 rho -1.384573e-08 lambda: 10.0\n",
      "    Train error: 22.41301\n",
      "-- Epoch: 21  Batch: 26 --\n",
      "    Loss_i: 0.1502002 Loss_f: 0.15130761 rho -0.00018744206 lambda: 0.01\n",
      "    Loss_i: 0.1502002 Loss_f: 0.1514305 rho -2.43775e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1502002 Loss_f: 0.1506318 rho -8.700243e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1502002 Loss_f: 0.15016693 rho 6.7191763e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15016693 Loss_f: 0.15021896 rho -1.05086375e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15016693 Loss_f: 0.15021896 rho -1.05086375e-08 lambda: 10.0\n",
      "    Train error: 21.594946\n",
      "-- Epoch: 21  Batch: 27 --\n",
      "    Loss_i: 0.14925946 Loss_f: 0.14990844 rho -0.00010130318 lambda: 0.01\n",
      "    Loss_i: 0.14925946 Loss_f: 0.1501201 rho -1.595637e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14925946 Loss_f: 0.14959553 rho -6.3499783e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14925946 Loss_f: 0.14923897 rho 3.878845e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14923897 Loss_f: 0.14928451 rho -8.620565e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14923897 Loss_f: 0.14928451 rho -8.620564e-09 lambda: 10.0\n",
      "    Train error: 21.301064\n",
      "-- Epoch: 21  Batch: 28 --\n",
      "    Loss_i: 0.14825958 Loss_f: 0.1482299 rho 4.2602614e-06 lambda: 0.01\n",
      "    Loss_i: 0.1482299 Loss_f: 0.14964865 rho -0.00020444914 lambda: 0.01\n",
      "    Loss_i: 0.1482299 Loss_f: 0.14976089 rho -2.5904597e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1482299 Loss_f: 0.14911199 rho -1.5189634e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1482299 Loss_f: 0.14839225 rho -2.8006085e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1482299 Loss_f: 0.14839225 rho -2.8006085e-08 lambda: 10.0\n",
      "    Train error: 21.199749\n",
      "-- Epoch: 21  Batch: 29 --\n",
      "    Loss_i: 0.15050407 Loss_f: 0.1873318 rho -0.0059813703 lambda: 0.01\n",
      "    Loss_i: 0.15050407 Loss_f: 0.18294771 rho -0.0007375942 lambda: 0.099999994\n",
      "    Loss_i: 0.15050407 Loss_f: 0.16369495 rho -3.1237832e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15050407 Loss_f: 0.15021725 rho 6.820641e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15021725 Loss_f: 0.15208006 rho -4.4298335e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15021725 Loss_f: 0.15208006 rho -4.4298326e-07 lambda: 10.0\n",
      "    Train error: 21.466848\n",
      "-- Epoch: 21  Batch: 30 --\n",
      "    Loss_i: 0.15113313 Loss_f: 0.17155208 rho -0.002664096 lambda: 0.01\n",
      "    Loss_i: 0.15113313 Loss_f: 0.16926982 rho -0.00042785134 lambda: 0.099999994\n",
      "    Loss_i: 0.15113313 Loss_f: 0.15814754 rho -1.8001972e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15113313 Loss_f: 0.15096045 rho 4.4712607e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15096045 Loss_f: 0.1518982 rho -2.4273803e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15096045 Loss_f: 0.1518982 rho -2.4273803e-07 lambda: 10.0\n",
      "    Train error: 21.46497\n",
      "\n",
      "*** Epoch: 21 Train error: 21.7333065032959  Test error: 21.31571  Time: 706.9151517000009 sec\n",
      "\n",
      "-- Epoch: 22  Batch: 1 --\n",
      "    Loss_i: 0.14771326 Loss_f: 0.15513688 rho -0.001282237 lambda: 0.01\n",
      "    Loss_i: 0.14771326 Loss_f: 0.1544926 rho -0.00014016681 lambda: 0.099999994\n",
      "    Loss_i: 0.14771326 Loss_f: 0.15028954 rho -5.4336574e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14771326 Loss_f: 0.14762345 rho 1.897994e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14762345 Loss_f: 0.14796637 rho -7.246404e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14762345 Loss_f: 0.14796638 rho -7.246718e-08 lambda: 10.0\n",
      "    Train error: 20.779722\n",
      "-- Epoch: 22  Batch: 2 --\n",
      "    Loss_i: 0.14973098 Loss_f: 0.15402304 rho -0.0006548513 lambda: 0.01\n",
      "    Loss_i: 0.14973098 Loss_f: 0.15372401 rho -7.8093675e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14973098 Loss_f: 0.15121622 rho -2.989006e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14973098 Loss_f: 0.1496834 rho 9.603057e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1496834 Loss_f: 0.14987463 rho -3.8597868e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1496834 Loss_f: 0.14987463 rho -3.8597864e-08 lambda: 10.0\n",
      "    Train error: 21.343218\n",
      "-- Epoch: 22  Batch: 3 --\n",
      "    Loss_i: 0.15170282 Loss_f: 0.15468252 rho -0.0004789748 lambda: 0.01\n",
      "    Loss_i: 0.15170282 Loss_f: 0.15462382 rho -5.7148703e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15170282 Loss_f: 0.15282011 rho -2.2344661e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15170282 Loss_f: 0.15166642 rho 7.296534e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15166642 Loss_f: 0.15181535 rho -2.984816e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15166642 Loss_f: 0.15181535 rho -2.984816e-08 lambda: 10.0\n",
      "    Train error: 21.646938\n",
      "-- Epoch: 22  Batch: 4 --\n",
      "    Loss_i: 0.14720869 Loss_f: 0.14745867 rho -3.7260445e-05 lambda: 0.01\n",
      "    Loss_i: 0.14720869 Loss_f: 0.14772056 rho -8.9714895e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14720869 Loss_f: 0.14741151 rho -3.618445e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14720869 Loss_f: 0.14719364 rho 2.6898712e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14719364 Loss_f: 0.14722015 rho -4.737962e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14719364 Loss_f: 0.14722015 rho -4.7379616e-09 lambda: 10.0\n",
      "    Train error: 20.92646\n",
      "-- Epoch: 22  Batch: 5 --\n",
      "    Loss_i: 0.15078622 Loss_f: 0.15035246 rho 6.530276e-05 lambda: 0.01\n",
      "    Loss_i: 0.15035246 Loss_f: 0.15019993 rho 2.2734732e-05 lambda: 0.01\n",
      "    Loss_i: 0.15019993 Loss_f: 0.15033214 rho -1.9508261e-05 lambda: 0.01\n",
      "    Loss_i: 0.15019993 Loss_f: 0.15045767 rho -4.3385003e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15019993 Loss_f: 0.15034036 rho -2.3976514e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15019993 Loss_f: 0.15022613 rho -4.479095e-09 lambda: 9.999999\n",
      "    Train error: 21.52837\n",
      "-- Epoch: 22  Batch: 6 --\n",
      "    Loss_i: 0.1449636 Loss_f: 0.14908099 rho -0.00063056004 lambda: 0.01\n",
      "    Loss_i: 0.1449636 Loss_f: 0.14893688 rho -7.455491e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1449636 Loss_f: 0.14642371 rho -2.8028894e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1449636 Loss_f: 0.1448649 rho 1.8991708e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1448649 Loss_f: 0.14504907 rho -3.543464e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1448649 Loss_f: 0.14504907 rho -3.5434635e-08 lambda: 10.0\n",
      "    Train error: 20.396894\n",
      "-- Epoch: 22  Batch: 7 --\n",
      "    Loss_i: 0.15337637 Loss_f: 0.15504642 rho -0.00028496384 lambda: 0.01\n",
      "    Loss_i: 0.15337637 Loss_f: 0.15525417 rho -3.998067e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15337637 Loss_f: 0.15403727 rho -1.4428824e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15337637 Loss_f: 0.15331525 rho 1.3378833e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15331525 Loss_f: 0.15339203 rho -1.680704e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15331525 Loss_f: 0.15339205 rho -1.6810299e-08 lambda: 10.0\n",
      "    Train error: 21.979136\n",
      "-- Epoch: 22  Batch: 8 --\n",
      "    Loss_i: 0.15358141 Loss_f: 0.15410262 rho -0.00010071989 lambda: 0.01\n",
      "    Loss_i: 0.15358141 Loss_f: 0.15448149 rho -2.101539e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15358141 Loss_f: 0.15392412 rho -8.1719907e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15358141 Loss_f: 0.15355398 rho 6.5553745e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15355398 Loss_f: 0.15359604 rho -1.0052485e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15355398 Loss_f: 0.15359604 rho -1.0052482e-08 lambda: 10.0\n",
      "    Train error: 22.443792\n",
      "-- Epoch: 22  Batch: 9 --\n",
      "    Loss_i: 0.15055482 Loss_f: 0.15156892 rho -0.0001357576 lambda: 0.01\n",
      "    Loss_i: 0.15055482 Loss_f: 0.15174714 rho -2.3217352e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15055482 Loss_f: 0.15099531 rho -8.985975e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15055482 Loss_f: 0.15053172 rho 4.7342468e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15053172 Loss_f: 0.15058777 rho -1.1487113e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15053172 Loss_f: 0.15058777 rho -1.1487109e-08 lambda: 10.0\n",
      "    Train error: 21.65452\n",
      "-- Epoch: 22  Batch: 10 --\n",
      "    Loss_i: 0.14986648 Loss_f: 0.15017718 rho -4.8733218e-05 lambda: 0.01\n",
      "    Loss_i: 0.14986648 Loss_f: 0.1504937 rho -1.2258079e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14986648 Loss_f: 0.15011609 rho -5.001287e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14986648 Loss_f: 0.14984977 rho 3.3553937e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14984977 Loss_f: 0.14988272 rho -6.618627e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14984977 Loss_f: 0.14988272 rho -6.6186256e-09 lambda: 10.0\n",
      "    Train error: 21.492535\n",
      "-- Epoch: 22  Batch: 11 --\n",
      "    Loss_i: 0.14949566 Loss_f: 0.1491737 rho 3.6364832e-05 lambda: 0.01\n",
      "    Loss_i: 0.1491737 Loss_f: 0.14999275 rho -9.08408e-05 lambda: 0.01\n",
      "    Loss_i: 0.1491737 Loss_f: 0.15015066 rho -1.5680296e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1491737 Loss_f: 0.14972404 rho -9.2463665e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1491737 Loss_f: 0.14927126 rho -1.6468277e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1491737 Loss_f: 0.14927125 rho -1.6465762e-08 lambda: 10.0\n",
      "    Train error: 21.19724\n",
      "-- Epoch: 22  Batch: 12 --\n",
      "    Loss_i: 0.15444522 Loss_f: 0.16685677 rho -0.0023691144 lambda: 0.01\n",
      "    Loss_i: 0.15444522 Loss_f: 0.16492453 rho -0.00025499202 lambda: 0.099999994\n",
      "    Loss_i: 0.15444522 Loss_f: 0.15822467 rho -9.45635e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15444522 Loss_f: 0.15422083 rho 5.630027e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15422083 Loss_f: 0.15469694 rho -1.1947762e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15422083 Loss_f: 0.15469694 rho -1.194776e-07 lambda: 10.0\n",
      "    Train error: 22.397848\n",
      "-- Epoch: 22  Batch: 13 --\n",
      "    Loss_i: 0.14568175 Loss_f: 0.14961691 rho -0.0006058181 lambda: 0.01\n",
      "    Loss_i: 0.14568175 Loss_f: 0.14936016 rho -6.519419e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14568175 Loss_f: 0.14698243 rho -2.3406533e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14568175 Loss_f: 0.14561872 rho 1.1360444e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14561872 Loss_f: 0.14577317 rho -2.7839942e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14561872 Loss_f: 0.14577317 rho -2.783994e-08 lambda: 10.0\n",
      "    Train error: 20.683111\n",
      "-- Epoch: 22  Batch: 14 --\n",
      "    Loss_i: 0.14998008 Loss_f: 0.15034486 rho -5.667587e-05 lambda: 0.01\n",
      "    Loss_i: 0.14998008 Loss_f: 0.15061723 rho -1.3318898e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14998008 Loss_f: 0.1502217 rho -5.2316085e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14998008 Loss_f: 0.1499615 rho 4.03777e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1499615 Loss_f: 0.1499914 rho -6.4954504e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1499615 Loss_f: 0.1499914 rho -6.4954495e-09 lambda: 10.0\n",
      "    Train error: 21.504622\n",
      "-- Epoch: 22  Batch: 15 --\n",
      "    Loss_i: 0.14819716 Loss_f: 0.14822063 rho -3.239471e-06 lambda: 0.01\n",
      "    Loss_i: 0.14819716 Loss_f: 0.1485303 rho -5.6817817e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14819716 Loss_f: 0.14831878 rho -2.1243301e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14819716 Loss_f: 0.14818276 rho 2.5202973e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14818276 Loss_f: 0.14819479 rho -2.1054052e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14818276 Loss_f: 0.14819479 rho -2.1054052e-09 lambda: 10.0\n",
      "    Train error: 20.853516\n",
      "-- Epoch: 22  Batch: 16 --\n",
      "    Loss_i: 0.15044306 Loss_f: 0.15038633 rho 9.552818e-06 lambda: 0.01\n",
      "    Loss_i: 0.15038633 Loss_f: 0.15183303 rho -0.00024613313 lambda: 0.01\n",
      "    Loss_i: 0.15038633 Loss_f: 0.15196 rho -3.109588e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15038633 Loss_f: 0.15132943 rho -1.8941472e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15038633 Loss_f: 0.15056585 rho -3.6113608e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15038633 Loss_f: 0.15056585 rho -3.6113608e-08 lambda: 10.0\n",
      "    Train error: 21.829887\n",
      "-- Epoch: 22  Batch: 17 --\n",
      "    Loss_i: 0.14852099 Loss_f: 0.19614856 rho -0.0070174406 lambda: 0.01\n",
      "    Loss_i: 0.14852099 Loss_f: 0.19051239 rho -0.00084024004 lambda: 0.099999994\n",
      "    Loss_i: 0.14852099 Loss_f: 0.16561775 rho -3.548074e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14852099 Loss_f: 0.14820994 rho 6.479189e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14820994 Loss_f: 0.15065539 rho -5.093822e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14820994 Loss_f: 0.15065539 rho -5.093821e-07 lambda: 10.0\n",
      "    Train error: 20.903051\n",
      "-- Epoch: 22  Batch: 18 --\n",
      "    Loss_i: 0.15066023 Loss_f: 0.17176636 rho -0.0024982188 lambda: 0.01\n",
      "    Loss_i: 0.15066023 Loss_f: 0.16932814 rho -0.00039189737 lambda: 0.099999994\n",
      "    Loss_i: 0.15066023 Loss_f: 0.15796597 rho -1.6622971e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.15066023 Loss_f: 0.1505058 rho 3.543632e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1505058 Loss_f: 0.15151036 rho -2.3057655e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1505058 Loss_f: 0.15151036 rho -2.3057652e-07 lambda: 10.0\n",
      "    Train error: 21.51794\n",
      "-- Epoch: 22  Batch: 19 --\n",
      "    Loss_i: 0.15225132 Loss_f: 0.16276231 rho -0.0019062859 lambda: 0.01\n",
      "    Loss_i: 0.15225132 Loss_f: 0.16174392 rho -0.00021384873 lambda: 0.099999994\n",
      "    Loss_i: 0.15225132 Loss_f: 0.15594037 rho -8.516912e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15225132 Loss_f: 0.15217213 rho 1.8326872e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15217213 Loss_f: 0.15267782 rho -1.1697103e-07 lambda: 9.999999\n",
      "    Loss_i: 0.15217213 Loss_f: 0.15267782 rho -1.16971e-07 lambda: 10.0\n",
      "    Train error: 21.89187\n",
      "-- Epoch: 22  Batch: 20 --\n",
      "    Loss_i: 0.14853464 Loss_f: 0.15425965 rho -0.00081875734 lambda: 0.01\n",
      "    Loss_i: 0.14853464 Loss_f: 0.15379266 rho -9.9348064e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14853464 Loss_f: 0.15061219 rho -4.055695e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14853464 Loss_f: 0.14849453 rho 7.856927e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14849453 Loss_f: 0.14878757 rho -5.738749e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14849453 Loss_f: 0.14878757 rho -5.7387478e-08 lambda: 10.0\n",
      "    Train error: 21.162846\n",
      "-- Epoch: 22  Batch: 21 --\n",
      "    Loss_i: 0.15041327 Loss_f: 0.15106784 rho -8.957336e-05 lambda: 0.01\n",
      "    Loss_i: 0.15041327 Loss_f: 0.15130556 rho -1.5626927e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15041327 Loss_f: 0.15076116 rho -6.2680084e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15041327 Loss_f: 0.15039566 rho 3.1826344e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15039566 Loss_f: 0.15044054 rho -8.109617e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15039566 Loss_f: 0.15044054 rho -8.109615e-09 lambda: 10.0\n",
      "    Train error: 21.562325\n",
      "-- Epoch: 22  Batch: 22 --\n",
      "    Loss_i: 0.14761701 Loss_f: 0.14725198 rho 5.1247967e-05 lambda: 0.01\n",
      "    Loss_i: 0.14725198 Loss_f: 0.14726472 rho -1.7834693e-06 lambda: 0.01\n",
      "    Loss_i: 0.14725198 Loss_f: 0.14749621 rho -3.855599e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14725198 Loss_f: 0.1474059 rho -2.4612467e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14725198 Loss_f: 0.14728029 rho -4.5332826e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14725198 Loss_f: 0.14728029 rho -4.533282e-09 lambda: 10.0\n",
      "    Train error: 20.880804\n",
      "-- Epoch: 22  Batch: 23 --\n",
      "    Loss_i: 0.15261582 Loss_f: 0.15657486 rho -0.0006627535 lambda: 0.01\n",
      "    Loss_i: 0.15261582 Loss_f: 0.15636896 rho -7.607369e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15261582 Loss_f: 0.1540344 rho -2.9373125e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15261582 Loss_f: 0.15256962 rho 9.585419e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15256962 Loss_f: 0.15275629 rho -3.87352e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15256962 Loss_f: 0.15275629 rho -3.87352e-08 lambda: 10.0\n",
      "    Train error: 21.825478\n",
      "-- Epoch: 22  Batch: 24 --\n",
      "    Loss_i: 0.15230353 Loss_f: 0.15305395 rho -9.908361e-05 lambda: 0.01\n",
      "    Loss_i: 0.15230353 Loss_f: 0.15323475 rho -1.7450167e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15230353 Loss_f: 0.15264663 rho -6.7106936e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15230353 Loss_f: 0.15228517 rho 3.6064747e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15228517 Loss_f: 0.15232779 rho -8.3736245e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15228517 Loss_f: 0.15232776 rho -8.367768e-09 lambda: 10.0\n",
      "    Train error: 21.952446\n",
      "-- Epoch: 22  Batch: 25 --\n",
      "    Loss_i: 0.1532092 Loss_f: 0.15322812 rho -2.6319115e-06 lambda: 0.01\n",
      "    Loss_i: 0.1532092 Loss_f: 0.15351608 rho -5.241154e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1532092 Loss_f: 0.15333934 rho -2.274547e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1532092 Loss_f: 0.15319952 rho 1.6941069e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15319952 Loss_f: 0.15321608 rho -2.9000424e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15319952 Loss_f: 0.1532161 rho -2.9026523e-09 lambda: 10.0\n",
      "    Train error: 22.08123\n",
      "-- Epoch: 22  Batch: 26 --\n",
      "    Loss_i: 0.14908065 Loss_f: 0.14906865 rho 1.6943414e-06 lambda: 0.01\n",
      "    Loss_i: 0.14906865 Loss_f: 0.15008922 rho -0.00014478283 lambda: 0.01\n",
      "    Loss_i: 0.14906865 Loss_f: 0.15021741 rho -1.873084e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14906865 Loss_f: 0.14976045 rho -1.1451026e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14906865 Loss_f: 0.14920059 rho -2.1871648e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14906865 Loss_f: 0.14920059 rho -2.1871648e-08 lambda: 10.0\n",
      "    Train error: 21.391695\n",
      "-- Epoch: 22  Batch: 27 --\n",
      "    Loss_i: 0.14852569 Loss_f: 0.1784428 rho -0.004787722 lambda: 0.01\n",
      "    Loss_i: 0.14852569 Loss_f: 0.17499328 rho -0.00055695506 lambda: 0.099999994\n",
      "    Loss_i: 0.14852569 Loss_f: 0.15913874 rho -2.3059114e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14852569 Loss_f: 0.14827946 rho 5.3672505e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14827946 Loss_f: 0.14978983 rho -3.290124e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14827946 Loss_f: 0.14978983 rho -3.2901238e-07 lambda: 10.0\n",
      "    Train error: 21.023813\n",
      "-- Epoch: 22  Batch: 28 --\n",
      "    Loss_i: 0.14709447 Loss_f: 0.15914108 rho -0.0019123319 lambda: 0.01\n",
      "    Loss_i: 0.14709447 Loss_f: 0.15796213 rho -0.00020120306 lambda: 0.099999994\n",
      "    Loss_i: 0.14709447 Loss_f: 0.15139973 rho -8.1055e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14709447 Loss_f: 0.1470016 rho 1.7515912e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1470016 Loss_f: 0.14760244 rho -1.13293076e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1470016 Loss_f: 0.14760244 rho -1.1329306e-07 lambda: 10.0\n",
      "    Train error: 20.81606\n",
      "-- Epoch: 22  Batch: 29 --\n",
      "    Loss_i: 0.14912945 Loss_f: 0.15516287 rho -0.0009076632 lambda: 0.01\n",
      "    Loss_i: 0.14912945 Loss_f: 0.15471195 rho -0.00010690293 lambda: 0.099999994\n",
      "    Loss_i: 0.14912945 Loss_f: 0.1513373 rho -4.3465766e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14912945 Loss_f: 0.14908463 rho 8.849053e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14908463 Loss_f: 0.14939515 rho -6.1290464e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14908463 Loss_f: 0.14939515 rho -6.129046e-08 lambda: 10.0\n",
      "    Train error: 21.227016\n",
      "-- Epoch: 22  Batch: 30 --\n",
      "    Loss_i: 0.15004997 Loss_f: 0.15436159 rho -0.0007462722 lambda: 0.01\n",
      "    Loss_i: 0.15004997 Loss_f: 0.15429199 rho -8.977576e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15004997 Loss_f: 0.15176816 rho -3.7191262e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15004997 Loss_f: 0.15001482 rho 7.626188e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15001482 Loss_f: 0.1502603 rho -5.325223e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15001482 Loss_f: 0.1502603 rho -5.325223e-08 lambda: 10.0\n",
      "    Train error: 21.187757\n",
      "\n",
      "*** Epoch: 22 Train error: 21.40273806254069  Test error: 21.01794  Time: 706.9408811999983 sec\n",
      "\n",
      "-- Epoch: 23  Batch: 1 --\n",
      "    Loss_i: 0.1466659 Loss_f: 0.14941664 rho -0.00042516473 lambda: 0.01\n",
      "    Loss_i: 0.1466659 Loss_f: 0.14935601 rho -4.697821e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1466659 Loss_f: 0.14767885 rho -1.7922201e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1466659 Loss_f: 0.14663726 rho 5.073972e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14663726 Loss_f: 0.14676872 rho -2.3287827e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14663726 Loss_f: 0.14676872 rho -2.3287825e-08 lambda: 10.0\n",
      "    Train error: 20.520784\n",
      "-- Epoch: 23  Batch: 2 --\n",
      "    Loss_i: 0.14877254 Loss_f: 0.15051907 rho -0.00025338947 lambda: 0.01\n",
      "    Loss_i: 0.14877254 Loss_f: 0.15062527 rho -3.222562e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14877254 Loss_f: 0.14951418 rho -1.3161601e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14877254 Loss_f: 0.14874816 rho 4.335087e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14874816 Loss_f: 0.14884976 rho -1.8065958e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14874816 Loss_f: 0.14884976 rho -1.8065954e-08 lambda: 10.0\n",
      "    Train error: 21.094913\n",
      "-- Epoch: 23  Batch: 3 --\n",
      "    Loss_i: 0.15061054 Loss_f: 0.15223888 rho -0.00023499002 lambda: 0.01\n",
      "    Loss_i: 0.15061054 Loss_f: 0.15239763 rho -3.4315693e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15061054 Loss_f: 0.1513241 rho -1.4170096e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15061054 Loss_f: 0.15058763 rho 4.563789e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15058763 Loss_f: 0.1506856 rho -1.9516099e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15058763 Loss_f: 0.1506856 rho -1.9516097e-08 lambda: 10.0\n",
      "    Train error: 21.367046\n",
      "-- Epoch: 23  Batch: 4 --\n",
      "    Loss_i: 0.14619793 Loss_f: 0.14621603 rho -2.5856884e-06 lambda: 0.01\n",
      "    Loss_i: 0.14619793 Loss_f: 0.14648108 rho -4.6010023e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14619793 Loss_f: 0.1463196 rho -2.004629e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14619793 Loss_f: 0.14619194 rho 9.883505e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14619194 Loss_f: 0.14620799 rho -2.6478548e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14619194 Loss_f: 0.14620799 rho -2.6478544e-09 lambda: 10.0\n",
      "    Train error: 20.66795\n",
      "-- Epoch: 23  Batch: 5 --\n",
      "    Loss_i: 0.14958863 Loss_f: 0.14910075 rho 7.6795244e-05 lambda: 0.01\n",
      "    Loss_i: 0.14910075 Loss_f: 0.1488808 rho 3.4358916e-05 lambda: 0.01\n",
      "    Loss_i: 0.1488808 Loss_f: 0.14893232 rho -7.9872625e-06 lambda: 0.01\n",
      "    Loss_i: 0.1488808 Loss_f: 0.14904438 rho -2.910339e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1488808 Loss_f: 0.14895843 rho -1.4019182e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1488808 Loss_f: 0.14889412 rho -2.4092075e-09 lambda: 9.999999\n",
      "    Train error: 21.220816\n",
      "-- Epoch: 23  Batch: 6 --\n",
      "    Loss_i: 0.14391735 Loss_f: 0.14350833 rho 6.0668426e-05 lambda: 0.01\n",
      "    Loss_i: 0.14350833 Loss_f: 0.14505419 rho -0.0002258189 lambda: 0.01\n",
      "    Loss_i: 0.14350833 Loss_f: 0.14509712 rho -2.7433887e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14350833 Loss_f: 0.14430892 rho -1.4080298e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14350833 Loss_f: 0.1436423 rho -2.3606589e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14350833 Loss_f: 0.1436423 rho -2.3606589e-08 lambda: 10.0\n",
      "    Train error: 19.974493\n",
      "-- Epoch: 23  Batch: 7 --\n",
      "    Loss_i: 0.15227142 Loss_f: 0.16008414 rho -0.0015187406 lambda: 0.01\n",
      "    Loss_i: 0.15227142 Loss_f: 0.1591358 rho -0.00016506773 lambda: 0.099999994\n",
      "    Loss_i: 0.15227142 Loss_f: 0.1544711 rho -5.4179864e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15227142 Loss_f: 0.15206084 rho 5.1994558e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15206084 Loss_f: 0.15228568 rho -5.5528805e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15206084 Loss_f: 0.15228568 rho -5.552879e-08 lambda: 10.0\n",
      "    Train error: 21.733744\n",
      "-- Epoch: 23  Batch: 8 --\n",
      "    Loss_i: 0.15241861 Loss_f: 0.15310934 rho -0.0001113773 lambda: 0.01\n",
      "    Loss_i: 0.15241861 Loss_f: 0.1533902 rho -2.272296e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15241861 Loss_f: 0.15267093 rho -6.179504e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15241861 Loss_f: 0.15235053 rho 1.6753061e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15235053 Loss_f: 0.15236355 rho -3.2045029e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15235053 Loss_f: 0.15236355 rho -3.2045024e-09 lambda: 10.0\n",
      "    Train error: 22.086966\n",
      "-- Epoch: 23  Batch: 9 --\n",
      "    Loss_i: 0.14935918 Loss_f: 0.14908244 rho 5.135731e-05 lambda: 0.01\n",
      "    Loss_i: 0.14908244 Loss_f: 0.15081283 rho -0.00031815175 lambda: 0.01\n",
      "    Loss_i: 0.14908244 Loss_f: 0.15075128 rho -3.589475e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14908244 Loss_f: 0.14986908 rho -1.7212167e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14908244 Loss_f: 0.14920637 rho -2.7163972e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14908244 Loss_f: 0.14920637 rho -2.7163972e-08 lambda: 10.0\n",
      "    Train error: 21.310844\n",
      "-- Epoch: 23  Batch: 10 --\n",
      "    Loss_i: 0.14860685 Loss_f: 0.1497621 rho -0.00014493383 lambda: 0.01\n",
      "    Loss_i: 0.14860685 Loss_f: 0.14965896 rho -1.9767833e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14860685 Loss_f: 0.14855425 rho 1.0400627e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14855425 Loss_f: 0.15066947 rho -4.1823464e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14855425 Loss_f: 0.14883363 rho -5.5530244e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14855425 Loss_f: 0.14883363 rho -5.5530236e-08 lambda: 10.0\n",
      "    Train error: 21.185379\n",
      "-- Epoch: 23  Batch: 11 --\n",
      "    Loss_i: 0.14815761 Loss_f: 0.151826 rho -0.00056402636 lambda: 0.01\n",
      "    Loss_i: 0.14815761 Loss_f: 0.15141873 rho -5.689962e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14815761 Loss_f: 0.14882395 rho -1.1784999e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14815761 Loss_f: 0.14790988 rho 4.387451e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14790988 Loss_f: 0.14788239 rho 4.869027e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14788239 Loss_f: 0.1480759 rho -3.427009e-08 lambda: 9.999999\n",
      "    Train error: 20.959469\n",
      "-- Epoch: 23  Batch: 12 --\n",
      "    Loss_i: 0.15282038 Loss_f: 0.1534146 rho -9.6488795e-05 lambda: 0.01\n",
      "    Loss_i: 0.15282038 Loss_f: 0.15359747 rho -1.5565754e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15282038 Loss_f: 0.15301888 rho -4.0711578e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15282038 Loss_f: 0.15276888 rho 1.0587534e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15276888 Loss_f: 0.15277344 rho -9.374075e-10 lambda: 9.999999\n",
      "    Loss_i: 0.15276888 Loss_f: 0.15277344 rho -9.374075e-10 lambda: 10.0\n",
      "    Train error: 21.999102\n",
      "-- Epoch: 23  Batch: 13 --\n",
      "    Loss_i: 0.14427295 Loss_f: 0.14410493 rho 2.0365987e-05 lambda: 0.01\n",
      "    Loss_i: 0.14410493 Loss_f: 0.14465971 rho -6.6066175e-05 lambda: 0.01\n",
      "    Loss_i: 0.14410493 Loss_f: 0.14482196 rho -1.0455354e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14410493 Loss_f: 0.14453371 rho -6.3958333e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14410493 Loss_f: 0.14418507 rho -1.1981221e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14410493 Loss_f: 0.14418507 rho -1.19812205e-08 lambda: 10.0\n",
      "    Train error: 20.240786\n",
      "-- Epoch: 23  Batch: 14 --\n",
      "    Loss_i: 0.14876384 Loss_f: 0.16319655 rho -0.0024106354 lambda: 0.01\n",
      "    Loss_i: 0.14876384 Loss_f: 0.16175479 rho -0.00030248024 lambda: 0.099999994\n",
      "    Loss_i: 0.14876384 Loss_f: 0.15397444 rho -1.2629996e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14876384 Loss_f: 0.1486472 rho 2.8386738e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1486472 Loss_f: 0.14939344 rho -1.8159295e-07 lambda: 9.999999\n",
      "    Loss_i: 0.1486472 Loss_f: 0.14939344 rho -1.8159295e-07 lambda: 10.0\n",
      "    Train error: 21.199379\n",
      "-- Epoch: 23  Batch: 15 --\n",
      "    Loss_i: 0.14676397 Loss_f: 0.15358856 rho -0.0008018014 lambda: 0.01\n",
      "    Loss_i: 0.14676397 Loss_f: 0.15296206 rho -0.00010525897 lambda: 0.099999994\n",
      "    Loss_i: 0.14676397 Loss_f: 0.14908977 rho -4.133946e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14676397 Loss_f: 0.14670797 rho 9.999962e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14670797 Loss_f: 0.14701322 rho -5.4498692e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14670797 Loss_f: 0.14701322 rho -5.4498692e-08 lambda: 10.0\n",
      "    Train error: 20.550362\n",
      "-- Epoch: 23  Batch: 16 --\n",
      "    Loss_i: 0.14899345 Loss_f: 0.15131603 rho -0.0004008803 lambda: 0.01\n",
      "    Loss_i: 0.14899345 Loss_f: 0.15135716 rho -5.103105e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14899345 Loss_f: 0.14988676 rho -1.9782183e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14899345 Loss_f: 0.14895557 rho 8.409816e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14895557 Loss_f: 0.14907019 rho -2.5449713e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14895557 Loss_f: 0.14907019 rho -2.5449713e-08 lambda: 10.0\n",
      "    Train error: 21.32233\n",
      "-- Epoch: 23  Batch: 17 --\n",
      "    Loss_i: 0.14672309 Loss_f: 0.14712714 rho -5.4922773e-05 lambda: 0.01\n",
      "    Loss_i: 0.14672309 Loss_f: 0.14736606 rho -1.0675344e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14672309 Loss_f: 0.14698495 rho -4.446125e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14672309 Loss_f: 0.14671181 rho 1.919624e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14671181 Loss_f: 0.14674771 rho -6.1081904e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14671181 Loss_f: 0.14674772 rho -6.1107257e-09 lambda: 10.0\n",
      "    Train error: 20.554031\n",
      "-- Epoch: 23  Batch: 18 --\n",
      "    Loss_i: 0.14926295 Loss_f: 0.1493393 rho -1.0668559e-05 lambda: 0.01\n",
      "    Loss_i: 0.14926295 Loss_f: 0.14962825 rho -6.144694e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14926295 Loss_f: 0.14941227 rho -2.5640347e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14926295 Loss_f: 0.14925338 rho 1.6460856e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14925338 Loss_f: 0.14927171 rho -3.153931e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14925338 Loss_f: 0.14927171 rho -3.153931e-09 lambda: 10.0\n",
      "    Train error: 21.14905\n",
      "-- Epoch: 23  Batch: 19 --\n",
      "    Loss_i: 0.1506586 Loss_f: 0.1509783 rho -4.5892983e-05 lambda: 0.01\n",
      "    Loss_i: 0.1506586 Loss_f: 0.1512415 rho -1.0537166e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1506586 Loss_f: 0.1508914 rho -4.3203747e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1506586 Loss_f: 0.15064617 rho 2.3124933e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15064617 Loss_f: 0.15067668 rho -5.6782965e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15064617 Loss_f: 0.15067668 rho -5.6782965e-09 lambda: 10.0\n",
      "    Train error: 21.45093\n",
      "-- Epoch: 23  Batch: 20 --\n",
      "    Loss_i: 0.1471885 Loss_f: 0.14745586 rho -3.550741e-05 lambda: 0.01\n",
      "    Loss_i: 0.1471885 Loss_f: 0.14767486 rho -7.80674e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1471885 Loss_f: 0.1473847 rho -3.2164377e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1471885 Loss_f: 0.14717937 rho 1.5006351e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14717937 Loss_f: 0.14720546 rho -4.286294e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14717937 Loss_f: 0.14720546 rho -4.2862935e-09 lambda: 10.0\n",
      "    Train error: 20.804241\n",
      "-- Epoch: 23  Batch: 21 --\n",
      "    Loss_i: 0.1492491 Loss_f: 0.14870945 rho 8.508876e-05 lambda: 0.01\n",
      "    Loss_i: 0.14870945 Loss_f: 0.1485321 rho 2.7787768e-05 lambda: 0.01\n",
      "    Loss_i: 0.1485321 Loss_f: 0.14872278 rho -2.9685621e-05 lambda: 0.01\n",
      "    Loss_i: 0.1485321 Loss_f: 0.1488448 rho -5.5806768e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1485321 Loss_f: 0.14871763 rho -3.3603743e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1485321 Loss_f: 0.1485698 rho -6.8410277e-09 lambda: 9.999999\n",
      "    Train error: 21.140335\n",
      "-- Epoch: 23  Batch: 22 --\n",
      "    Loss_i: 0.14643417 Loss_f: 0.14908902 rho -0.00037134805 lambda: 0.01\n",
      "    Loss_i: 0.14643417 Loss_f: 0.14893907 rho -4.7911e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14643417 Loss_f: 0.14700335 rho -1.130191e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14643417 Loss_f: 0.14625704 rho 3.53065e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14625704 Loss_f: 0.14625409 rho 5.880707e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14625409 Loss_f: 0.14642516 rho -3.409477e-08 lambda: 9.999999\n",
      "    Train error: 20.5934\n",
      "-- Epoch: 23  Batch: 23 --\n",
      "    Loss_i: 0.15134445 Loss_f: 0.15248187 rho -0.00018173245 lambda: 0.01\n",
      "    Loss_i: 0.15134445 Loss_f: 0.15262789 rho -2.4155057e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15134445 Loss_f: 0.15172762 rho -7.3421035e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15134445 Loss_f: 0.15128323 rho 1.1750836e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15128323 Loss_f: 0.15130953 rho -5.048752e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15128323 Loss_f: 0.15130953 rho -5.0487516e-09 lambda: 10.0\n",
      "    Train error: 21.52981\n",
      "-- Epoch: 23  Batch: 24 --\n",
      "    Loss_i: 0.15097587 Loss_f: 0.15048856 rho 7.1934206e-05 lambda: 0.01\n",
      "    Loss_i: 0.15048856 Loss_f: 0.15078472 rho -4.3470838e-05 lambda: 0.01\n",
      "    Loss_i: 0.15048856 Loss_f: 0.15094155 rho -8.509398e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.15048856 Loss_f: 0.15072457 rho -4.5611742e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.15048856 Loss_f: 0.15052798 rho -7.641712e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15048856 Loss_f: 0.15052798 rho -7.641711e-09 lambda: 10.0\n",
      "    Train error: 21.485596\n",
      "-- Epoch: 23  Batch: 25 --\n",
      "    Loss_i: 0.1516145 Loss_f: 0.15171485 rho -1.5016587e-05 lambda: 0.01\n",
      "    Loss_i: 0.1516145 Loss_f: 0.15191504 rho -5.312998e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1516145 Loss_f: 0.15160954 rho 8.933983e-09 lambda: 0.99999994\n",
      "    Loss_i: 0.15160954 Loss_f: 0.15234584 rho -1.3251994e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15160954 Loss_f: 0.15171112 rho -1.8318094e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15160954 Loss_f: 0.15171112 rho -1.8318092e-08 lambda: 10.0\n",
      "    Train error: 21.700583\n",
      "-- Epoch: 23  Batch: 26 --\n",
      "    Loss_i: 0.1476392 Loss_f: 0.15464824 rho -0.000788236 lambda: 0.01\n",
      "    Loss_i: 0.1476392 Loss_f: 0.15413688 rho -0.000116252835 lambda: 0.099999994\n",
      "    Loss_i: 0.1476392 Loss_f: 0.15017101 rho -4.814256e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1476392 Loss_f: 0.14756323 rho 1.4536304e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14756323 Loss_f: 0.1479109 rho -6.6515916e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14756323 Loss_f: 0.1479109 rho -6.651591e-08 lambda: 10.0\n",
      "    Train error: 20.922495\n",
      "-- Epoch: 23  Batch: 27 --\n",
      "    Loss_i: 0.14689682 Loss_f: 0.14951725 rho -0.00040293444 lambda: 0.01\n",
      "    Loss_i: 0.14689682 Loss_f: 0.14950418 rho -4.8223465e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14689682 Loss_f: 0.14786874 rho -1.8347764e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14689682 Loss_f: 0.1468536 rho 8.177553e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1468536 Loss_f: 0.14697632 rho -2.3214414e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1468536 Loss_f: 0.14697632 rho -2.3214414e-08 lambda: 10.0\n",
      "    Train error: 20.677095\n",
      "-- Epoch: 23  Batch: 28 --\n",
      "    Loss_i: 0.14565988 Loss_f: 0.14594343 rho -3.5199722e-05 lambda: 0.01\n",
      "    Loss_i: 0.14565988 Loss_f: 0.14621821 rho -8.607323e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14565988 Loss_f: 0.1458809 rho -3.4916334e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14565988 Loss_f: 0.14564711 rho 2.0224957e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14564711 Loss_f: 0.14567508 rho -4.4295465e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14564711 Loss_f: 0.14567508 rho -4.4295465e-09 lambda: 10.0\n",
      "    Train error: 20.41405\n",
      "-- Epoch: 23  Batch: 29 --\n",
      "    Loss_i: 0.14771745 Loss_f: 0.14783558 rho -1.5614482e-05 lambda: 0.01\n",
      "    Loss_i: 0.14771745 Loss_f: 0.14811026 rho -6.3612583e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14771745 Loss_f: 0.14787926 rho -2.6807996e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14771745 Loss_f: 0.14770699 rho 1.7370548e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14770699 Loss_f: 0.14772752 rho -3.4096603e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14770699 Loss_f: 0.14772752 rho -3.40966e-09 lambda: 10.0\n",
      "    Train error: 20.840153\n",
      "-- Epoch: 23  Batch: 30 --\n",
      "    Loss_i: 0.14852849 Loss_f: 0.14894952 rho -5.893623e-05 lambda: 0.01\n",
      "    Loss_i: 0.14852849 Loss_f: 0.14931221 rho -1.4405357e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14852849 Loss_f: 0.14885615 rho -6.2172694e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14852849 Loss_f: 0.14851235 rho 3.0720526e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14851235 Loss_f: 0.1485569 rho -8.481544e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14851235 Loss_f: 0.1485569 rho -8.481543e-09 lambda: 10.0\n",
      "    Train error: 20.78153\n",
      "\n",
      "*** Epoch: 23 Train error: 21.049255307515462  Test error: 20.615973  Time: 709.4761636000003 sec\n",
      "\n",
      "-- Epoch: 24  Batch: 1 --\n",
      "    Loss_i: 0.14517747 Loss_f: 0.14553387 rho -4.1027615e-05 lambda: 0.01\n",
      "    Loss_i: 0.14517747 Loss_f: 0.14577414 rho -8.770202e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14517747 Loss_f: 0.14541711 rho -3.622659e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14517747 Loss_f: 0.14516367 rho 2.0918798e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14516367 Loss_f: 0.14519565 rho -4.8478497e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14516367 Loss_f: 0.14519565 rho -4.8478492e-09 lambda: 10.0\n",
      "    Train error: 20.145657\n",
      "-- Epoch: 24  Batch: 2 --\n",
      "    Loss_i: 0.14735226 Loss_f: 0.14797167 rho -7.517702e-05 lambda: 0.01\n",
      "    Loss_i: 0.14735226 Loss_f: 0.14821146 rho -1.3676445e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14735226 Loss_f: 0.147718 rho -6.008803e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14735226 Loss_f: 0.14733945 rho 2.1122235e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14733945 Loss_f: 0.1473927 rho -8.77836e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14733945 Loss_f: 0.1473927 rho -8.77836e-09 lambda: 10.0\n",
      "    Train error: 20.724482\n",
      "-- Epoch: 24  Batch: 3 --\n",
      "    Loss_i: 0.14920232 Loss_f: 0.15047885 rho -0.00018243195 lambda: 0.01\n",
      "    Loss_i: 0.14920232 Loss_f: 0.15068792 rho -2.5488325e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14920232 Loss_f: 0.14980276 rho -1.0512538e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14920232 Loss_f: 0.14918214 rho 3.5396857e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14918214 Loss_f: 0.14926508 rho -1.4549467e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14918214 Loss_f: 0.14926508 rho -1.4549466e-08 lambda: 10.0\n",
      "    Train error: 20.996855\n",
      "-- Epoch: 24  Batch: 4 --\n",
      "    Loss_i: 0.1448225 Loss_f: 0.1447835 rho 5.2953387e-06 lambda: 0.01\n",
      "    Loss_i: 0.1447835 Loss_f: 0.14583728 rho -0.00014391841 lambda: 0.01\n",
      "    Loss_i: 0.1447835 Loss_f: 0.14598145 rho -1.8530807e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1447835 Loss_f: 0.14551376 rho -1.1448091e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1447835 Loss_f: 0.14492431 rho -2.2105027e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1447835 Loss_f: 0.14492431 rho -2.2105024e-08 lambda: 10.0\n",
      "    Train error: 20.411737\n",
      "-- Epoch: 24  Batch: 5 --\n",
      "    Loss_i: 0.14816873 Loss_f: 0.18297435 rho -0.004114621 lambda: 0.01\n",
      "    Loss_i: 0.14816873 Loss_f: 0.1786651 rho -0.0005077676 lambda: 0.099999994\n",
      "    Loss_i: 0.14816873 Loss_f: 0.16017535 rho -2.0842419e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14816873 Loss_f: 0.14800662 rho 2.8261132e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14800662 Loss_f: 0.14969377 rho -2.9414255e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14800662 Loss_f: 0.14969377 rho -2.9414255e-07 lambda: 10.0\n",
      "    Train error: 20.940136\n",
      "-- Epoch: 24  Batch: 6 --\n",
      "    Loss_i: 0.14252974 Loss_f: 0.15744518 rho -0.0020836655 lambda: 0.01\n",
      "    Loss_i: 0.14252974 Loss_f: 0.15591945 rho -0.00022776656 lambda: 0.099999994\n",
      "    Loss_i: 0.14252974 Loss_f: 0.14769925 rho -8.989294e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14252974 Loss_f: 0.14241317 rho 2.0315952e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14241317 Loss_f: 0.14311358 rho -1.2205888e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14241317 Loss_f: 0.14311358 rho -1.2205885e-07 lambda: 10.0\n",
      "    Train error: 19.793434\n",
      "-- Epoch: 24  Batch: 7 --\n",
      "    Loss_i: 0.15083723 Loss_f: 0.1570139 rho -0.00058041106 lambda: 0.01\n",
      "    Loss_i: 0.15083723 Loss_f: 0.15660816 rho -0.00010099135 lambda: 0.099999994\n",
      "    Loss_i: 0.15083723 Loss_f: 0.15302369 rho -4.1873986e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15083723 Loss_f: 0.15078144 rho 1.0786395e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15078144 Loss_f: 0.15107225 rho -5.620537e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15078144 Loss_f: 0.15107225 rho -5.6205366e-08 lambda: 10.0\n",
      "    Train error: 21.360453\n",
      "-- Epoch: 24  Batch: 8 --\n",
      "    Loss_i: 0.15115577 Loss_f: 0.15315507 rho -0.0003217108 lambda: 0.01\n",
      "    Loss_i: 0.15115577 Loss_f: 0.15329303 rho -4.344128e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15115577 Loss_f: 0.15199311 rho -1.7479538e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15115577 Loss_f: 0.15112449 rho 6.5468977e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15112449 Loss_f: 0.15123779 rho -2.371367e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15112449 Loss_f: 0.15123779 rho -2.3713667e-08 lambda: 10.0\n",
      "    Train error: 21.83746\n",
      "-- Epoch: 24  Batch: 9 --\n",
      "    Loss_i: 0.14805418 Loss_f: 0.14965522 rho -0.00020508241 lambda: 0.01\n",
      "    Loss_i: 0.14805418 Loss_f: 0.14971887 rho -2.9064144e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14805418 Loss_f: 0.14870997 rho -1.1880817e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14805418 Loss_f: 0.14803746 rho 3.0404412e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14803746 Loss_f: 0.14812864 rho -1.6579135e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14803746 Loss_f: 0.14812864 rho -1.6579133e-08 lambda: 10.0\n",
      "    Train error: 21.029669\n",
      "-- Epoch: 24  Batch: 10 --\n",
      "    Loss_i: 0.14749429 Loss_f: 0.14794643 rho -6.477283e-05 lambda: 0.01\n",
      "    Loss_i: 0.14749429 Loss_f: 0.14818534 rho -1.1815203e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14749429 Loss_f: 0.14776307 rho -4.6862044e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14749429 Loss_f: 0.14748205 rho 2.1371436e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14748205 Loss_f: 0.14751646 rho -6.010553e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14748205 Loss_f: 0.14751646 rho -6.010553e-09 lambda: 10.0\n",
      "    Train error: 20.89067\n",
      "-- Epoch: 24  Batch: 11 --\n",
      "    Loss_i: 0.14715463 Loss_f: 0.1467471 rho 5.527927e-05 lambda: 0.01\n",
      "    Loss_i: 0.1467471 Loss_f: 0.1467841 rho -4.9645464e-06 lambda: 0.01\n",
      "    Loss_i: 0.1467471 Loss_f: 0.1470157 rho -4.0540453e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1467471 Loss_f: 0.14690611 rho -2.4302435e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1467471 Loss_f: 0.14677465 rho -4.216292e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1467471 Loss_f: 0.14677465 rho -4.2162918e-09 lambda: 10.0\n",
      "    Train error: 20.618824\n",
      "-- Epoch: 24  Batch: 12 --\n",
      "    Loss_i: 0.1518843 Loss_f: 0.15340221 rho -0.00021853879 lambda: 0.01\n",
      "    Loss_i: 0.1518843 Loss_f: 0.15346561 rho -2.8909808e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1518843 Loss_f: 0.15236145 rho -8.965252e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1518843 Loss_f: 0.15180591 rho 1.4770729e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15180591 Loss_f: 0.15184195 rho -6.792181e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15180591 Loss_f: 0.15184195 rho -6.7921806e-09 lambda: 10.0\n",
      "    Train error: 21.766163\n",
      "-- Epoch: 24  Batch: 13 --\n",
      "    Loss_i: 0.1433575 Loss_f: 0.14339757 rho -4.7599497e-06 lambda: 0.01\n",
      "    Loss_i: 0.1433575 Loss_f: 0.14365907 rho -4.357139e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1433575 Loss_f: 0.14344148 rho -1.240222e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1433575 Loss_f: 0.14333203 rho 3.769045e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14333203 Loss_f: 0.14333692 rho -7.2335044e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14333203 Loss_f: 0.1433369 rho -7.2114503e-10 lambda: 10.0\n",
      "    Train error: 20.068964\n",
      "-- Epoch: 24  Batch: 14 --\n",
      "    Loss_i: 0.1477994 Loss_f: 0.14744599 rho 5.5916546e-05 lambda: 0.01\n",
      "    Loss_i: 0.14744599 Loss_f: 0.14746237 rho -2.585512e-06 lambda: 0.01\n",
      "    Loss_i: 0.14744599 Loss_f: 0.14769697 rho -4.5889733e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14744599 Loss_f: 0.1475928 rho -2.7273566e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14744599 Loss_f: 0.1474712 rho -4.6915454e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14744599 Loss_f: 0.1474712 rho -4.6915445e-09 lambda: 10.0\n",
      "    Train error: 20.897703\n",
      "-- Epoch: 24  Batch: 15 --\n",
      "    Loss_i: 0.14587145 Loss_f: 0.14857182 rho -0.00031251568 lambda: 0.01\n",
      "    Loss_i: 0.14587145 Loss_f: 0.1485023 rho -4.0491755e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14587145 Loss_f: 0.14685307 rho -1.5623758e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14587145 Loss_f: 0.14582253 rho 7.812919e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14582253 Loss_f: 0.14594957 rho -2.0289223e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14582253 Loss_f: 0.14594957 rho -2.0289221e-08 lambda: 10.0\n",
      "    Train error: 20.337242\n",
      "-- Epoch: 24  Batch: 16 --\n",
      "    Loss_i: 0.14800331 Loss_f: 0.14922543 rho -0.00020871892 lambda: 0.01\n",
      "    Loss_i: 0.14800331 Loss_f: 0.14938104 rho -2.7852084e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14800331 Loss_f: 0.14851305 rho -1.0497682e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14800331 Loss_f: 0.14797539 rho 5.7616774e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14797539 Loss_f: 0.14803952 rho -1.3233346e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14797539 Loss_f: 0.14803952 rho -1.3233345e-08 lambda: 10.0\n",
      "    Train error: 21.087011\n",
      "-- Epoch: 24  Batch: 17 --\n",
      "    Loss_i: 0.14579684 Loss_f: 0.1461258 rho -4.222622e-05 lambda: 0.01\n",
      "    Loss_i: 0.14579684 Loss_f: 0.14636809 rho -9.0160465e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14579684 Loss_f: 0.14602424 rho -3.6734812e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14579684 Loss_f: 0.14578557 rho 1.8240564e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14578557 Loss_f: 0.14581586 rho -4.904702e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14578557 Loss_f: 0.14581586 rho -4.904702e-09 lambda: 10.0\n",
      "    Train error: 20.303375\n",
      "-- Epoch: 24  Batch: 18 --\n",
      "    Loss_i: 0.14825125 Loss_f: 0.14836559 rho -1.5865973e-05 lambda: 0.01\n",
      "    Loss_i: 0.14825125 Loss_f: 0.14862446 rho -6.0198327e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14825125 Loss_f: 0.14839397 rho -2.3400779e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14825125 Loss_f: 0.14824337 rho 1.2945798e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14824337 Loss_f: 0.14826058 rho -2.826769e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14824337 Loss_f: 0.14826058 rho -2.8267688e-09 lambda: 10.0\n",
      "    Train error: 20.904242\n",
      "-- Epoch: 24  Batch: 19 --\n",
      "    Loss_i: 0.14976485 Loss_f: 0.15011464 rho -4.220903e-05 lambda: 0.01\n",
      "    Loss_i: 0.14976485 Loss_f: 0.15038784 rho -1.0820965e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14976485 Loss_f: 0.15000835 rho -4.4238564e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14976485 Loss_f: 0.14974992 rho 2.7251523e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14974992 Loss_f: 0.14978115 rho -5.7002154e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14974992 Loss_f: 0.14978115 rho -5.7002145e-09 lambda: 10.0\n",
      "    Train error: 21.22533\n",
      "-- Epoch: 24  Batch: 20 --\n",
      "    Loss_i: 0.14627534 Loss_f: 0.14671779 rho -5.5168937e-05 lambda: 0.01\n",
      "    Loss_i: 0.14627534 Loss_f: 0.14691533 rho -9.82532e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14627534 Loss_f: 0.14652696 rho -3.9544005e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14627534 Loss_f: 0.14626339 rho 1.8825965e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14626339 Loss_f: 0.14629683 rho -5.267701e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14626339 Loss_f: 0.14629683 rho -5.267701e-09 lambda: 10.0\n",
      "    Train error: 20.585438\n",
      "-- Epoch: 24  Batch: 21 --\n",
      "    Loss_i: 0.14809002 Loss_f: 0.14759572 rho 6.837022e-05 lambda: 0.01\n",
      "    Loss_i: 0.14759572 Loss_f: 0.14747798 rho 1.6169777e-05 lambda: 0.01\n",
      "    Loss_i: 0.14747798 Loss_f: 0.14774513 rho -3.642861e-05 lambda: 0.01\n",
      "    Loss_i: 0.14747798 Loss_f: 0.14783488 rho -5.7820957e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14747798 Loss_f: 0.14766635 rho -3.1102098e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14747798 Loss_f: 0.14751284 rho -5.765962e-09 lambda: 9.999999\n",
      "    Train error: 20.848581\n",
      "-- Epoch: 24  Batch: 22 --\n",
      "    Loss_i: 0.14556402 Loss_f: 0.14712365 rho -0.00018959845 lambda: 0.01\n",
      "    Loss_i: 0.14556402 Loss_f: 0.14715698 rho -2.747947e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14556402 Loss_f: 0.14583163 rho -4.81831e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14556402 Loss_f: 0.14540398 rho 2.8941477e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14540398 Loss_f: 0.14537501 rho 5.2388454e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14537501 Loss_f: 0.14547691 rho -1.842855e-08 lambda: 9.999999\n",
      "    Train error: 20.396341\n",
      "-- Epoch: 24  Batch: 23 --\n",
      "    Loss_i: 0.15043746 Loss_f: 0.1529329 rho -0.00028093232 lambda: 0.01\n",
      "    Loss_i: 0.15043746 Loss_f: 0.15293886 rho -4.730291e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15043746 Loss_f: 0.15138389 rho -1.9202957e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15043746 Loss_f: 0.15039274 rho 9.139949e-09 lambda: 9.999999\n",
      "    Loss_i: 0.15039274 Loss_f: 0.15051864 rho -2.5732106e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15039274 Loss_f: 0.15051864 rho -2.5732104e-08 lambda: 10.0\n",
      "    Train error: 21.281485\n",
      "-- Epoch: 24  Batch: 24 --\n",
      "    Loss_i: 0.14998637 Loss_f: 0.15019697 rho -2.6246424e-05 lambda: 0.01\n",
      "    Loss_i: 0.14998637 Loss_f: 0.15050533 rho -9.25906e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14998637 Loss_f: 0.15018663 rho -3.7340274e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14998637 Loss_f: 0.14996724 rho 3.5837606e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14996724 Loss_f: 0.14999114 rho -4.476972e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14996724 Loss_f: 0.14999114 rho -4.4769717e-09 lambda: 10.0\n",
      "    Train error: 21.36308\n",
      "-- Epoch: 24  Batch: 25 --\n",
      "    Loss_i: 0.15087688 Loss_f: 0.15070209 rho 2.43051e-05 lambda: 0.01\n",
      "    Loss_i: 0.15070209 Loss_f: 0.1516937 rho -0.00013490723 lambda: 0.01\n",
      "    Loss_i: 0.15070209 Loss_f: 0.15180674 rho -1.8606876e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15070209 Loss_f: 0.15132484 rho -1.0745493e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15070209 Loss_f: 0.15081434 rho -1.9416095e-08 lambda: 9.999999\n",
      "    Loss_i: 0.15070209 Loss_f: 0.15081435 rho -1.9418673e-08 lambda: 10.0\n",
      "    Train error: 21.439625\n",
      "-- Epoch: 24  Batch: 26 --\n",
      "    Loss_i: 0.14702137 Loss_f: 0.17201114 rho -0.0031352593 lambda: 0.01\n",
      "    Loss_i: 0.14702137 Loss_f: 0.16927363 rho -0.00043357027 lambda: 0.099999994\n",
      "    Loss_i: 0.14702137 Loss_f: 0.15594299 rho -1.8400768e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14702137 Loss_f: 0.14680138 rho 4.5639037e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14680138 Loss_f: 0.14806513 rho -2.6219695e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14680138 Loss_f: 0.14806513 rho -2.6219695e-07 lambda: 10.0\n",
      "    Train error: 20.758554\n",
      "-- Epoch: 24  Batch: 27 --\n",
      "    Loss_i: 0.14617163 Loss_f: 0.15860076 rho -0.002041702 lambda: 0.01\n",
      "    Loss_i: 0.14617163 Loss_f: 0.15732515 rho -0.00021898584 lambda: 0.099999994\n",
      "    Loss_i: 0.14617163 Loss_f: 0.15045315 rho -8.573634e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14617163 Loss_f: 0.14607696 rho 1.8994664e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14607696 Loss_f: 0.14666097 rho -1.1711509e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14607696 Loss_f: 0.14666097 rho -1.17115086e-07 lambda: 10.0\n",
      "    Train error: 20.466413\n",
      "-- Epoch: 24  Batch: 28 --\n",
      "    Loss_i: 0.1448408 Loss_f: 0.14928772 rho -0.0006093774 lambda: 0.01\n",
      "    Loss_i: 0.1448408 Loss_f: 0.1490814 rho -7.335251e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1448408 Loss_f: 0.14647755 rho -2.9074554e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1448408 Loss_f: 0.14479807 rho 7.612074e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14479807 Loss_f: 0.14502037 rho -3.9586418e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14479807 Loss_f: 0.14502035 rho -3.9583753e-08 lambda: 10.0\n",
      "    Train error: 20.208202\n",
      "-- Epoch: 24  Batch: 29 --\n",
      "    Loss_i: 0.14687741 Loss_f: 0.14928801 rho -0.00037087296 lambda: 0.01\n",
      "    Loss_i: 0.14687741 Loss_f: 0.14926752 rho -4.2459124e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14687741 Loss_f: 0.14776208 rho -1.5962501e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14687741 Loss_f: 0.14684574 rho 5.722461e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14684574 Loss_f: 0.14695914 rho -2.0489612e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14684574 Loss_f: 0.14695916 rho -2.0492301e-08 lambda: 10.0\n",
      "    Train error: 20.633543\n",
      "-- Epoch: 24  Batch: 30 --\n",
      "    Loss_i: 0.14764424 Loss_f: 0.14897345 rho -0.00019404387 lambda: 0.01\n",
      "    Loss_i: 0.14764424 Loss_f: 0.14921452 rho -2.8582048e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14764424 Loss_f: 0.14826731 rho -1.1628171e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14764424 Loss_f: 0.14761914 rho 4.6949626e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14761914 Loss_f: 0.14770404 rho -1.5883462e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14761914 Loss_f: 0.14770404 rho -1.5883458e-08 lambda: 10.0\n",
      "    Train error: 20.551172\n",
      "\n",
      "*** Epoch: 24 Train error: 20.7957280476888  Test error: 20.37494  Time: 708.6779962 sec\n",
      "\n",
      "-- Epoch: 25  Batch: 1 --\n",
      "    Loss_i: 0.14431264 Loss_f: 0.14492604 rho -6.354758e-05 lambda: 0.01\n",
      "    Loss_i: 0.14431264 Loss_f: 0.1451448 rho -1.20977575e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14431264 Loss_f: 0.14463776 rho -4.9252066e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14431264 Loss_f: 0.14429691 rho 2.3915037e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14429691 Loss_f: 0.14433958 rho -6.489708e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14429691 Loss_f: 0.14433958 rho -6.489707e-09 lambda: 10.0\n",
      "    Train error: 19.922825\n",
      "-- Epoch: 25  Batch: 2 --\n",
      "    Loss_i: 0.14643806 Loss_f: 0.14738859 rho -0.00012323032 lambda: 0.01\n",
      "    Loss_i: 0.14643806 Loss_f: 0.14755686 rho -1.8202605e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14643806 Loss_f: 0.14686346 rho -7.102246e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14643806 Loss_f: 0.14641984 rho 3.0505936e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14641984 Loss_f: 0.14647551 rho -9.319157e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14641984 Loss_f: 0.14647551 rho -9.319157e-09 lambda: 10.0\n",
      "    Train error: 20.480562\n",
      "-- Epoch: 25  Batch: 3 --\n",
      "    Loss_i: 0.1483572 Loss_f: 0.14962606 rho -0.00015074581 lambda: 0.01\n",
      "    Loss_i: 0.1483572 Loss_f: 0.14985603 rho -2.4546573e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1483572 Loss_f: 0.14896075 rho -1.0273366e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1483572 Loss_f: 0.14833538 rho 3.7279326e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14833538 Loss_f: 0.14841895 rho -1.4277814e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14833538 Loss_f: 0.14841895 rho -1.4277814e-08 lambda: 10.0\n",
      "    Train error: 20.780375\n",
      "-- Epoch: 25  Batch: 4 --\n",
      "    Loss_i: 0.14391902 Loss_f: 0.14389876 rho 2.6204432e-06 lambda: 0.01\n",
      "    Loss_i: 0.14389876 Loss_f: 0.14505668 rho -0.0001506568 lambda: 0.01\n",
      "    Loss_i: 0.14389876 Loss_f: 0.14517516 rho -1.8954226e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14389876 Loss_f: 0.14466706 rho -1.1572637e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14389876 Loss_f: 0.14404494 rho -2.2050147e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14389876 Loss_f: 0.14404494 rho -2.2050145e-08 lambda: 10.0\n",
      "    Train error: 20.18634\n",
      "-- Epoch: 25  Batch: 5 --\n",
      "    Loss_i: 0.1473146 Loss_f: 0.18002035 rho -0.0044028177 lambda: 0.01\n",
      "    Loss_i: 0.1473146 Loss_f: 0.17610165 rho -0.0004893538 lambda: 0.099999994\n",
      "    Loss_i: 0.1473146 Loss_f: 0.15885533 rho -2.0147594e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1473146 Loss_f: 0.14709172 rho 3.901464e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14709172 Loss_f: 0.14872645 rho -2.8611046e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14709172 Loss_f: 0.14872645 rho -2.861104e-07 lambda: 10.0\n",
      "    Train error: 20.70768\n",
      "-- Epoch: 25  Batch: 6 --\n",
      "    Loss_i: 0.1416344 Loss_f: 0.15473503 rho -0.0017464408 lambda: 0.01\n",
      "    Loss_i: 0.1416344 Loss_f: 0.15346132 rho -0.0001845787 lambda: 0.099999994\n",
      "    Loss_i: 0.1416344 Loss_f: 0.1463046 rho -7.4152085e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1416344 Loss_f: 0.14153877 rho 1.5211159e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14153877 Loss_f: 0.1421923 rho -1.03941034e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14153877 Loss_f: 0.1421923 rho -1.0394103e-07 lambda: 10.0\n",
      "    Train error: 19.565157\n",
      "-- Epoch: 25  Batch: 7 --\n",
      "    Loss_i: 0.14985363 Loss_f: 0.15523581 rho -0.00076726655 lambda: 0.01\n",
      "    Loss_i: 0.14985363 Loss_f: 0.1549146 rho -9.110468e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14985363 Loss_f: 0.15180714 rho -3.611493e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14985363 Loss_f: 0.14980222 rho 9.529788e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14980222 Loss_f: 0.15006621 rho -4.8924854e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14980222 Loss_f: 0.15006621 rho -4.8924846e-08 lambda: 10.0\n",
      "    Train error: 21.10953\n",
      "-- Epoch: 25  Batch: 8 --\n",
      "    Loss_i: 0.15017451 Loss_f: 0.15236266 rho -0.00033475956 lambda: 0.01\n",
      "    Loss_i: 0.15017451 Loss_f: 0.15247224 rho -4.377742e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.15017451 Loss_f: 0.15109268 rho -1.7933313e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15017451 Loss_f: 0.1501508 rho 4.6422e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1501508 Loss_f: 0.150278 rho -2.4904487e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1501508 Loss_f: 0.150278 rho -2.4904482e-08 lambda: 10.0\n",
      "    Train error: 21.58987\n",
      "-- Epoch: 25  Batch: 9 --\n",
      "    Loss_i: 0.14719404 Loss_f: 0.14881226 rho -0.00023907867 lambda: 0.01\n",
      "    Loss_i: 0.14719404 Loss_f: 0.14887354 rho -3.098881e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14719404 Loss_f: 0.14783676 rho -1.2161628e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14719404 Loss_f: 0.14717275 rho 4.0395634e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14717275 Loss_f: 0.14725812 rho -1.6193304e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14717275 Loss_f: 0.14725812 rho -1.6193303e-08 lambda: 10.0\n",
      "    Train error: 20.81\n",
      "-- Epoch: 25  Batch: 10 --\n",
      "    Loss_i: 0.14656878 Loss_f: 0.1467894 rho -2.9262272e-05 lambda: 0.01\n",
      "    Loss_i: 0.14656878 Loss_f: 0.14704795 rho -7.5990024e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14656878 Loss_f: 0.14675967 rho -3.0877786e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14656878 Loss_f: 0.14655924 rho 1.5456485e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14655924 Loss_f: 0.14658354 rho -3.9389714e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14655924 Loss_f: 0.14658354 rho -3.938971e-09 lambda: 10.0\n",
      "    Train error: 20.650826\n",
      "-- Epoch: 25  Batch: 11 --\n",
      "    Loss_i: 0.14621224 Loss_f: 0.14576437 rho 6.321725e-05 lambda: 0.01\n",
      "    Loss_i: 0.14576437 Loss_f: 0.14574866 rho 2.1939104e-06 lambda: 0.01\n",
      "    Loss_i: 0.14574866 Loss_f: 0.1461744 rho -5.884651e-05 lambda: 0.01\n",
      "    Loss_i: 0.14574866 Loss_f: 0.14625008 rho -7.975782e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14574866 Loss_f: 0.14603883 rho -4.6861842e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14574866 Loss_f: 0.14580591 rho -9.259961e-09 lambda: 9.999999\n",
      "    Train error: 20.380438\n",
      "-- Epoch: 25  Batch: 12 --\n",
      "    Loss_i: 0.15132193 Loss_f: 0.15973316 rho -0.0012889119 lambda: 0.01\n",
      "    Loss_i: 0.15132193 Loss_f: 0.15865716 rho -0.00016710341 lambda: 0.099999994\n",
      "    Loss_i: 0.15132193 Loss_f: 0.15343507 rho -5.060166e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.15132193 Loss_f: 0.1509882 rho 8.032593e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1509882 Loss_f: 0.15113175 rho -3.455446e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1509882 Loss_f: 0.15113175 rho -3.455446e-08 lambda: 10.0\n",
      "    Train error: 21.554586\n",
      "-- Epoch: 25  Batch: 13 --\n",
      "    Loss_i: 0.14257832 Loss_f: 0.14224575 rho 4.120948e-05 lambda: 0.01\n",
      "    Loss_i: 0.14224575 Loss_f: 0.14412068 rho -0.00023029378 lambda: 0.01\n",
      "    Loss_i: 0.14224575 Loss_f: 0.14405479 rho -3.1224663e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14224575 Loss_f: 0.14307532 rho -1.4923381e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14224575 Loss_f: 0.1423673 rho -2.1958703e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14224575 Loss_f: 0.1423673 rho -2.1958702e-08 lambda: 10.0\n",
      "    Train error: 19.873383\n",
      "-- Epoch: 25  Batch: 14 --\n",
      "    Loss_i: 0.1469015 Loss_f: 0.14794067 rho -0.0001722086 lambda: 0.01\n",
      "    Loss_i: 0.1469015 Loss_f: 0.14779529 rho -2.0400012e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1469015 Loss_f: 0.14664869 rho 5.996512e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14664869 Loss_f: 0.14841664 rho -4.193244e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14664869 Loss_f: 0.1468415 rho -4.5911882e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14664869 Loss_f: 0.1468415 rho -4.591188e-08 lambda: 10.0\n",
      "    Train error: 20.641222\n",
      "-- Epoch: 25  Batch: 15 --\n",
      "    Loss_i: 0.14483604 Loss_f: 0.14670245 rho -0.00026010373 lambda: 0.01\n",
      "    Loss_i: 0.14483604 Loss_f: 0.14666244 rho -2.9254887e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14483604 Loss_f: 0.14535657 rho -8.464107e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14483604 Loss_f: 0.14474279 rho 1.5186309e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14474279 Loss_f: 0.14477916 rho -5.9241936e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14474279 Loss_f: 0.14477916 rho -5.9241936e-09 lambda: 10.0\n",
      "    Train error: 20.104702\n",
      "-- Epoch: 25  Batch: 16 --\n",
      "    Loss_i: 0.14702922 Loss_f: 0.14682142 rho 3.1690884e-05 lambda: 0.01\n",
      "    Loss_i: 0.14682142 Loss_f: 0.14796582 rho -0.00017225275 lambda: 0.01\n",
      "    Loss_i: 0.14682142 Loss_f: 0.14806409 rho -2.284571e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14682142 Loss_f: 0.14746657 rho -1.2129169e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14682142 Loss_f: 0.14692725 rho -1.994153e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14682142 Loss_f: 0.14692725 rho -1.994152e-08 lambda: 10.0\n",
      "    Train error: 20.743988\n",
      "-- Epoch: 25  Batch: 17 --\n",
      "    Loss_i: 0.14484622 Loss_f: 0.14845659 rho -0.00043658356 lambda: 0.01\n",
      "    Loss_i: 0.14484622 Loss_f: 0.14818706 rho -5.662167e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14484622 Loss_f: 0.14584208 rho -1.7584265e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14484622 Loss_f: 0.14467153 rho 3.097461e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14467153 Loss_f: 0.14474662 rho -1.33152405e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14467153 Loss_f: 0.14474662 rho -1.3315239e-08 lambda: 10.0\n",
      "    Train error: 20.109364\n",
      "-- Epoch: 25  Batch: 18 --\n",
      "    Loss_i: 0.14710192 Loss_f: 0.14747365 rho -4.085096e-05 lambda: 0.01\n",
      "    Loss_i: 0.14710192 Loss_f: 0.14764857 rho -9.079642e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14710192 Loss_f: 0.14719135 rho -1.5653146e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14710192 Loss_f: 0.14703828 rho 1.1200943e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14703828 Loss_f: 0.14702544 rho 2.2606783e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14702544 Loss_f: 0.14706352 rho -6.7034565e-09 lambda: 9.999999\n",
      "    Train error: 20.556557\n",
      "-- Epoch: 25  Batch: 19 --\n",
      "    Loss_i: 0.14858945 Loss_f: 0.14888507 rho -3.6884656e-05 lambda: 0.01\n",
      "    Loss_i: 0.14858945 Loss_f: 0.14916977 rho -9.722726e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14858945 Loss_f: 0.14879669 rho -3.5954204e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14858945 Loss_f: 0.14856488 rho 4.2780903e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14856488 Loss_f: 0.14858761 rho -3.959005e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14856488 Loss_f: 0.14858761 rho -3.9590042e-09 lambda: 10.0\n",
      "    Train error: 21.014112\n",
      "-- Epoch: 25  Batch: 20 --\n",
      "    Loss_i: 0.14509547 Loss_f: 0.14473131 rho 5.0059396e-05 lambda: 0.01\n",
      "    Loss_i: 0.14473131 Loss_f: 0.1446808 rho 6.895984e-06 lambda: 0.01\n",
      "    Loss_i: 0.1446808 Loss_f: 0.14494699 rho -3.608394e-05 lambda: 0.01\n",
      "    Loss_i: 0.1446808 Loss_f: 0.14505647 rho -5.7879374e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1446808 Loss_f: 0.14488386 rho -3.1718028e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1446808 Loss_f: 0.14471741 rho -5.726813e-09 lambda: 9.999999\n",
      "    Train error: 20.17356\n",
      "-- Epoch: 25  Batch: 21 --\n",
      "    Loss_i: 0.14692439 Loss_f: 0.14794016 rho -0.00015075371 lambda: 0.01\n",
      "    Loss_i: 0.14692439 Loss_f: 0.14794394 rho -1.8883815e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14692439 Loss_f: 0.14690474 rho 3.732935e-08 lambda: 0.99999994\n",
      "    Loss_i: 0.14690474 Loss_f: 0.14897032 rho -3.9227007e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14690474 Loss_f: 0.14717577 rho -5.1604232e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14690474 Loss_f: 0.14717577 rho -5.1604232e-08 lambda: 10.0\n",
      "    Train error: 20.693468\n",
      "-- Epoch: 25  Batch: 22 --\n",
      "    Loss_i: 0.14429052 Loss_f: 0.15231067 rho -0.00085114426 lambda: 0.01\n",
      "    Loss_i: 0.14429052 Loss_f: 0.1513921 rho -0.0001308469 lambda: 0.099999994\n",
      "    Loss_i: 0.14429052 Loss_f: 0.14648046 rho -4.355601e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14429052 Loss_f: 0.14399621 rho 5.90053e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14399621 Loss_f: 0.14417146 rho -3.513573e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14399621 Loss_f: 0.14417145 rho -3.5132743e-08 lambda: 10.0\n",
      "    Train error: 20.031832\n",
      "-- Epoch: 25  Batch: 23 --\n",
      "    Loss_i: 0.14929338 Loss_f: 0.14931068 rho -2.1561934e-06 lambda: 0.01\n",
      "    Loss_i: 0.14929338 Loss_f: 0.14964409 rho -6.031394e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14929338 Loss_f: 0.149389 rho -1.7093657e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14929338 Loss_f: 0.14926375 rho 5.316642e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14926375 Loss_f: 0.1492647 rho -1.6848256e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14926375 Loss_f: 0.1492647 rho -1.6848255e-10 lambda: 10.0\n",
      "    Train error: 20.956924\n",
      "-- Epoch: 25  Batch: 24 --\n",
      "    Loss_i: 0.14871408 Loss_f: 0.148313 rho 5.6247427e-05 lambda: 0.01\n",
      "    Loss_i: 0.148313 Loss_f: 0.14858802 rho -3.819861e-05 lambda: 0.01\n",
      "    Loss_i: 0.148313 Loss_f: 0.14873342 rho -7.1441696e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.148313 Loss_f: 0.14853358 rho -3.8339806e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.148313 Loss_f: 0.14835027 rho -6.492435e-09 lambda: 9.999999\n",
      "    Loss_i: 0.148313 Loss_f: 0.14835025 rho -6.4898384e-09 lambda: 10.0\n",
      "    Train error: 20.901903\n",
      "-- Epoch: 25  Batch: 25 --\n",
      "    Loss_i: 0.14944942 Loss_f: 0.15009034 rho -8.147383e-05 lambda: 0.01\n",
      "    Loss_i: 0.14944942 Loss_f: 0.15022153 rho -1.2919172e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14944942 Loss_f: 0.1496116 rho -2.802363e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14944942 Loss_f: 0.14938954 rho 1.0381846e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14938954 Loss_f: 0.14938395 rho 9.687274e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14938395 Loss_f: 0.14943276 rho -8.463072e-09 lambda: 9.999999\n",
      "    Train error: 21.193691\n",
      "-- Epoch: 25  Batch: 26 --\n",
      "    Loss_i: 0.14533864 Loss_f: 0.14645566 rho -0.00012662812 lambda: 0.01\n",
      "    Loss_i: 0.14533864 Loss_f: 0.14655554 rho -1.814592e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14533864 Loss_f: 0.14578225 rho -6.830299e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14533864 Loss_f: 0.14531378 rho 3.839492e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14531378 Loss_f: 0.14536873 rho -8.486361e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14531378 Loss_f: 0.14536873 rho -8.48636e-09 lambda: 10.0\n",
      "    Train error: 20.285234\n",
      "-- Epoch: 25  Batch: 27 --\n",
      "    Loss_i: 0.14481252 Loss_f: 0.14472848 rho 8.951527e-06 lambda: 0.01\n",
      "    Loss_i: 0.14472848 Loss_f: 0.14576872 rho -0.000111215704 lambda: 0.01\n",
      "    Loss_i: 0.14472848 Loss_f: 0.14588721 rho -1.7493805e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14472848 Loss_f: 0.1453792 rho -1.0246429e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14472848 Loss_f: 0.14484519 rho -1.8456216e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14472848 Loss_f: 0.14484519 rho -1.8456216e-08 lambda: 10.0\n",
      "    Train error: 20.289396\n",
      "-- Epoch: 25  Batch: 28 --\n",
      "    Loss_i: 0.14362249 Loss_f: 0.17018446 rho -0.00309129 lambda: 0.01\n",
      "    Loss_i: 0.14362249 Loss_f: 0.1671272 rho -0.0003881036 lambda: 0.099999994\n",
      "    Loss_i: 0.14362249 Loss_f: 0.15293975 rho -1.6056827e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14362249 Loss_f: 0.14341378 rho 3.612503e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14341378 Loss_f: 0.14471905 rho -2.2590879e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14341378 Loss_f: 0.14471905 rho -2.2590876e-07 lambda: 10.0\n",
      "    Train error: 19.796352\n",
      "-- Epoch: 25  Batch: 29 --\n",
      "    Loss_i: 0.14557911 Loss_f: 0.15474619 rho -0.001287718 lambda: 0.01\n",
      "    Loss_i: 0.14557911 Loss_f: 0.15388866 rho -0.00015116835 lambda: 0.099999994\n",
      "    Loss_i: 0.14557911 Loss_f: 0.14883047 rho -6.0947555e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14557911 Loss_f: 0.14550641 rho 1.3669883e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14550641 Loss_f: 0.14595869 rho -8.5029356e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14550641 Loss_f: 0.14595869 rho -8.502935e-08 lambda: 10.0\n",
      "    Train error: 20.384453\n",
      "-- Epoch: 25  Batch: 30 --\n",
      "    Loss_i: 0.14628772 Loss_f: 0.14740348 rho -0.00012748959 lambda: 0.01\n",
      "    Loss_i: 0.14628772 Loss_f: 0.1476224 rho -2.25243e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14628772 Loss_f: 0.14679217 rho -8.939539e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14628772 Loss_f: 0.14626464 rho 4.1110217e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14626464 Loss_f: 0.14632986 rho -1.1616049e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14626464 Loss_f: 0.14632986 rho -1.1616047e-08 lambda: 10.0\n",
      "    Train error: 20.157787\n",
      "\n",
      "*** Epoch: 25 Train error: 20.521537208557127  Test error: 20.01512  Time: 706.2634971000007 sec\n",
      "\n",
      "-- Epoch: 26  Batch: 1 --\n",
      "    Loss_i: 0.1429959 Loss_f: 0.14282663 rho 2.078238e-05 lambda: 0.01\n",
      "    Loss_i: 0.14282663 Loss_f: 0.1433703 rho -6.661154e-05 lambda: 0.01\n",
      "    Loss_i: 0.14282663 Loss_f: 0.14352396 rho -9.8366e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14282663 Loss_f: 0.143214 rho -5.5482394e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14282663 Loss_f: 0.14289421 rho -9.693789e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14282663 Loss_f: 0.14289421 rho -9.693788e-09 lambda: 10.0\n",
      "    Train error: 19.641497\n",
      "-- Epoch: 26  Batch: 2 --\n",
      "    Loss_i: 0.14510374 Loss_f: 0.15070459 rho -0.0007498544 lambda: 0.01\n",
      "    Loss_i: 0.14510374 Loss_f: 0.15019475 rho -8.347752e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14510374 Loss_f: 0.14687262 rho -2.9671328e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14510374 Loss_f: 0.1449819 rho 2.0483178e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1449819 Loss_f: 0.14518453 rho -3.4064893e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1449819 Loss_f: 0.14518453 rho -3.406489e-08 lambda: 10.0\n",
      "    Train error: 20.07481\n",
      "-- Epoch: 26  Batch: 3 --\n",
      "    Loss_i: 0.14666711 Loss_f: 0.14654683 rho 1.2177639e-05 lambda: 0.01\n",
      "    Loss_i: 0.14654683 Loss_f: 0.14752541 rho -9.895138e-05 lambda: 0.01\n",
      "    Loss_i: 0.14654683 Loss_f: 0.14764382 rho -1.5422209e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14654683 Loss_f: 0.14714764 rho -8.7897183e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14654683 Loss_f: 0.14664987 rho -1.5136116e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14654683 Loss_f: 0.14664987 rho -1.5136115e-08 lambda: 10.0\n",
      "    Train error: 20.429123\n",
      "-- Epoch: 26  Batch: 4 --\n",
      "    Loss_i: 0.14251226 Loss_f: 0.14787987 rho -0.0006747274 lambda: 0.01\n",
      "    Loss_i: 0.14251226 Loss_f: 0.14727817 rho -8.0911945e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14251226 Loss_f: 0.14389652 rho -2.4354672e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14251226 Loss_f: 0.14228068 rho 4.0892626e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14228068 Loss_f: 0.14237437 rho -1.6540566e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14228068 Loss_f: 0.14237437 rho -1.6540563e-08 lambda: 10.0\n",
      "    Train error: 19.645613\n",
      "-- Epoch: 26  Batch: 5 --\n",
      "    Loss_i: 0.14556539 Loss_f: 0.14641199 rho -0.0001046723 lambda: 0.01\n",
      "    Loss_i: 0.14556539 Loss_f: 0.14653549 rho -1.3721282e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14556539 Loss_f: 0.1458735 rho -4.421677e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14556539 Loss_f: 0.14551817 rho 6.7866717e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14551817 Loss_f: 0.14554504 rho -3.8613615e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14551817 Loss_f: 0.14554504 rho -3.8613615e-09 lambda: 10.0\n",
      "    Train error: 20.402065\n",
      "-- Epoch: 26  Batch: 6 --\n",
      "    Loss_i: 0.14002685 Loss_f: 0.13997152 rho 5.6260806e-06 lambda: 0.01\n",
      "    Loss_i: 0.13997152 Loss_f: 0.14121516 rho -0.0001246762 lambda: 0.01\n",
      "    Loss_i: 0.13997152 Loss_f: 0.14131385 rho -1.708979e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13997152 Loss_f: 0.14071794 rho -9.766596e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13997152 Loss_f: 0.14010184 rho -1.7098182e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13997152 Loss_f: 0.14010184 rho -1.709818e-08 lambda: 10.0\n",
      "    Train error: 19.086788\n",
      "-- Epoch: 26  Batch: 7 --\n",
      "    Loss_i: 0.14861578 Loss_f: 0.15767933 rho -0.0012562431 lambda: 0.01\n",
      "    Loss_i: 0.14861578 Loss_f: 0.15649585 rho -0.0001570677 lambda: 0.099999994\n",
      "    Loss_i: 0.14861578 Loss_f: 0.15102217 rho -5.0162253e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14861578 Loss_f: 0.14835338 rho 5.4949062e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14835338 Loss_f: 0.14855821 rho -4.2904333e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14835338 Loss_f: 0.14855821 rho -4.290433e-08 lambda: 10.0\n",
      "    Train error: 20.786133\n",
      "-- Epoch: 26  Batch: 8 --\n",
      "    Loss_i: 0.1487889 Loss_f: 0.15004389 rho -0.00019201364 lambda: 0.01\n",
      "    Loss_i: 0.1487889 Loss_f: 0.1501711 rho -2.8714234e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1487889 Loss_f: 0.14914669 rho -7.708675e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1487889 Loss_f: 0.14869823 rho 1.9608516e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14869823 Loss_f: 0.14870737 rho -1.978701e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14869823 Loss_f: 0.14870737 rho -1.9787008e-09 lambda: 10.0\n",
      "    Train error: 21.171118\n",
      "-- Epoch: 26  Batch: 9 --\n",
      "    Loss_i: 0.14573446 Loss_f: 0.14549612 rho 2.9102346e-05 lambda: 0.01\n",
      "    Loss_i: 0.14549612 Loss_f: 0.14632267 rho -0.00010066235 lambda: 0.01\n",
      "    Loss_i: 0.14549612 Loss_f: 0.1464164 rho -1.5915899e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14549612 Loss_f: 0.14596374 rho -8.442086e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14549612 Loss_f: 0.14557219 rho -1.3793471e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14549612 Loss_f: 0.14557219 rho -1.379347e-08 lambda: 10.0\n",
      "    Train error: 20.427628\n",
      "-- Epoch: 26  Batch: 10 --\n",
      "    Loss_i: 0.14494845 Loss_f: 0.14590925 rho -0.00010610094 lambda: 0.01\n",
      "    Loss_i: 0.14494845 Loss_f: 0.14592588 rho -1.50807855e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14494845 Loss_f: 0.1450584 rho -1.7664438e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14494845 Loss_f: 0.14482217 rho 2.0375149e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14482217 Loss_f: 0.14478603 rho 5.830056e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14478603 Loss_f: 0.14483933 rho -8.599632e-09 lambda: 9.999999\n",
      "    Train error: 20.219778\n",
      "-- Epoch: 26  Batch: 11 --\n",
      "    Loss_i: 0.144301 Loss_f: 0.14435722 rho -5.3542813e-06 lambda: 0.01\n",
      "    Loss_i: 0.144301 Loss_f: 0.14456688 rho -3.3217416e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.144301 Loss_f: 0.1443574 rho -7.273154e-08 lambda: 0.99999994\n",
      "    Loss_i: 0.144301 Loss_f: 0.1442729 rho 3.6357897e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1442729 Loss_f: 0.14426975 rho 4.067483e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14426975 Loss_f: 0.14429267 rho -2.9647351e-09 lambda: 9.999999\n",
      "    Train error: 19.982496\n",
      "-- Epoch: 26  Batch: 12 --\n",
      "    Loss_i: 0.14920358 Loss_f: 0.14948943 rho -3.5510675e-05 lambda: 0.01\n",
      "    Loss_i: 0.14920358 Loss_f: 0.14971152 rho -8.221594e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14920358 Loss_f: 0.14936534 rho -2.6999584e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14920358 Loss_f: 0.14917839 rho 4.2191957e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14917839 Loss_f: 0.14919202 rho -2.2830917e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14917839 Loss_f: 0.14919202 rho -2.2830913e-09 lambda: 10.0\n",
      "    Train error: 21.081669\n",
      "-- Epoch: 26  Batch: 13 --\n",
      "    Loss_i: 0.14086853 Loss_f: 0.14080034 rho 7.92936e-06 lambda: 0.01\n",
      "    Loss_i: 0.14080034 Loss_f: 0.14167446 rho -9.9765406e-05 lambda: 0.01\n",
      "    Loss_i: 0.14080034 Loss_f: 0.14179406 rho -1.3080688e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14080034 Loss_f: 0.14138749 rho -7.8492815e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14080034 Loss_f: 0.14091018 rho -1.47063215e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14080034 Loss_f: 0.14091018 rho -1.4706319e-08 lambda: 10.0\n",
      "    Train error: 19.401773\n",
      "-- Epoch: 26  Batch: 14 --\n",
      "    Loss_i: 0.14546824 Loss_f: 0.1675623 rho -0.0032132473 lambda: 0.01\n",
      "    Loss_i: 0.14546824 Loss_f: 0.1650422 rho -0.00040240254 lambda: 0.099999994\n",
      "    Loss_i: 0.14546824 Loss_f: 0.1531936 rho -1.6566952e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14546824 Loss_f: 0.14530154 rho 3.5903405e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14530154 Loss_f: 0.14638527 rho -2.3338865e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14530154 Loss_f: 0.14638527 rho -2.333886e-07 lambda: 10.0\n",
      "    Train error: 20.36172\n",
      "-- Epoch: 26  Batch: 15 --\n",
      "    Loss_i: 0.14341699 Loss_f: 0.15363018 rho -0.0011147987 lambda: 0.01\n",
      "    Loss_i: 0.14341699 Loss_f: 0.15261747 rho -0.00013519694 lambda: 0.099999994\n",
      "    Loss_i: 0.14341699 Loss_f: 0.14703676 rho -5.509868e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14341699 Loss_f: 0.14333944 rho 1.1846158e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14333944 Loss_f: 0.14384928 rho -7.7860236e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14333944 Loss_f: 0.14384928 rho -7.786022e-08 lambda: 10.0\n",
      "    Train error: 19.69836\n",
      "-- Epoch: 26  Batch: 16 --\n",
      "    Loss_i: 0.14565381 Loss_f: 0.15050372 rho -0.0007012971 lambda: 0.01\n",
      "    Loss_i: 0.14565381 Loss_f: 0.15018365 rho -8.971011e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14565381 Loss_f: 0.1473736 rho -3.5366263e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14565381 Loss_f: 0.14559136 rho 1.2892055e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14559136 Loss_f: 0.14582115 rho -4.7442267e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14559136 Loss_f: 0.14582115 rho -4.744226e-08 lambda: 10.0\n",
      "    Train error: 20.478039\n",
      "-- Epoch: 26  Batch: 17 --\n",
      "    Loss_i: 0.143336 Loss_f: 0.14498481 rho -0.00018005699 lambda: 0.01\n",
      "    Loss_i: 0.143336 Loss_f: 0.14504677 rho -2.4223697e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.143336 Loss_f: 0.14399111 rho -9.559649e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.143336 Loss_f: 0.14331938 rho 2.4319198e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14331938 Loss_f: 0.14340582 rho -1.2649426e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14331938 Loss_f: 0.14340582 rho -1.2649426e-08 lambda: 10.0\n",
      "    Train error: 19.70043\n",
      "-- Epoch: 26  Batch: 18 --\n",
      "    Loss_i: 0.14580512 Loss_f: 0.14637329 rho -5.2822743e-05 lambda: 0.01\n",
      "    Loss_i: 0.14580512 Loss_f: 0.14657308 rho -1.1333545e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14580512 Loss_f: 0.146103 rho -4.670345e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14580512 Loss_f: 0.1457889 rho 2.5602518e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1457889 Loss_f: 0.14582789 rho -6.152635e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1457889 Loss_f: 0.14582789 rho -6.1526344e-09 lambda: 10.0\n",
      "    Train error: 20.277937\n",
      "-- Epoch: 26  Batch: 19 --\n",
      "    Loss_i: 0.14717127 Loss_f: 0.1476883 rho -6.808934e-05 lambda: 0.01\n",
      "    Loss_i: 0.14717127 Loss_f: 0.14788057 rho -1.1527832e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14717127 Loss_f: 0.14743021 rho -4.3092672e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14717127 Loss_f: 0.14715572 rho 2.595203e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14715572 Loss_f: 0.14718775 rho -5.344424e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14715572 Loss_f: 0.14718775 rho -5.3444236e-09 lambda: 10.0\n",
      "    Train error: 20.601067\n",
      "-- Epoch: 26  Batch: 20 --\n",
      "    Loss_i: 0.14370894 Loss_f: 0.14403018 rho -3.6991452e-05 lambda: 0.01\n",
      "    Loss_i: 0.14370894 Loss_f: 0.14420928 rho -6.71372e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14370894 Loss_f: 0.14389639 rho -2.5574394e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14370894 Loss_f: 0.14369889 rho 1.3746538e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14369889 Loss_f: 0.14372247 rho -3.2237826e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14369889 Loss_f: 0.14372247 rho -3.2237824e-09 lambda: 10.0\n",
      "    Train error: 19.926619\n",
      "-- Epoch: 26  Batch: 21 --\n",
      "    Loss_i: 0.14558907 Loss_f: 0.14512688 rho 6.432001e-05 lambda: 0.01\n",
      "    Loss_i: 0.14512688 Loss_f: 0.14504899 rho 1.0780691e-05 lambda: 0.01\n",
      "    Loss_i: 0.14504899 Loss_f: 0.14536354 rho -4.3295677e-05 lambda: 0.01\n",
      "    Loss_i: 0.14504899 Loss_f: 0.14544427 rho -6.3338252e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14504899 Loss_f: 0.1452587 rho -3.4162676e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14504899 Loss_f: 0.14508732 rho -6.2540497e-09 lambda: 9.999999\n",
      "    Train error: 20.250422\n",
      "-- Epoch: 26  Batch: 22 --\n",
      "    Loss_i: 0.14313312 Loss_f: 0.1458334 rho -0.0004314585 lambda: 0.01\n",
      "    Loss_i: 0.14313312 Loss_f: 0.14566179 rho -4.6854973e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14313312 Loss_f: 0.14371566 rho -1.0969444e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14313312 Loss_f: 0.14294703 rho 3.5097223e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14294703 Loss_f: 0.14295001 rho -5.62063e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14294703 Loss_f: 0.14295001 rho -5.620629e-10 lambda: 10.0\n",
      "    Train error: 19.737875\n",
      "-- Epoch: 26  Batch: 23 --\n",
      "    Loss_i: 0.14807907 Loss_f: 0.14781907 rho 3.7242546e-05 lambda: 0.01\n",
      "    Loss_i: 0.14781907 Loss_f: 0.14961027 rho -0.00025502432 lambda: 0.01\n",
      "    Loss_i: 0.14781907 Loss_f: 0.14953382 rho -3.063299e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14781907 Loss_f: 0.14857706 rho -1.3895012e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14781907 Loss_f: 0.14793114 rho -2.0598073e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14781907 Loss_f: 0.14793114 rho -2.0598067e-08 lambda: 10.0\n",
      "    Train error: 20.66118\n",
      "-- Epoch: 26  Batch: 24 --\n",
      "    Loss_i: 0.14763193 Loss_f: 0.1490579 rho -0.00017677895 lambda: 0.01\n",
      "    Loss_i: 0.14763193 Loss_f: 0.14885682 rho -2.3063607e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14763193 Loss_f: 0.14762996 rho 3.9062673e-09 lambda: 0.99999994\n",
      "    Loss_i: 0.14762996 Loss_f: 0.15004508 rho -4.795239e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14762996 Loss_f: 0.14795041 rho -6.398829e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14762996 Loss_f: 0.14795041 rho -6.398828e-08 lambda: 10.0\n",
      "    Train error: 20.687462\n",
      "-- Epoch: 26  Batch: 25 --\n",
      "    Loss_i: 0.1485335 Loss_f: 0.16111481 rho -0.0016452873 lambda: 0.01\n",
      "    Loss_i: 0.1485335 Loss_f: 0.15944555 rho -0.00020386987 lambda: 0.099999994\n",
      "    Loss_i: 0.1485335 Loss_f: 0.15214367 rho -7.0469696e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1485335 Loss_f: 0.1482225 rho 6.097694e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1482225 Loss_f: 0.14859377 rho -7.281038e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1482225 Loss_f: 0.14859378 rho -7.281329e-08 lambda: 10.0\n",
      "    Train error: 20.923683\n",
      "-- Epoch: 26  Batch: 26 --\n",
      "    Loss_i: 0.14428683 Loss_f: 0.14624643 rho -0.00025730865 lambda: 0.01\n",
      "    Loss_i: 0.14428683 Loss_f: 0.14626752 rho -3.0284502e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14428683 Loss_f: 0.14497949 rho -1.076783e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14428683 Loss_f: 0.14423305 rho 8.374104e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14423305 Loss_f: 0.14431061 rho -1.2076013e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14423305 Loss_f: 0.14431061 rho -1.2076013e-08 lambda: 10.0\n",
      "    Train error: 20.012384\n",
      "-- Epoch: 26  Batch: 27 --\n",
      "    Loss_i: 0.14355455 Loss_f: 0.1437396 rho -2.1111455e-05 lambda: 0.01\n",
      "    Loss_i: 0.14355455 Loss_f: 0.14397301 rho -5.984788e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14355455 Loss_f: 0.14370912 rho -2.268194e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14355455 Loss_f: 0.14353849 rho 2.36334e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14353849 Loss_f: 0.14355774 rho -2.8323397e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14353849 Loss_f: 0.14355774 rho -2.832339e-09 lambda: 10.0\n",
      "    Train error: 19.8858\n",
      "-- Epoch: 26  Batch: 28 --\n",
      "    Loss_i: 0.14239393 Loss_f: 0.14220767 rho 2.1624308e-05 lambda: 0.01\n",
      "    Loss_i: 0.14220767 Loss_f: 0.14295867 rho -8.55116e-05 lambda: 0.01\n",
      "    Loss_i: 0.14220767 Loss_f: 0.14311454 rho -1.1849089e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14220767 Loss_f: 0.14274468 rho -7.121559e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14220767 Loss_f: 0.14230786 rho -1.33073845e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14220767 Loss_f: 0.14230786 rho -1.3307383e-08 lambda: 10.0\n",
      "    Train error: 19.45209\n",
      "-- Epoch: 26  Batch: 29 --\n",
      "    Loss_i: 0.1445111 Loss_f: 0.16472617 rho -0.0023767946 lambda: 0.01\n",
      "    Loss_i: 0.1445111 Loss_f: 0.16250212 rho -0.00032020503 lambda: 0.099999994\n",
      "    Loss_i: 0.1445111 Loss_f: 0.15167171 rho -1.3434689e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.1445111 Loss_f: 0.14435436 rho 2.9568614e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14435436 Loss_f: 0.14536317 rho -1.9024121e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14435436 Loss_f: 0.14536317 rho -1.9024121e-07 lambda: 10.0\n",
      "    Train error: 20.110456\n",
      "-- Epoch: 26  Batch: 30 --\n",
      "    Loss_i: 0.14502987 Loss_f: 0.14839114 rho -0.00047291946 lambda: 0.01\n",
      "    Loss_i: 0.14502987 Loss_f: 0.14823008 rho -5.761628e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14502987 Loss_f: 0.14616035 rho -2.0938533e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14502987 Loss_f: 0.14497069 rho 1.0994245e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14497069 Loss_f: 0.14510724 rho -2.5365665e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14497069 Loss_f: 0.14510724 rho -2.5365665e-08 lambda: 10.0\n",
      "    Train error: 19.847084\n",
      "\n",
      "*** Epoch: 26 Train error: 20.16543655395508  Test error: 19.693977  Time: 711.8070317999991 sec\n",
      "\n",
      "-- Epoch: 27  Batch: 1 --\n",
      "    Loss_i: 0.14165673 Loss_f: 0.14146797 rho 2.1274787e-05 lambda: 0.01\n",
      "    Loss_i: 0.14146797 Loss_f: 0.14182034 rho -3.969034e-05 lambda: 0.01\n",
      "    Loss_i: 0.14146797 Loss_f: 0.14196928 rho -6.338084e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14146797 Loss_f: 0.14175873 rho -3.7215926e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14146797 Loss_f: 0.14152008 rho -6.6782295e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14146797 Loss_f: 0.14152008 rho -6.6782295e-09 lambda: 10.0\n",
      "    Train error: 19.304337\n",
      "-- Epoch: 27  Batch: 2 --\n",
      "    Loss_i: 0.14381972 Loss_f: 0.14856368 rho -0.0005356457 lambda: 0.01\n",
      "    Loss_i: 0.14381972 Loss_f: 0.14817101 rho -6.682013e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14381972 Loss_f: 0.14541732 rho -2.544966e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14381972 Loss_f: 0.14372896 rho 1.4512701e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14372896 Loss_f: 0.1439303 rho -3.2193807e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14372896 Loss_f: 0.1439303 rho -3.2193803e-08 lambda: 10.0\n",
      "    Train error: 19.761652\n",
      "-- Epoch: 27  Batch: 3 --\n",
      "    Loss_i: 0.14539798 Loss_f: 0.14518103 rho 2.6108446e-05 lambda: 0.01\n",
      "    Loss_i: 0.14518103 Loss_f: 0.14577095 rho -7.086202e-05 lambda: 0.01\n",
      "    Loss_i: 0.14518103 Loss_f: 0.14591785 rho -1.0357026e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14518103 Loss_f: 0.14558147 rho -5.726195e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14518103 Loss_f: 0.14524892 rho -9.724926e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14518103 Loss_f: 0.14524892 rho -9.724924e-09 lambda: 10.0\n",
      "    Train error: 20.067207\n",
      "-- Epoch: 27  Batch: 4 --\n",
      "    Loss_i: 0.14127423 Loss_f: 0.14443646 rho -0.00043589552 lambda: 0.01\n",
      "    Loss_i: 0.14127423 Loss_f: 0.14412405 rho -4.4968063e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14127423 Loss_f: 0.142086 rho -1.2997242e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14127423 Loss_f: 0.14111726 rho 2.5169218e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14111726 Loss_f: 0.14117415 rho -9.121971e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14111726 Loss_f: 0.14117415 rho -9.12197e-09 lambda: 10.0\n",
      "    Train error: 19.35312\n",
      "-- Epoch: 27  Batch: 5 --\n",
      "    Loss_i: 0.1443491 Loss_f: 0.14474186 rho -4.5265773e-05 lambda: 0.01\n",
      "    Loss_i: 0.1443491 Loss_f: 0.1449191 rho -7.779128e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1443491 Loss_f: 0.14453185 rho -2.5408704e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1443491 Loss_f: 0.1443222 rho 3.7466523e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1443222 Loss_f: 0.14433928 rho -2.3787503e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1443222 Loss_f: 0.1443393 rho -2.3808255e-09 lambda: 10.0\n",
      "    Train error: 20.09483\n",
      "-- Epoch: 27  Batch: 6 --\n",
      "    Loss_i: 0.13876694 Loss_f: 0.13867566 rho 9.345964e-06 lambda: 0.01\n",
      "    Loss_i: 0.13867566 Loss_f: 0.13971075 rho -0.00010471536 lambda: 0.01\n",
      "    Loss_i: 0.13867566 Loss_f: 0.13980672 rho -1.3654987e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13867566 Loss_f: 0.13929923 rho -7.6766435e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13867566 Loss_f: 0.1387855 rho -1.3548475e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13867566 Loss_f: 0.1387855 rho -1.3548474e-08 lambda: 10.0\n",
      "    Train error: 18.772322\n",
      "-- Epoch: 27  Batch: 7 --\n",
      "    Loss_i: 0.14725009 Loss_f: 0.15446222 rho -0.0010497108 lambda: 0.01\n",
      "    Loss_i: 0.14725009 Loss_f: 0.15362918 rho -0.00011554562 lambda: 0.099999994\n",
      "    Loss_i: 0.14725009 Loss_f: 0.14921781 rho -3.6534977e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14725009 Loss_f: 0.14702062 rho 4.271161e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14702062 Loss_f: 0.14719582 rho -3.2610576e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14702062 Loss_f: 0.1471958 rho -3.2607797e-08 lambda: 10.0\n",
      "    Train error: 20.457676\n",
      "-- Epoch: 27  Batch: 8 --\n",
      "    Loss_i: 0.14753015 Loss_f: 0.14819786 rho -9.262824e-05 lambda: 0.01\n",
      "    Loss_i: 0.14753015 Loss_f: 0.14840691 rho -1.657942e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14753015 Loss_f: 0.14774637 rho -4.2427e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14753015 Loss_f: 0.14747575 rho 1.0715839e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14747575 Loss_f: 0.1474837 rho -1.567295e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14747575 Loss_f: 0.1474837 rho -1.5672946e-09 lambda: 10.0\n",
      "    Train error: 20.865976\n",
      "-- Epoch: 27  Batch: 9 --\n",
      "    Loss_i: 0.1445023 Loss_f: 0.1442838 rho 3.1798558e-05 lambda: 0.01\n",
      "    Loss_i: 0.1442838 Loss_f: 0.1452352 rho -0.00013788612 lambda: 0.01\n",
      "    Loss_i: 0.1442838 Loss_f: 0.14528753 rho -1.7130316e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1442838 Loss_f: 0.1447489 rho -8.081141e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1442838 Loss_f: 0.1443543 rho -1.227128e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1442838 Loss_f: 0.1443543 rho -1.2271279e-08 lambda: 10.0\n",
      "    Train error: 20.111946\n",
      "-- Epoch: 27  Batch: 10 --\n",
      "    Loss_i: 0.14378949 Loss_f: 0.1444461 rho -7.437825e-05 lambda: 0.01\n",
      "    Loss_i: 0.14378949 Loss_f: 0.14445975 rho -1.0055941e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14378949 Loss_f: 0.14370613 rho 1.2925314e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14370613 Loss_f: 0.14496024 rho -1.9444258e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14370613 Loss_f: 0.14385323 rho -2.2884732e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14370613 Loss_f: 0.14385323 rho -2.2884732e-08 lambda: 10.0\n",
      "    Train error: 19.939117\n",
      "-- Epoch: 27  Batch: 11 --\n",
      "    Loss_i: 0.14327997 Loss_f: 0.14477989 rho -0.00016772098 lambda: 0.01\n",
      "    Loss_i: 0.14327997 Loss_f: 0.14472255 rho -2.0991562e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14327997 Loss_f: 0.1435237 rho -3.6566968e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14327997 Loss_f: 0.14316818 rho 1.682443e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14316818 Loss_f: 0.14314418 rho 3.6128538e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14314418 Loss_f: 0.14320889 rho -9.739483e-09 lambda: 9.999999\n",
      "    Train error: 19.76189\n",
      "-- Epoch: 27  Batch: 12 --\n",
      "    Loss_i: 0.14792658 Loss_f: 0.14780758 rho 9.548068e-06 lambda: 0.01\n",
      "    Loss_i: 0.14780758 Loss_f: 0.148874 rho -8.5337786e-05 lambda: 0.01\n",
      "    Loss_i: 0.14780758 Loss_f: 0.1489754 rho -1.8016914e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14780758 Loss_f: 0.14844434 rho -1.0828585e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14780758 Loss_f: 0.14791793 rho -1.8958701e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14780758 Loss_f: 0.14791793 rho -1.89587e-08 lambda: 10.0\n",
      "    Train error: 20.73038\n",
      "-- Epoch: 27  Batch: 13 --\n",
      "    Loss_i: 0.13995837 Loss_f: 0.14727242 rho -0.0008240312 lambda: 0.01\n",
      "    Loss_i: 0.13995837 Loss_f: 0.14622112 rho -9.212978e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13995837 Loss_f: 0.1416609 rho -2.5835357e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.13995837 Loss_f: 0.13960938 rho 5.312489e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13960938 Loss_f: 0.13969328 rho -1.2769829e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13960938 Loss_f: 0.13969328 rho -1.2769829e-08 lambda: 10.0\n",
      "    Train error: 19.17944\n",
      "-- Epoch: 27  Batch: 14 --\n",
      "    Loss_i: 0.14414337 Loss_f: 0.14423957 rho -1.17117215e-05 lambda: 0.01\n",
      "    Loss_i: 0.14414337 Loss_f: 0.14448339 rho -5.836371e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14414337 Loss_f: 0.1442161 rho -1.3018293e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14414337 Loss_f: 0.14410892 rho 6.1928938e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14410892 Loss_f: 0.14410527 rho 6.562381e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14410527 Loss_f: 0.14413227 rho -4.8533653e-09 lambda: 9.999999\n",
      "    Train error: 19.9796\n",
      "-- Epoch: 27  Batch: 15 --\n",
      "    Loss_i: 0.14214452 Loss_f: 0.14214948 rho -5.6743994e-07 lambda: 0.01\n",
      "    Loss_i: 0.14214452 Loss_f: 0.14236733 rho -2.9115145e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14214452 Loss_f: 0.14222044 rho -1.00641046e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14214452 Loss_f: 0.1421318 rho 1.6873642e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1421318 Loss_f: 0.14213881 rho -9.2972097e-10 lambda: 9.999999\n",
      "    Loss_i: 0.1421318 Loss_f: 0.14213881 rho -9.297208e-10 lambda: 10.0\n",
      "    Train error: 19.425325\n",
      "-- Epoch: 27  Batch: 16 --\n",
      "    Loss_i: 0.1443953 Loss_f: 0.14394847 rho 5.8548132e-05 lambda: 0.01\n",
      "    Loss_i: 0.14394847 Loss_f: 0.14377607 rho 2.24956e-05 lambda: 0.01\n",
      "    Loss_i: 0.14377607 Loss_f: 0.14388557 rho -1.4228005e-05 lambda: 0.01\n",
      "    Loss_i: 0.14377607 Loss_f: 0.14398691 rho -3.2957448e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14377607 Loss_f: 0.14388534 rho -1.7434712e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14377607 Loss_f: 0.14379625 rho -3.2259115e-09 lambda: 9.999999\n",
      "    Train error: 20.048931\n",
      "-- Epoch: 27  Batch: 17 --\n",
      "    Loss_i: 0.14220501 Loss_f: 0.14351673 rho -0.0001768598 lambda: 0.01\n",
      "    Loss_i: 0.14220501 Loss_f: 0.14360154 rho -2.1867498e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14220501 Loss_f: 0.1425787 rho -5.947424e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14220501 Loss_f: 0.14211234 rho 1.4773023e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14211234 Loss_f: 0.14213088 rho -2.9548384e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14211234 Loss_f: 0.1421309 rho -2.9572136e-09 lambda: 10.0\n",
      "    Train error: 19.403246\n",
      "-- Epoch: 27  Batch: 18 --\n",
      "    Loss_i: 0.14435405 Loss_f: 0.14474119 rho -4.740394e-05 lambda: 0.01\n",
      "    Loss_i: 0.14435405 Loss_f: 0.14494929 rho -8.777424e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14435405 Loss_f: 0.14454141 rho -2.8205372e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14435405 Loss_f: 0.1443272 rho 4.050606e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1443272 Loss_f: 0.14434458 rho -2.6232663e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1443272 Loss_f: 0.14434458 rho -2.623266e-09 lambda: 10.0\n",
      "    Train error: 19.922672\n",
      "-- Epoch: 27  Batch: 19 --\n",
      "    Loss_i: 0.14623676 Loss_f: 0.14630766 rho -1.0018265e-05 lambda: 0.01\n",
      "    Loss_i: 0.14623676 Loss_f: 0.14676456 rho -9.256017e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14623676 Loss_f: 0.14642026 rho -3.2974162e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14623676 Loss_f: 0.14620616 rho 5.5137828e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14620616 Loss_f: 0.14622219 rho -2.888389e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14620616 Loss_f: 0.14622219 rho -2.888389e-09 lambda: 10.0\n",
      "    Train error: 20.255547\n",
      "-- Epoch: 27  Batch: 20 --\n",
      "    Loss_i: 0.1426011 Loss_f: 0.14279316 rho -2.1443035e-05 lambda: 0.01\n",
      "    Loss_i: 0.1426011 Loss_f: 0.14310034 rho -6.9940297e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1426011 Loss_f: 0.14279638 rho -2.8073086e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1426011 Loss_f: 0.14258471 rho 2.3625601e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14258471 Loss_f: 0.14260827 rho -3.3955503e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14258471 Loss_f: 0.14260827 rho -3.39555e-09 lambda: 10.0\n",
      "    Train error: 19.662176\n",
      "-- Epoch: 27  Batch: 21 --\n",
      "    Loss_i: 0.14401796 Loss_f: 0.14360854 rho 4.9350067e-05 lambda: 0.01\n",
      "    Loss_i: 0.14360854 Loss_f: 0.14376366 rho -1.8543571e-05 lambda: 0.01\n",
      "    Loss_i: 0.14360854 Loss_f: 0.14392544 rho -4.3132513e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14360854 Loss_f: 0.14376225 rho -2.1214238e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14360854 Loss_f: 0.1436336 rho -3.4641356e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14360854 Loss_f: 0.1436336 rho -3.4641348e-09 lambda: 10.0\n",
      "    Train error: 19.836273\n",
      "-- Epoch: 27  Batch: 22 --\n",
      "    Loss_i: 0.14153269 Loss_f: 0.14171153 rho -1.3999087e-05 lambda: 0.01\n",
      "    Loss_i: 0.14153269 Loss_f: 0.1419636 rho -5.708756e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14153269 Loss_f: 0.14165202 rho -1.6984941e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14153269 Loss_f: 0.1414982 rho 4.9469113e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1414982 Loss_f: 0.14150378 rho -8.013239e-10 lambda: 9.999999\n",
      "    Loss_i: 0.1414982 Loss_f: 0.14150378 rho -8.0132373e-10 lambda: 10.0\n",
      "    Train error: 19.381495\n",
      "-- Epoch: 27  Batch: 23 --\n",
      "    Loss_i: 0.14650042 Loss_f: 0.14618984 rho 3.951394e-05 lambda: 0.01\n",
      "    Loss_i: 0.14618984 Loss_f: 0.14653772 rho -4.399332e-05 lambda: 0.01\n",
      "    Loss_i: 0.14618984 Loss_f: 0.14666614 rho -7.501022e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14618984 Loss_f: 0.14641307 rho -3.6040234e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14618984 Loss_f: 0.14622448 rho -5.607424e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14618984 Loss_f: 0.14622448 rho -5.6074234e-09 lambda: 10.0\n",
      "    Train error: 20.235449\n",
      "-- Epoch: 27  Batch: 24 --\n",
      "    Loss_i: 0.14612472 Loss_f: 0.14638297 rho -3.1599382e-05 lambda: 0.01\n",
      "    Loss_i: 0.14612472 Loss_f: 0.14655022 rho -6.5425547e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14612472 Loss_f: 0.14615208 rho -4.317467e-08 lambda: 0.99999994\n",
      "    Loss_i: 0.14612472 Loss_f: 0.1460561 rho 1.0857538e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1460561 Loss_f: 0.14603399 rho 3.4988883e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14603399 Loss_f: 0.1460564 rho -3.5459955e-09 lambda: 9.999999\n",
      "    Train error: 20.369982\n",
      "-- Epoch: 27  Batch: 25 --\n",
      "    Loss_i: 0.14691037 Loss_f: 0.14711292 rho -2.4645176e-05 lambda: 0.01\n",
      "    Loss_i: 0.14691037 Loss_f: 0.14733602 rho -6.294129e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14691037 Loss_f: 0.1470627 rho -2.3021448e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14691037 Loss_f: 0.14688975 rho 3.1235388e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14688975 Loss_f: 0.14690724 rho -2.649497e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14688975 Loss_f: 0.14690724 rho -2.6494966e-09 lambda: 10.0\n",
      "    Train error: 20.51145\n",
      "-- Epoch: 27  Batch: 26 --\n",
      "    Loss_i: 0.14290449 Loss_f: 0.14283082 rho 8.702345e-06 lambda: 0.01\n",
      "    Loss_i: 0.14283082 Loss_f: 0.1438128 rho -0.00011645006 lambda: 0.01\n",
      "    Loss_i: 0.14283082 Loss_f: 0.14390907 rho -1.4488172e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14283082 Loss_f: 0.1434331 rho -8.201785e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14283082 Loss_f: 0.14293912 rho -1.4768446e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14283082 Loss_f: 0.14293914 rho -1.47704755e-08 lambda: 10.0\n",
      "    Train error: 19.777552\n",
      "-- Epoch: 27  Batch: 27 --\n",
      "    Loss_i: 0.14246428 Loss_f: 0.16237345 rho -0.0023542598 lambda: 0.01\n",
      "    Loss_i: 0.14246428 Loss_f: 0.16006795 rho -0.00030124298 lambda: 0.099999994\n",
      "    Loss_i: 0.14246428 Loss_f: 0.14919461 rho -1.2056386e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14246428 Loss_f: 0.14220133 rho 4.7324374e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14220133 Loss_f: 0.14308812 rho -1.5959088e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14220133 Loss_f: 0.14308812 rho -1.5959085e-07 lambda: 10.0\n",
      "    Train error: 19.49195\n",
      "-- Epoch: 27  Batch: 28 --\n",
      "    Loss_i: 0.14094995 Loss_f: 0.14548478 rho -0.0005589767 lambda: 0.01\n",
      "    Loss_i: 0.14094995 Loss_f: 0.14518908 rho -6.181614e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14094995 Loss_f: 0.14257023 rho -2.4067829e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14094995 Loss_f: 0.14090441 rho 6.7769053e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14090441 Loss_f: 0.14112127 rho -3.2275242e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14090441 Loss_f: 0.14112127 rho -3.2275235e-08 lambda: 10.0\n",
      "    Train error: 19.228022\n",
      "-- Epoch: 27  Batch: 29 --\n",
      "    Loss_i: 0.1429877 Loss_f: 0.14516453 rho -0.00022054142 lambda: 0.01\n",
      "    Loss_i: 0.1429877 Loss_f: 0.1451267 rho -2.943589e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1429877 Loss_f: 0.14380348 rho -1.1643558e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1429877 Loss_f: 0.14296292 rho 3.5501164e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14296292 Loss_f: 0.14307205 rho -1.5632702e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14296292 Loss_f: 0.14307205 rho -1.56327e-08 lambda: 10.0\n",
      "    Train error: 19.666142\n",
      "-- Epoch: 27  Batch: 30 --\n",
      "    Loss_i: 0.14376178 Loss_f: 0.14549793 rho -0.000240793 lambda: 0.01\n",
      "    Loss_i: 0.14376178 Loss_f: 0.14561218 rho -3.0947416e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14376178 Loss_f: 0.14447746 rho -1.2221049e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14376178 Loss_f: 0.14373378 rho 4.7913136e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14373378 Loss_f: 0.1438294 rho -1.6363812e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14373378 Loss_f: 0.1438294 rho -1.6363808e-08 lambda: 10.0\n",
      "    Train error: 19.569801\n",
      "\n",
      "*** Epoch: 27 Train error: 19.83885014851888  Test error: 19.403982  Time: 709.4550710000003 sec\n",
      "\n",
      "-- Epoch: 28  Batch: 1 --\n",
      "    Loss_i: 0.14040887 Loss_f: 0.14136198 rho -9.144332e-05 lambda: 0.01\n",
      "    Loss_i: 0.14040887 Loss_f: 0.14147392 rho -1.3270171e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14040887 Loss_f: 0.14080578 rho -5.0976024e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14040887 Loss_f: 0.14039092 rho 2.3132538e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14039092 Loss_f: 0.1404411 rho -6.465442e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14039092 Loss_f: 0.1404411 rho -6.465441e-09 lambda: 10.0\n",
      "    Train error: 18.945961\n",
      "-- Epoch: 28  Batch: 2 --\n",
      "    Loss_i: 0.14262426 Loss_f: 0.14349228 rho -8.555162e-05 lambda: 0.01\n",
      "    Loss_i: 0.14262426 Loss_f: 0.14361627 rho -1.2918721e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14262426 Loss_f: 0.14301142 rho -5.2092764e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14262426 Loss_f: 0.14260997 rho 1.9291568e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14260997 Loss_f: 0.142663 rho -7.1595827e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14260997 Loss_f: 0.142663 rho -7.1595823e-09 lambda: 10.0\n",
      "    Train error: 19.526958\n",
      "-- Epoch: 28  Batch: 3 --\n",
      "    Loss_i: 0.14424212 Loss_f: 0.14570901 rho -0.00015522962 lambda: 0.01\n",
      "    Loss_i: 0.14424212 Loss_f: 0.14583687 rho -2.347217e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14424212 Loss_f: 0.14486544 rho -9.547357e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14424212 Loss_f: 0.14421959 rho 3.4651115e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14421959 Loss_f: 0.1443033 rho -1.2872831e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14421959 Loss_f: 0.1443033 rho -1.2872829e-08 lambda: 10.0\n",
      "    Train error: 19.741383\n",
      "-- Epoch: 28  Batch: 4 --\n",
      "    Loss_i: 0.14010723 Loss_f: 0.14033203 rho -2.3457842e-05 lambda: 0.01\n",
      "    Loss_i: 0.14010723 Loss_f: 0.14054102 rho -5.6881913e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14010723 Loss_f: 0.14027983 rho -2.3228888e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14010723 Loss_f: 0.14009735 rho 1.3331108e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14009735 Loss_f: 0.14011998 rho -3.0542668e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14009735 Loss_f: 0.14011998 rho -3.0542664e-09 lambda: 10.0\n",
      "    Train error: 19.129194\n",
      "-- Epoch: 28  Batch: 5 --\n",
      "    Loss_i: 0.1433355 Loss_f: 0.14297935 rho 3.962484e-05 lambda: 0.01\n",
      "    Loss_i: 0.14297935 Loss_f: 0.14290962 rho 7.688655e-06 lambda: 0.01\n",
      "    Loss_i: 0.14290962 Loss_f: 0.14312765 rho -2.381985e-05 lambda: 0.01\n",
      "    Loss_i: 0.14290962 Loss_f: 0.14323324 rho -4.055014e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14290962 Loss_f: 0.14309724 rho -2.3859496e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14290962 Loss_f: 0.14294703 rho -4.765362e-09 lambda: 9.999999\n",
      "    Train error: 19.65325\n",
      "-- Epoch: 28  Batch: 6 --\n",
      "    Loss_i: 0.13787763 Loss_f: 0.14714792 rho -0.00085537956 lambda: 0.01\n",
      "    Loss_i: 0.13787763 Loss_f: 0.14614387 rho -0.0001152403 lambda: 0.099999994\n",
      "    Loss_i: 0.13787763 Loss_f: 0.1409481 rho -4.511033e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.13787763 Loss_f: 0.1377289 rho 2.1968944e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1377289 Loss_f: 0.13813165 rho -5.9493317e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1377289 Loss_f: 0.13813163 rho -5.9491118e-08 lambda: 10.0\n",
      "    Train error: 18.598953\n",
      "-- Epoch: 28  Batch: 7 --\n",
      "    Loss_i: 0.14607342 Loss_f: 0.14892486 rho -0.00027839647 lambda: 0.01\n",
      "    Loss_i: 0.14607342 Loss_f: 0.14884518 rho -4.511354e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14607342 Loss_f: 0.14705198 rho -1.7065504e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14607342 Loss_f: 0.14600176 rho 1.25869954e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14600176 Loss_f: 0.14612082 rho -2.0912001e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14600176 Loss_f: 0.14612082 rho -2.0912e-08 lambda: 10.0\n",
      "    Train error: 20.12339\n",
      "-- Epoch: 28  Batch: 8 --\n",
      "    Loss_i: 0.146324 Loss_f: 0.1470221 rho -0.0001056528 lambda: 0.01\n",
      "    Loss_i: 0.146324 Loss_f: 0.14727741 rho -1.7301147e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.146324 Loss_f: 0.14666775 rho -6.364585e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.146324 Loss_f: 0.14628959 rho 6.3833405e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14628959 Loss_f: 0.14633171 rho -7.815909e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14628959 Loss_f: 0.14633171 rho -7.815908e-09 lambda: 10.0\n",
      "    Train error: 20.599308\n",
      "-- Epoch: 28  Batch: 9 --\n",
      "    Loss_i: 0.14333548 Loss_f: 0.14415449 rho -9.883079e-05 lambda: 0.01\n",
      "    Loss_i: 0.14333548 Loss_f: 0.14430925 rho -1.5024029e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14333548 Loss_f: 0.14370394 rho -5.8477536e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14333548 Loss_f: 0.14331155 rho 3.808985e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14331155 Loss_f: 0.14335945 rho -7.624791e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14331155 Loss_f: 0.14335945 rho -7.62479e-09 lambda: 10.0\n",
      "    Train error: 19.826761\n",
      "-- Epoch: 28  Batch: 10 --\n",
      "    Loss_i: 0.14261302 Loss_f: 0.14274846 rho -1.4474335e-05 lambda: 0.01\n",
      "    Loss_i: 0.14261302 Loss_f: 0.14299396 rho -5.3595795e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14261302 Loss_f: 0.14275305 rho -2.0345041e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14261302 Loss_f: 0.1425983 rho 2.1460909e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1425983 Loss_f: 0.14261591 rho -2.567495e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1425983 Loss_f: 0.14261591 rho -2.5674949e-09 lambda: 10.0\n",
      "    Train error: 19.64016\n",
      "-- Epoch: 28  Batch: 11 --\n",
      "    Loss_i: 0.14228158 Loss_f: 0.14191352 rho 3.842976e-05 lambda: 0.01\n",
      "    Loss_i: 0.14191352 Loss_f: 0.14235513 rho -4.5622837e-05 lambda: 0.01\n",
      "    Loss_i: 0.14191352 Loss_f: 0.14248314 rho -7.335264e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14191352 Loss_f: 0.1421916 rho -3.671486e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14191352 Loss_f: 0.14195815 rho -5.9071694e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14191352 Loss_f: 0.14195815 rho -5.9071694e-09 lambda: 10.0\n",
      "    Train error: 19.393068\n",
      "-- Epoch: 28  Batch: 12 --\n",
      "    Loss_i: 0.14680327 Loss_f: 0.14939708 rho -0.00035191214 lambda: 0.01\n",
      "    Loss_i: 0.14680327 Loss_f: 0.14927591 rho -4.232157e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14680327 Loss_f: 0.14761057 rho -1.418882e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14680327 Loss_f: 0.14671253 rho 1.5992502e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14671253 Loss_f: 0.14679487 rho -1.4511639e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14671253 Loss_f: 0.14679487 rho -1.4511636e-08 lambda: 10.0\n",
      "    Train error: 20.452536\n",
      "-- Epoch: 28  Batch: 13 --\n",
      "    Loss_i: 0.13866785 Loss_f: 0.13897236 rho -3.267303e-05 lambda: 0.01\n",
      "    Loss_i: 0.13866785 Loss_f: 0.13918266 rho -6.6138387e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.13866785 Loss_f: 0.13885278 rho -2.4235854e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13866785 Loss_f: 0.1386431 rho 3.2503598e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1386431 Loss_f: 0.13866438 rho -2.7943168e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1386431 Loss_f: 0.13866438 rho -2.7943163e-09 lambda: 10.0\n",
      "    Train error: 18.91373\n",
      "-- Epoch: 28  Batch: 14 --\n",
      "    Loss_i: 0.14302233 Loss_f: 0.14274317 rho 3.771301e-05 lambda: 0.01\n",
      "    Loss_i: 0.14274317 Loss_f: 0.14334236 rho -8.1186605e-05 lambda: 0.01\n",
      "    Loss_i: 0.14274317 Loss_f: 0.14346485 rho -1.1531287e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14274317 Loss_f: 0.143125 rho -6.212377e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14274317 Loss_f: 0.1428094 rho -1.0796329e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14274317 Loss_f: 0.1428094 rho -1.0796327e-08 lambda: 10.0\n",
      "    Train error: 19.72918\n",
      "-- Epoch: 28  Batch: 15 --\n",
      "    Loss_i: 0.14129841 Loss_f: 0.1526837 rho -0.0012701445 lambda: 0.01\n",
      "    Loss_i: 0.14129841 Loss_f: 0.15149204 rho -0.00015217719 lambda: 0.099999994\n",
      "    Loss_i: 0.14129841 Loss_f: 0.14526916 rho -6.135263e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14129841 Loss_f: 0.14120673 rho 1.4216449e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14120673 Loss_f: 0.14176136 rho -8.597283e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14120673 Loss_f: 0.14176136 rho -8.5972815e-08 lambda: 10.0\n",
      "    Train error: 19.162773\n",
      "-- Epoch: 28  Batch: 16 --\n",
      "    Loss_i: 0.14324297 Loss_f: 0.1489881 rho -0.0007300443 lambda: 0.01\n",
      "    Loss_i: 0.14324297 Loss_f: 0.14840928 rho -9.566857e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14324297 Loss_f: 0.14518282 rho -3.7642992e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14324297 Loss_f: 0.14317618 rho 1.3022499e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14317618 Loss_f: 0.14343964 rho -5.1371323e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14317618 Loss_f: 0.14343964 rho -5.1371323e-08 lambda: 10.0\n",
      "    Train error: 19.870182\n",
      "-- Epoch: 28  Batch: 17 --\n",
      "    Loss_i: 0.14113532 Loss_f: 0.14278251 rho -0.00018140952 lambda: 0.01\n",
      "    Loss_i: 0.14113532 Loss_f: 0.14284205 rho -2.5333404e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14113532 Loss_f: 0.14177671 rho -9.863295e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14113532 Loss_f: 0.14110306 rho 4.9790305e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14110306 Loss_f: 0.14118758 rho -1.30432865e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14110306 Loss_f: 0.14118758 rho -1.3043284e-08 lambda: 10.0\n",
      "    Train error: 19.137821\n",
      "-- Epoch: 28  Batch: 18 --\n",
      "    Loss_i: 0.14347105 Loss_f: 0.1440295 rho -6.914639e-05 lambda: 0.01\n",
      "    Loss_i: 0.14347105 Loss_f: 0.14423218 rho -1.1447342e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14347105 Loss_f: 0.14377633 rho -4.6920783e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14347105 Loss_f: 0.14345472 rho 2.5156555e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14345472 Loss_f: 0.14349626 rho -6.399101e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14345472 Loss_f: 0.14349627 rho -6.4013945e-09 lambda: 10.0\n",
      "    Train error: 19.68677\n",
      "-- Epoch: 28  Batch: 19 --\n",
      "    Loss_i: 0.1448693 Loss_f: 0.14571446 rho -8.94617e-05 lambda: 0.01\n",
      "    Loss_i: 0.1448693 Loss_f: 0.1458895 rho -1.5866493e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1448693 Loss_f: 0.14526232 rho -6.413246e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1448693 Loss_f: 0.14484699 rho 3.65806e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14484699 Loss_f: 0.1448985 rho -8.447246e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14484699 Loss_f: 0.14489849 rho -8.444801e-09 lambda: 10.0\n",
      "    Train error: 20.0107\n",
      "-- Epoch: 28  Batch: 20 --\n",
      "    Loss_i: 0.14151756 Loss_f: 0.14241005 rho -8.49645e-05 lambda: 0.01\n",
      "    Loss_i: 0.14151756 Loss_f: 0.14253378 rho -1.3457115e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14151756 Loss_f: 0.14190917 rho -5.3967796e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14151756 Loss_f: 0.14150104 rho 2.286713e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14150104 Loss_f: 0.14155333 rho -7.2347417e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14150104 Loss_f: 0.14155333 rho -7.2347395e-09 lambda: 10.0\n",
      "    Train error: 19.37951\n",
      "-- Epoch: 28  Batch: 21 --\n",
      "    Loss_i: 0.14318888 Loss_f: 0.14280817 rho 4.3721055e-05 lambda: 0.01\n",
      "    Loss_i: 0.14280817 Loss_f: 0.14300323 rho -2.2179434e-05 lambda: 0.01\n",
      "    Loss_i: 0.14280817 Loss_f: 0.14317979 rho -4.8517586e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14280817 Loss_f: 0.1429994 rho -2.5341458e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14280817 Loss_f: 0.14284007 rho -4.2342245e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14280817 Loss_f: 0.14284007 rho -4.2342245e-09 lambda: 10.0\n",
      "    Train error: 19.61213\n",
      "-- Epoch: 28  Batch: 22 --\n",
      "    Loss_i: 0.14057635 Loss_f: 0.14311826 rho -0.00029735744 lambda: 0.01\n",
      "    Loss_i: 0.14057635 Loss_f: 0.14290804 rho -3.1824427e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14057635 Loss_f: 0.1414105 rho -1.1578104e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14057635 Loss_f: 0.14052963 rho 6.4951156e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14052963 Loss_f: 0.1406363 rho -1.4830511e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14052963 Loss_f: 0.14063628 rho -1.4828439e-08 lambda: 10.0\n",
      "    Train error: 19.143957\n",
      "-- Epoch: 28  Batch: 23 --\n",
      "    Loss_i: 0.14561455 Loss_f: 0.14570306 rho -9.965272e-06 lambda: 0.01\n",
      "    Loss_i: 0.14561455 Loss_f: 0.14593053 rho -4.324804e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14561455 Loss_f: 0.14572436 rho -1.5360561e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14561455 Loss_f: 0.14559935 rho 2.1308704e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14559935 Loss_f: 0.14561126 rho -1.6691937e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14559935 Loss_f: 0.14561126 rho -1.6691935e-09 lambda: 10.0\n",
      "    Train error: 19.994879\n",
      "-- Epoch: 28  Batch: 24 --\n",
      "    Loss_i: 0.14518358 Loss_f: 0.14492378 rho 3.0540705e-05 lambda: 0.01\n",
      "    Loss_i: 0.14492378 Loss_f: 0.14513887 rho -2.5190639e-05 lambda: 0.01\n",
      "    Loss_i: 0.14492378 Loss_f: 0.14527193 rho -4.6725995e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14492378 Loss_f: 0.14509225 rho -2.2946068e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14492378 Loss_f: 0.14495005 rho -3.5834027e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14492378 Loss_f: 0.14495005 rho -3.5834025e-09 lambda: 10.0\n",
      "    Train error: 20.072731\n",
      "-- Epoch: 28  Batch: 25 --\n",
      "    Loss_i: 0.14599977 Loss_f: 0.14688468 rho -0.0001068412 lambda: 0.01\n",
      "    Loss_i: 0.14599977 Loss_f: 0.14697096 rho -1.4501415e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14599977 Loss_f: 0.14633733 rho -5.1624846e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14599977 Loss_f: 0.1459601 rho 6.081285e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1459601 Loss_f: 0.14599729 rho -5.7001546e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1459601 Loss_f: 0.14599729 rho -5.700154e-09 lambda: 10.0\n",
      "    Train error: 20.293913\n",
      "-- Epoch: 28  Batch: 26 --\n",
      "    Loss_i: 0.14189574 Loss_f: 0.14194484 rho -5.1479583e-06 lambda: 0.01\n",
      "    Loss_i: 0.14189574 Loss_f: 0.14214158 rho -3.1170732e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14189574 Loss_f: 0.14196889 rho -9.473167e-08 lambda: 0.99999994\n",
      "    Loss_i: 0.14189574 Loss_f: 0.14187977 rho 2.0731297e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14187977 Loss_f: 0.141886 rho -8.083899e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14187977 Loss_f: 0.141886 rho -8.083898e-10 lambda: 10.0\n",
      "    Train error: 19.448195\n",
      "-- Epoch: 28  Batch: 27 --\n",
      "    Loss_i: 0.14149834 Loss_f: 0.14134279 rho 1.6268823e-05 lambda: 0.01\n",
      "    Loss_i: 0.14134279 Loss_f: 0.14206193 rho -7.4347256e-05 lambda: 0.01\n",
      "    Loss_i: 0.14134279 Loss_f: 0.14217357 rho -1.1047193e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14134279 Loss_f: 0.14180832 rho -6.3726577e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14134279 Loss_f: 0.14142653 rho -1.1497766e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14134279 Loss_f: 0.14142653 rho -1.14977645e-08 lambda: 10.0\n",
      "    Train error: 19.262602\n",
      "-- Epoch: 28  Batch: 28 --\n",
      "    Loss_i: 0.14022519 Loss_f: 0.1520618 rho -0.0015058176 lambda: 0.01\n",
      "    Loss_i: 0.14022519 Loss_f: 0.15079632 rho -0.0001603423 lambda: 0.099999994\n",
      "    Loss_i: 0.14022519 Loss_f: 0.14419019 rho -6.1320047e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14022519 Loss_f: 0.14006545 rho 2.4752891e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14006545 Loss_f: 0.14058273 rho -8.014795e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14006545 Loss_f: 0.14058273 rho -8.0147935e-08 lambda: 10.0\n",
      "    Train error: 19.027382\n",
      "-- Epoch: 28  Batch: 29 --\n",
      "    Loss_i: 0.14220224 Loss_f: 0.14665464 rho -0.00055074546 lambda: 0.01\n",
      "    Loss_i: 0.14220224 Loss_f: 0.1463296 rho -5.845276e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14220224 Loss_f: 0.14376575 rho -2.2468425e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14220224 Loss_f: 0.1421529 rho 7.1005464e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1421529 Loss_f: 0.14236023 rho -2.982894e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1421529 Loss_f: 0.14236023 rho -2.9828936e-08 lambda: 10.0\n",
      "    Train error: 19.46143\n",
      "-- Epoch: 28  Batch: 30 --\n",
      "    Loss_i: 0.14295934 Loss_f: 0.1459435 rho -0.0004216586 lambda: 0.01\n",
      "    Loss_i: 0.14295934 Loss_f: 0.14589582 rho -5.028522e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14295934 Loss_f: 0.14406121 rho -1.927721e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14295934 Loss_f: 0.14291517 rho 7.743818e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14291517 Loss_f: 0.14305629 rho -2.4741647e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14291517 Loss_f: 0.14305629 rho -2.4741647e-08 lambda: 10.0\n",
      "    Train error: 19.353642\n",
      "\n",
      "*** Epoch: 28 Train error: 19.57308158874512  Test error: 19.186104  Time: 712.5693647000007 sec\n",
      "\n",
      "-- Epoch: 29  Batch: 1 --\n",
      "    Loss_i: 0.1396301 Loss_f: 0.14115709 rho -0.00015619074 lambda: 0.01\n",
      "    Loss_i: 0.1396301 Loss_f: 0.14120372 rho -1.9888925e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1396301 Loss_f: 0.14023335 rho -7.808555e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1396301 Loss_f: 0.13960749 rho 2.9330682e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13960749 Loss_f: 0.13968813 rho -1.0463463e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13960749 Loss_f: 0.13968815 rho -1.0465394e-08 lambda: 10.0\n",
      "    Train error: 18.736393\n",
      "-- Epoch: 29  Batch: 2 --\n",
      "    Loss_i: 0.14177896 Loss_f: 0.14336163 rho -0.0001564988 lambda: 0.01\n",
      "    Loss_i: 0.14177896 Loss_f: 0.14341651 rho -2.0985852e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14177896 Loss_f: 0.14244181 rho -8.753804e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14177896 Loss_f: 0.14176416 rho 1.9601034e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14176416 Loss_f: 0.14185983 rho -1.2672976e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14176416 Loss_f: 0.14185984 rho -1.2674948e-08 lambda: 10.0\n",
      "    Train error: 19.30805\n",
      "-- Epoch: 29  Batch: 3 --\n",
      "    Loss_i: 0.14334936 Loss_f: 0.14512281 rho -0.00022085533 lambda: 0.01\n",
      "    Loss_i: 0.14334936 Loss_f: 0.14517769 rho -2.6412306e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14334936 Loss_f: 0.14404179 rho -1.0165576e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14334936 Loss_f: 0.14331369 rho 5.245765e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14331369 Loss_f: 0.1434042 rho -1.3307692e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14331369 Loss_f: 0.1434042 rho -1.3307691e-08 lambda: 10.0\n",
      "    Train error: 19.50671\n",
      "-- Epoch: 29  Batch: 4 --\n",
      "    Loss_i: 0.13929066 Loss_f: 0.13945964 rho -1.6894428e-05 lambda: 0.01\n",
      "    Loss_i: 0.13929066 Loss_f: 0.13966857 rho -4.689948e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.13929066 Loss_f: 0.1394395 rho -1.8927255e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13929066 Loss_f: 0.13928103 rho 1.2272043e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13928103 Loss_f: 0.13930102 rho -2.5474212e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13928103 Loss_f: 0.13930102 rho -2.5474207e-09 lambda: 10.0\n",
      "    Train error: 18.914143\n",
      "-- Epoch: 29  Batch: 5 --\n",
      "    Loss_i: 0.14229362 Loss_f: 0.14202973 rho 2.718716e-05 lambda: 0.01\n",
      "    Loss_i: 0.14202973 Loss_f: 0.14209704 rho -6.86758e-06 lambda: 0.01\n",
      "    Loss_i: 0.14202973 Loss_f: 0.14225453 rho -2.5853983e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14202973 Loss_f: 0.14215496 rho -1.4588102e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14202973 Loss_f: 0.14205156 rho -2.5463045e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14202973 Loss_f: 0.14205156 rho -2.5463045e-09 lambda: 10.0\n",
      "    Train error: 19.418753\n",
      "-- Epoch: 29  Batch: 6 --\n",
      "    Loss_i: 0.13692151 Loss_f: 0.14001514 rho -0.00030204307 lambda: 0.01\n",
      "    Loss_i: 0.13692151 Loss_f: 0.1398844 rho -3.710082e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13692151 Loss_f: 0.1380411 rho -1.4426867e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.13692151 Loss_f: 0.1368674 rho 6.9924004e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1368674 Loss_f: 0.13701592 rho -1.919389e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1368674 Loss_f: 0.13701592 rho -1.9193886e-08 lambda: 10.0\n",
      "    Train error: 18.352089\n",
      "-- Epoch: 29  Batch: 7 --\n",
      "    Loss_i: 0.14517686 Loss_f: 0.14601924 rho -9.086212e-05 lambda: 0.01\n",
      "    Loss_i: 0.14517686 Loss_f: 0.14624043 rho -1.519024e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14517686 Loss_f: 0.14558955 rho -6.0915636e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14517686 Loss_f: 0.14515112 rho 3.8113357e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14515112 Loss_f: 0.14520657 rho -8.211321e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14515112 Loss_f: 0.14520657 rho -8.21132e-09 lambda: 10.0\n",
      "    Train error: 19.905243\n",
      "-- Epoch: 29  Batch: 8 --\n",
      "    Loss_i: 0.14541528 Loss_f: 0.14589478 rho -5.9188296e-05 lambda: 0.01\n",
      "    Loss_i: 0.14541528 Loss_f: 0.14611866 rho -1.1142607e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14541528 Loss_f: 0.14567424 rho -4.222081e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14541528 Loss_f: 0.14540005 rho 2.4901248e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14540005 Loss_f: 0.14543274 rho -5.345722e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14540005 Loss_f: 0.14543274 rho -5.345722e-09 lambda: 10.0\n",
      "    Train error: 20.363855\n",
      "-- Epoch: 29  Batch: 9 --\n",
      "    Loss_i: 0.14251617 Loss_f: 0.14285669 rho -4.1020034e-05 lambda: 0.01\n",
      "    Loss_i: 0.14251617 Loss_f: 0.1430467 rho -8.0053005e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14251617 Loss_f: 0.1427266 rho -3.2575082e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14251617 Loss_f: 0.1425036 rho 1.949592e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1425036 Loss_f: 0.14253113 rho -4.2712354e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1425036 Loss_f: 0.14253113 rho -4.2712354e-09 lambda: 10.0\n",
      "    Train error: 19.615088\n",
      "-- Epoch: 29  Batch: 10 --\n",
      "    Loss_i: 0.14177524 Loss_f: 0.1416704 rho 1.1033258e-05 lambda: 0.01\n",
      "    Loss_i: 0.1416704 Loss_f: 0.1423541 rho -7.232658e-05 lambda: 0.01\n",
      "    Loss_i: 0.1416704 Loss_f: 0.1424769 rho -9.689247e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1416704 Loss_f: 0.14214389 rho -5.766686e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1416704 Loss_f: 0.1417594 rho -1.0853213e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1416704 Loss_f: 0.1417594 rho -1.0853213e-08 lambda: 10.0\n",
      "    Train error: 19.492733\n",
      "-- Epoch: 29  Batch: 11 --\n",
      "    Loss_i: 0.1414509 Loss_f: 0.15729631 rho -0.001570478 lambda: 0.01\n",
      "    Loss_i: 0.1414509 Loss_f: 0.15563421 rho -0.00018621322 lambda: 0.099999994\n",
      "    Loss_i: 0.1414509 Loss_f: 0.1469905 rho -7.517008e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1414509 Loss_f: 0.14130718 rho 1.956812e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14130718 Loss_f: 0.14206916 rho -1.0372487e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14130718 Loss_f: 0.14206916 rho -1.0372487e-07 lambda: 10.0\n",
      "    Train error: 19.240984\n",
      "-- Epoch: 29  Batch: 12 --\n",
      "    Loss_i: 0.14597683 Loss_f: 0.15197895 rho -0.0006802702 lambda: 0.01\n",
      "    Loss_i: 0.14597683 Loss_f: 0.15145794 rho -8.811288e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14597683 Loss_f: 0.14805663 rho -3.4894233e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14597683 Loss_f: 0.14592066 rho 9.4640935e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14592066 Loss_f: 0.1461993 rho -4.6954195e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14592066 Loss_f: 0.1461993 rho -4.6954188e-08 lambda: 10.0\n",
      "    Train error: 20.265062\n",
      "-- Epoch: 29  Batch: 13 --\n",
      "    Loss_i: 0.13793474 Loss_f: 0.13996951 rho -0.00022169249 lambda: 0.01\n",
      "    Loss_i: 0.13793474 Loss_f: 0.13994135 rho -2.5450125e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13793474 Loss_f: 0.13869739 rho -9.834098e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13793474 Loss_f: 0.1379036 rho 4.02259e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1379036 Loss_f: 0.13800475 rho -1.3063963e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1379036 Loss_f: 0.13800475 rho -1.30639615e-08 lambda: 10.0\n",
      "    Train error: 18.712534\n",
      "-- Epoch: 29  Batch: 14 --\n",
      "    Loss_i: 0.14221177 Loss_f: 0.14226523 rho -6.551681e-06 lambda: 0.01\n",
      "    Loss_i: 0.14221177 Loss_f: 0.14251086 rho -4.2570164e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14221177 Loss_f: 0.14234103 rho -1.8700558e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14221177 Loss_f: 0.14220622 rho 8.03233e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14220622 Loss_f: 0.14222474 rho -2.6839693e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14220622 Loss_f: 0.14222474 rho -2.6839693e-09 lambda: 10.0\n",
      "    Train error: 19.524166\n",
      "-- Epoch: 29  Batch: 15 --\n",
      "    Loss_i: 0.14050157 Loss_f: 0.1405657 rho -6.7629453e-06 lambda: 0.01\n",
      "    Loss_i: 0.14050157 Loss_f: 0.14078233 rho -3.4051918e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14050157 Loss_f: 0.14061555 rho -1.4034703e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14050157 Loss_f: 0.140493 rho 1.0566421e-09 lambda: 9.999999\n",
      "    Loss_i: 0.140493 Loss_f: 0.14050724 rho -1.7549309e-09 lambda: 9.999999\n",
      "    Loss_i: 0.140493 Loss_f: 0.14050724 rho -1.7549305e-09 lambda: 10.0\n",
      "    Train error: 18.919632\n",
      "-- Epoch: 29  Batch: 16 --\n",
      "    Loss_i: 0.14259683 Loss_f: 0.14269014 rho -1.1156922e-05 lambda: 0.01\n",
      "    Loss_i: 0.14259683 Loss_f: 0.14294666 rho -4.8269917e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14259683 Loss_f: 0.14274755 rho -2.112222e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14259683 Loss_f: 0.1425891 rho 1.083386e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1425891 Loss_f: 0.1426102 rho -2.9615934e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1425891 Loss_f: 0.1426102 rho -2.961593e-09 lambda: 10.0\n",
      "    Train error: 19.685394\n",
      "-- Epoch: 29  Batch: 17 --\n",
      "    Loss_i: 0.1404705 Loss_f: 0.1404919 rho -2.299217e-06 lambda: 0.01\n",
      "    Loss_i: 0.1404705 Loss_f: 0.14072287 rho -3.0539895e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1404705 Loss_f: 0.14057481 rho -1.2784163e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1404705 Loss_f: 0.14046533 rho 6.345406e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14046533 Loss_f: 0.1404792 rho -1.7023742e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14046533 Loss_f: 0.1404792 rho -1.7023741e-09 lambda: 10.0\n",
      "    Train error: 18.905384\n",
      "-- Epoch: 29  Batch: 18 --\n",
      "    Loss_i: 0.14281614 Loss_f: 0.14283279 rho -1.8273838e-06 lambda: 0.01\n",
      "    Loss_i: 0.14281614 Loss_f: 0.14306904 rho -3.1696386e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14281614 Loss_f: 0.14292489 rho -1.3825235e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14281614 Loss_f: 0.14281127 rho 6.203554e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14281127 Loss_f: 0.1428261 rho -1.8877317e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14281127 Loss_f: 0.1428261 rho -1.8877317e-09 lambda: 10.0\n",
      "    Train error: 19.507547\n",
      "-- Epoch: 29  Batch: 19 --\n",
      "    Loss_i: 0.14420281 Loss_f: 0.14460373 rho -4.4390297e-05 lambda: 0.01\n",
      "    Loss_i: 0.14420281 Loss_f: 0.14480859 rho -8.697865e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14420281 Loss_f: 0.14443707 rho -3.4664447e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14420281 Loss_f: 0.14418988 rho 1.919792e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14418988 Loss_f: 0.14421986 rho -4.449632e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14418988 Loss_f: 0.14421986 rho -4.449632e-09 lambda: 10.0\n",
      "    Train error: 19.816008\n",
      "-- Epoch: 29  Batch: 20 --\n",
      "    Loss_i: 0.14093918 Loss_f: 0.14133145 rho -3.9371433e-05 lambda: 0.01\n",
      "    Loss_i: 0.14093918 Loss_f: 0.14151202 rho -7.295187e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14093918 Loss_f: 0.14116676 rho -2.9783712e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14093918 Loss_f: 0.14092901 rho 1.3336454e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14092901 Loss_f: 0.14095972 rho -4.030332e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14092901 Loss_f: 0.14095972 rho -4.0303316e-09 lambda: 10.0\n",
      "    Train error: 19.219313\n",
      "-- Epoch: 29  Batch: 21 --\n",
      "    Loss_i: 0.14246869 Loss_f: 0.14211789 rho 3.686969e-05 lambda: 0.01\n",
      "    Loss_i: 0.14211789 Loss_f: 0.14199883 rho 1.2444335e-05 lambda: 0.01\n",
      "    Loss_i: 0.14199883 Loss_f: 0.14211701 rho -1.22836855e-05 lambda: 0.01\n",
      "    Loss_i: 0.14199883 Loss_f: 0.14222571 rho -2.713135e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14199883 Loss_f: 0.14213434 rho -1.645228e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14199883 Loss_f: 0.14202523 rho -3.2106955e-09 lambda: 9.999999\n",
      "    Train error: 19.431305\n",
      "-- Epoch: 29  Batch: 22 --\n",
      "    Loss_i: 0.14017357 Loss_f: 0.14258443 rho -0.00023347895 lambda: 0.01\n",
      "    Loss_i: 0.14017357 Loss_f: 0.14247501 rho -3.33089e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14017357 Loss_f: 0.14079095 rho -9.400263e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14017357 Loss_f: 0.14004952 rho 1.8986878e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14004952 Loss_f: 0.14008433 rho -5.32814e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14004952 Loss_f: 0.14008433 rho -5.3281397e-09 lambda: 10.0\n",
      "    Train error: 19.024527\n",
      "-- Epoch: 29  Batch: 23 --\n",
      "    Loss_i: 0.14505567 Loss_f: 0.14523484 rho -1.7480183e-05 lambda: 0.01\n",
      "    Loss_i: 0.14505567 Loss_f: 0.14547673 rho -5.818406e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14505567 Loss_f: 0.14517981 rho -1.7899687e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14505567 Loss_f: 0.14502457 rho 4.503622e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14502457 Loss_f: 0.14503115 rho -9.537394e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14502457 Loss_f: 0.14503115 rho -9.537393e-10 lambda: 10.0\n",
      "    Train error: 19.855192\n",
      "-- Epoch: 29  Batch: 24 --\n",
      "    Loss_i: 0.14457719 Loss_f: 0.14425313 rho 3.778727e-05 lambda: 0.01\n",
      "    Loss_i: 0.14425313 Loss_f: 0.14454307 rho -3.3646193e-05 lambda: 0.01\n",
      "    Loss_i: 0.14425313 Loss_f: 0.1446864 rho -5.9212443e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14425313 Loss_f: 0.14447376 rho -3.0697333e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14425313 Loss_f: 0.14428805 rho -4.866555e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14425313 Loss_f: 0.14428805 rho -4.8665543e-09 lambda: 10.0\n",
      "    Train error: 19.892527\n",
      "-- Epoch: 29  Batch: 25 --\n",
      "    Loss_i: 0.14534427 Loss_f: 0.14623435 rho -0.0001037627 lambda: 0.01\n",
      "    Loss_i: 0.14534427 Loss_f: 0.14629836 rho -1.4916041e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14534427 Loss_f: 0.14559858 rho -4.1161027e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14534427 Loss_f: 0.1452846 rho 9.693779e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1452846 Loss_f: 0.14529751 rho -2.098484e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1452846 Loss_f: 0.14529751 rho -2.0984838e-09 lambda: 10.0\n",
      "    Train error: 20.11308\n",
      "-- Epoch: 29  Batch: 26 --\n",
      "    Loss_i: 0.14130715 Loss_f: 0.14145385 rho -1.5798765e-05 lambda: 0.01\n",
      "    Loss_i: 0.14130715 Loss_f: 0.1416537 rho -4.566816e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14130715 Loss_f: 0.14139971 rho -1.247711e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14130715 Loss_f: 0.141285 rho 2.9915477e-09 lambda: 9.999999\n",
      "    Loss_i: 0.141285 Loss_f: 0.14128909 rho -5.515927e-10 lambda: 9.999999\n",
      "    Loss_i: 0.141285 Loss_f: 0.14128909 rho -5.515926e-10 lambda: 10.0\n",
      "    Train error: 19.289885\n",
      "-- Epoch: 29  Batch: 27 --\n",
      "    Loss_i: 0.14084284 Loss_f: 0.14077346 rho 7.729635e-06 lambda: 0.01\n",
      "    Loss_i: 0.14077346 Loss_f: 0.14184748 rho -0.00011795815 lambda: 0.01\n",
      "    Loss_i: 0.14077346 Loss_f: 0.14194405 rho -1.6344415e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14077346 Loss_f: 0.14143977 rho -9.562749e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14077346 Loss_f: 0.14089352 rho -1.7278884e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14077346 Loss_f: 0.14089352 rho -1.727888e-08 lambda: 10.0\n",
      "    Train error: 19.101036\n",
      "-- Epoch: 29  Batch: 28 --\n",
      "    Loss_i: 0.13969749 Loss_f: 0.1591721 rho -0.0019450982 lambda: 0.01\n",
      "    Loss_i: 0.13969749 Loss_f: 0.15712208 rho -0.00025742545 lambda: 0.099999994\n",
      "    Loss_i: 0.13969749 Loss_f: 0.14654775 rho -1.0629696e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.13969749 Loss_f: 0.13950518 rho 2.9992805e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13950518 Loss_f: 0.14044471 rho -1.464998e-07 lambda: 9.999999\n",
      "    Loss_i: 0.13950518 Loss_f: 0.14044471 rho -1.4649977e-07 lambda: 10.0\n",
      "    Train error: 18.905882\n",
      "-- Epoch: 29  Batch: 29 --\n",
      "    Loss_i: 0.14164326 Loss_f: 0.15120083 rho -0.0009950087 lambda: 0.01\n",
      "    Loss_i: 0.14164326 Loss_f: 0.1503149 rho -0.00013089702 lambda: 0.099999994\n",
      "    Loss_i: 0.14164326 Loss_f: 0.1450104 rho -5.3221083e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14164326 Loss_f: 0.14155076 rho 1.4688424e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14155076 Loss_f: 0.14200768 rho -7.253345e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14155076 Loss_f: 0.14200768 rho -7.253345e-08 lambda: 10.0\n",
      "    Train error: 19.299507\n",
      "-- Epoch: 29  Batch: 30 --\n",
      "    Loss_i: 0.14229394 Loss_f: 0.14797959 rho -0.00070609996 lambda: 0.01\n",
      "    Loss_i: 0.14229394 Loss_f: 0.14761196 rho -8.9260444e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14229394 Loss_f: 0.14435853 rho -3.591562e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14229394 Loss_f: 0.14222474 rho 1.2082256e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14222474 Loss_f: 0.14250423 rho -4.88045e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14222474 Loss_f: 0.14250423 rho -4.88045e-08 lambda: 10.0\n",
      "    Train error: 19.187582\n",
      "\n",
      "*** Epoch: 29 Train error: 19.38365338643392  Test error: 19.02634  Time: 707.7643950999991 sec\n",
      "\n",
      "-- Epoch: 30  Batch: 1 --\n",
      "    Loss_i: 0.13897055 Loss_f: 0.14139146 rho -0.000283859 lambda: 0.01\n",
      "    Loss_i: 0.13897055 Loss_f: 0.14131333 rho -3.138556e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13897055 Loss_f: 0.13984407 rho -1.1871558e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.13897055 Loss_f: 0.13893367 rho 5.0194724e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13893367 Loss_f: 0.13904655 rho -1.5362158e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13893367 Loss_f: 0.13904655 rho -1.5362156e-08 lambda: 10.0\n",
      "    Train error: 18.562809\n",
      "-- Epoch: 30  Batch: 2 --\n",
      "    Loss_i: 0.14112592 Loss_f: 0.14253639 rho -0.00014235047 lambda: 0.01\n",
      "    Loss_i: 0.14112592 Loss_f: 0.14259915 rho -1.9019843e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14112592 Loss_f: 0.14169411 rho -7.5462606e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14112592 Loss_f: 0.14110373 rho 2.9552727e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14110373 Loss_f: 0.1411792 rho -1.00522755e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14110373 Loss_f: 0.1411792 rho -1.00522755e-08 lambda: 10.0\n",
      "    Train error: 19.130487\n",
      "-- Epoch: 30  Batch: 3 --\n",
      "    Loss_i: 0.14267708 Loss_f: 0.14495733 rho -0.00020196302 lambda: 0.01\n",
      "    Loss_i: 0.14267708 Loss_f: 0.14496116 rho -2.995719e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14267708 Loss_f: 0.14355387 rho -1.2080443e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14267708 Loss_f: 0.14265595 rho 2.9260752e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14265595 Loss_f: 0.14277223 rho -1.6099424e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14265595 Loss_f: 0.14277223 rho -1.6099422e-08 lambda: 10.0\n",
      "    Train error: 19.331745\n",
      "-- Epoch: 30  Batch: 4 --\n",
      "    Loss_i: 0.1386566 Loss_f: 0.13926744 rho -6.0185776e-05 lambda: 0.01\n",
      "    Loss_i: 0.1386566 Loss_f: 0.13943292 rho -9.375625e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1386566 Loss_f: 0.13896377 rho -3.795396e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1386566 Loss_f: 0.1386441 rho 1.5483204e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1386441 Loss_f: 0.13868628 rho -5.2243405e-09 lambda: 9.999999\n",
      "    Loss_i: 0.1386441 Loss_f: 0.13868628 rho -5.22434e-09 lambda: 10.0\n",
      "    Train error: 18.752304\n",
      "-- Epoch: 30  Batch: 5 --\n",
      "    Loss_i: 0.14160326 Loss_f: 0.14142103 rho 1.8151155e-05 lambda: 0.01\n",
      "    Loss_i: 0.14142103 Loss_f: 0.1416366 rho -2.1209002e-05 lambda: 0.01\n",
      "    Loss_i: 0.14142103 Loss_f: 0.14179319 rho -4.156164e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14142103 Loss_f: 0.14165637 rho -2.66414e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14142103 Loss_f: 0.1414664 rho -5.1419984e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14142103 Loss_f: 0.1414664 rho -5.141998e-09 lambda: 10.0\n",
      "    Train error: 19.261173\n",
      "-- Epoch: 30  Batch: 6 --\n",
      "    Loss_i: 0.13633062 Loss_f: 0.14734603 rho -0.0012586282 lambda: 0.01\n",
      "    Loss_i: 0.13633062 Loss_f: 0.14631478 rho -0.00013087387 lambda: 0.099999994\n",
      "    Loss_i: 0.13633062 Loss_f: 0.14021967 rho -5.173996e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.13633062 Loss_f: 0.13623932 rho 1.21646435e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13623932 Loss_f: 0.13676947 rho -7.063142e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13623932 Loss_f: 0.13676947 rho -7.063141e-08 lambda: 10.0\n",
      "    Train error: 18.215149\n",
      "-- Epoch: 30  Batch: 7 --\n",
      "    Loss_i: 0.1444421 Loss_f: 0.1493229 rho -0.0006173012 lambda: 0.01\n",
      "    Loss_i: 0.1444421 Loss_f: 0.14903769 rho -6.693433e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1444421 Loss_f: 0.14622797 rho -2.6411458e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1444421 Loss_f: 0.14439887 rho 6.4029266e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14439887 Loss_f: 0.14464322 rho -3.6183046e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14439887 Loss_f: 0.14464322 rho -3.6183042e-08 lambda: 10.0\n",
      "    Train error: 19.724337\n",
      "-- Epoch: 30  Batch: 8 --\n",
      "    Loss_i: 0.1447375 Loss_f: 0.14663933 rho -0.00020399215 lambda: 0.01\n",
      "    Loss_i: 0.1447375 Loss_f: 0.14672682 rho -3.0746916e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.1447375 Loss_f: 0.14552467 rho -1.2727731e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.1447375 Loss_f: 0.14471112 rho 4.2843484e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14471112 Loss_f: 0.14481911 rho -1.7541641e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14471112 Loss_f: 0.14481911 rho -1.7541641e-08 lambda: 10.0\n",
      "    Train error: 20.195986\n",
      "-- Epoch: 30  Batch: 9 --\n",
      "    Loss_i: 0.14188685 Loss_f: 0.14274855 rho -9.643298e-05 lambda: 0.01\n",
      "    Loss_i: 0.14188685 Loss_f: 0.14286996 rho -1.471344e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14188685 Loss_f: 0.14225577 rho -5.71409e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14188685 Loss_f: 0.14186734 rho 3.031725e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14186734 Loss_f: 0.14191331 rho -7.144207e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14186734 Loss_f: 0.14191331 rho -7.1442057e-09 lambda: 10.0\n",
      "    Train error: 19.46131\n",
      "-- Epoch: 30  Batch: 10 --\n",
      "    Loss_i: 0.14106704 Loss_f: 0.1409941 rho 7.651646e-06 lambda: 0.01\n",
      "    Loss_i: 0.1409941 Loss_f: 0.14163944 rho -6.8071015e-05 lambda: 0.01\n",
      "    Loss_i: 0.1409941 Loss_f: 0.14176774 rho -9.280839e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1409941 Loss_f: 0.14146134 rho -5.6832164e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1409941 Loss_f: 0.14108247 rho -1.0762997e-08 lambda: 9.999999\n",
      "    Loss_i: 0.1409941 Loss_f: 0.14108247 rho -1.0762995e-08 lambda: 10.0\n",
      "    Train error: 19.320175\n",
      "-- Epoch: 30  Batch: 11 --\n",
      "    Loss_i: 0.14084913 Loss_f: 0.1601089 rho -0.0018402836 lambda: 0.01\n",
      "    Loss_i: 0.14084913 Loss_f: 0.1579126 rho -0.00022468274 lambda: 0.099999994\n",
      "    Loss_i: 0.14084913 Loss_f: 0.14744475 rho -9.026009e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14084913 Loss_f: 0.14071341 rho 1.8646302e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14071341 Loss_f: 0.1416174 rho -1.241667e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14071341 Loss_f: 0.14161739 rho -1.2416466e-07 lambda: 10.0\n",
      "    Train error: 19.084988\n",
      "-- Epoch: 30  Batch: 12 --\n",
      "    Loss_i: 0.14529492 Loss_f: 0.1532039 rho -0.00081525027 lambda: 0.01\n",
      "    Loss_i: 0.14529492 Loss_f: 0.15248023 rho -0.000111262954 lambda: 0.099999994\n",
      "    Loss_i: 0.14529492 Loss_f: 0.148079 rho -4.5390375e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14529492 Loss_f: 0.14523329 rho 1.0101513e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14523329 Loss_f: 0.14560986 rho -6.17187e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14523329 Loss_f: 0.14560986 rho -6.171869e-08 lambda: 10.0\n",
      "    Train error: 20.093449\n",
      "-- Epoch: 30  Batch: 13 --\n",
      "    Loss_i: 0.13733938 Loss_f: 0.140761 rho -0.00034685087 lambda: 0.01\n",
      "    Loss_i: 0.13733938 Loss_f: 0.14057511 rho -4.1108477e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.13733938 Loss_f: 0.13858783 rho -1.6273165e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.13733938 Loss_f: 0.13730907 rho 3.9609773e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13730907 Loss_f: 0.13747823 rho -2.2105555e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13730907 Loss_f: 0.13747823 rho -2.2105553e-08 lambda: 10.0\n",
      "    Train error: 18.567715\n",
      "-- Epoch: 30  Batch: 14 --\n",
      "    Loss_i: 0.14163414 Loss_f: 0.14185245 rho -2.8284612e-05 lambda: 0.01\n",
      "    Loss_i: 0.14163414 Loss_f: 0.14209464 rho -7.269755e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14163414 Loss_f: 0.141821 rho -3.0157594e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14163414 Loss_f: 0.14162356 rho 1.7113101e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14162356 Loss_f: 0.14164889 rho -4.0974357e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14162356 Loss_f: 0.14164889 rho -4.097435e-09 lambda: 10.0\n",
      "    Train error: 19.372658\n",
      "-- Epoch: 30  Batch: 15 --\n",
      "    Loss_i: 0.13986607 Loss_f: 0.1398711 rho -5.2146555e-07 lambda: 0.01\n",
      "    Loss_i: 0.13986607 Loss_f: 0.14008822 rho -2.5787465e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.13986607 Loss_f: 0.13996024 rho -1.1066285e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13986607 Loss_f: 0.13986146 rho 5.417219e-10 lambda: 9.999999\n",
      "    Loss_i: 0.13986146 Loss_f: 0.13987346 rho -1.4112462e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13986146 Loss_f: 0.13987346 rho -1.4112461e-09 lambda: 10.0\n",
      "    Train error: 18.753765\n",
      "-- Epoch: 30  Batch: 16 --\n",
      "    Loss_i: 0.14198188 Loss_f: 0.14205253 rho -8.207129e-06 lambda: 0.01\n",
      "    Loss_i: 0.14198188 Loss_f: 0.14230068 rho -4.2744928e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14198188 Loss_f: 0.14211749 rho -1.8466388e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14198188 Loss_f: 0.14197555 rho 8.637935e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14197555 Loss_f: 0.14199376 rho -2.4835507e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14197555 Loss_f: 0.14199376 rho -2.4835505e-09 lambda: 10.0\n",
      "    Train error: 19.528719\n",
      "-- Epoch: 30  Batch: 17 --\n",
      "    Loss_i: 0.13981752 Loss_f: 0.13985762 rho -4.062551e-06 lambda: 0.01\n",
      "    Loss_i: 0.13981752 Loss_f: 0.1400732 rho -2.963014e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.13981752 Loss_f: 0.13992544 rho -1.2688767e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13981752 Loss_f: 0.13981378 rho 4.4042212e-10 lambda: 9.999999\n",
      "    Loss_i: 0.13981378 Loss_f: 0.13982846 rho -1.728286e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13981378 Loss_f: 0.13982846 rho -1.7282855e-09 lambda: 10.0\n",
      "    Train error: 18.745945\n",
      "-- Epoch: 30  Batch: 18 --\n",
      "    Loss_i: 0.14212608 Loss_f: 0.14214873 rho -2.4605595e-06 lambda: 0.01\n",
      "    Loss_i: 0.14212608 Loss_f: 0.14236563 rho -2.9584405e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14212608 Loss_f: 0.1422278 rho -1.2736044e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14212608 Loss_f: 0.14212057 rho 6.9131e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14212057 Loss_f: 0.1421341 rho -1.6965627e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14212057 Loss_f: 0.1421341 rho -1.6965627e-09 lambda: 10.0\n",
      "    Train error: 19.331898\n",
      "-- Epoch: 30  Batch: 19 --\n",
      "    Loss_i: 0.14355664 Loss_f: 0.14385782 rho -3.438349e-05 lambda: 0.01\n",
      "    Loss_i: 0.14355664 Loss_f: 0.1440731 rho -7.126903e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14355664 Loss_f: 0.1437604 rho -2.8717267e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14355664 Loss_f: 0.14354445 rho 1.7215822e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14354445 Loss_f: 0.14357117 rho -3.7733763e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14354445 Loss_f: 0.14357117 rho -3.7733754e-09 lambda: 10.0\n",
      "    Train error: 19.630045\n",
      "-- Epoch: 30  Batch: 20 --\n",
      "    Loss_i: 0.1403092 Loss_f: 0.14078382 rho -5.0772265e-05 lambda: 0.01\n",
      "    Loss_i: 0.1403092 Loss_f: 0.14095339 rho -7.959144e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.1403092 Loss_f: 0.14056422 rho -3.2004067e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.1403092 Loss_f: 0.14030126 rho 9.983088e-10 lambda: 9.999999\n",
      "    Loss_i: 0.14030126 Loss_f: 0.14033537 rho -4.2874433e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14030126 Loss_f: 0.14033537 rho -4.2874433e-09 lambda: 10.0\n",
      "    Train error: 19.060976\n",
      "-- Epoch: 30  Batch: 21 --\n",
      "    Loss_i: 0.14166848 Loss_f: 0.14139952 rho 2.637662e-05 lambda: 0.01\n",
      "    Loss_i: 0.14139952 Loss_f: 0.14135477 rho 4.3588e-06 lambda: 0.01\n",
      "    Loss_i: 0.14135477 Loss_f: 0.1415383 rho -1.7756278e-05 lambda: 0.01\n",
      "    Loss_i: 0.14135477 Loss_f: 0.1416427 rho -3.2016583e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14135477 Loss_f: 0.14153294 rho -2.0112118e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14135477 Loss_f: 0.1413911 rho -4.107035e-09 lambda: 9.999999\n",
      "    Train error: 19.231304\n",
      "-- Epoch: 30  Batch: 22 --\n",
      "    Loss_i: 0.139581 Loss_f: 0.14717618 rho -0.0008575329 lambda: 0.01\n",
      "    Loss_i: 0.139581 Loss_f: 0.14650811 rho -0.00010247663 lambda: 0.099999994\n",
      "    Loss_i: 0.139581 Loss_f: 0.14210416 rho -3.852182e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.139581 Loss_f: 0.13942121 rho 2.4473149e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13942121 Loss_f: 0.13972326 rho -4.6253735e-08 lambda: 9.999999\n",
      "    Loss_i: 0.13942121 Loss_f: 0.13972326 rho -4.6253728e-08 lambda: 10.0\n",
      "    Train error: 18.87316\n",
      "-- Epoch: 30  Batch: 23 --\n",
      "    Loss_i: 0.14443615 Loss_f: 0.1457977 rho -0.00013037803 lambda: 0.01\n",
      "    Loss_i: 0.14443615 Loss_f: 0.14588977 rho -2.0597708e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14443615 Loss_f: 0.14496551 rho -7.879059e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14443615 Loss_f: 0.14440887 rho 4.0815284e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14440887 Loss_f: 0.1444712 rho -9.324303e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14440887 Loss_f: 0.1444712 rho -9.324301e-09 lambda: 10.0\n",
      "    Train error: 19.736294\n",
      "-- Epoch: 30  Batch: 24 --\n",
      "    Loss_i: 0.1438921 Loss_f: 0.14387064 rho 2.2511456e-06 lambda: 0.01\n",
      "    Loss_i: 0.14387064 Loss_f: 0.1449871 rho -0.000117444964 lambda: 0.01\n",
      "    Loss_i: 0.14387064 Loss_f: 0.1450507 rho -1.5403823e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14387064 Loss_f: 0.14450924 rho -8.541679e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14387064 Loss_f: 0.14398132 rho -1.48414685e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14387064 Loss_f: 0.14398132 rho -1.4841468e-08 lambda: 10.0\n",
      "    Train error: 19.852922\n",
      "-- Epoch: 30  Batch: 25 --\n",
      "    Loss_i: 0.14494455 Loss_f: 0.16367395 rho -0.0020454058 lambda: 0.01\n",
      "    Loss_i: 0.14494455 Loss_f: 0.16142462 rho -0.0002720859 lambda: 0.099999994\n",
      "    Loss_i: 0.14494455 Loss_f: 0.1511634 rho -1.0821135e-05 lambda: 0.99999994\n",
      "    Loss_i: 0.14494455 Loss_f: 0.14475285 rho 3.3538335e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14475285 Loss_f: 0.14557557 rho -1.4386441e-07 lambda: 9.999999\n",
      "    Loss_i: 0.14475285 Loss_f: 0.14557557 rho -1.4386438e-07 lambda: 10.0\n",
      "    Train error: 19.951582\n",
      "-- Epoch: 30  Batch: 26 --\n",
      "    Loss_i: 0.14081489 Loss_f: 0.14631644 rho -0.0006074921 lambda: 0.01\n",
      "    Loss_i: 0.14081489 Loss_f: 0.14587127 rho -7.589133e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14081489 Loss_f: 0.14270124 rho -2.9367322e-06 lambda: 0.99999994\n",
      "    Loss_i: 0.14081489 Loss_f: 0.14072467 rho 1.4096949e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14072467 Loss_f: 0.14096853 rho -3.8104275e-08 lambda: 9.999999\n",
      "    Loss_i: 0.14072467 Loss_f: 0.14096853 rho -3.8104268e-08 lambda: 10.0\n",
      "    Train error: 19.161655\n",
      "-- Epoch: 30  Batch: 27 --\n",
      "    Loss_i: 0.14024259 Loss_f: 0.14160801 rho -0.00015234214 lambda: 0.01\n",
      "    Loss_i: 0.14024259 Loss_f: 0.14170475 rho -2.0312913e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14024259 Loss_f: 0.14079395 rho -7.8521776e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14024259 Loss_f: 0.14021046 rho 4.58689e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14021046 Loss_f: 0.14028037 rho -9.980113e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14021046 Loss_f: 0.14028037 rho -9.980111e-09 lambda: 10.0\n",
      "    Train error: 18.987661\n",
      "-- Epoch: 30  Batch: 28 --\n",
      "    Loss_i: 0.13897195 Loss_f: 0.13913223 rho -1.6122667e-05 lambda: 0.01\n",
      "    Loss_i: 0.13897195 Loss_f: 0.1393684 rho -4.8948327e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.13897195 Loss_f: 0.1391258 rho -1.9436274e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.13897195 Loss_f: 0.13895996 rho 1.5190516e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13895996 Loss_f: 0.1389789 rho -2.3983733e-09 lambda: 9.999999\n",
      "    Loss_i: 0.13895996 Loss_f: 0.1389789 rho -2.3983733e-09 lambda: 10.0\n",
      "    Train error: 18.704674\n",
      "-- Epoch: 30  Batch: 29 --\n",
      "    Loss_i: 0.14106932 Loss_f: 0.14119166 rho -1.3912064e-05 lambda: 0.01\n",
      "    Loss_i: 0.14106932 Loss_f: 0.14140981 rho -4.397118e-06 lambda: 0.099999994\n",
      "    Loss_i: 0.14106932 Loss_f: 0.14120555 rho -1.783419e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14106932 Loss_f: 0.14105988 rho 1.2385083e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14105988 Loss_f: 0.14107762 rho -2.326524e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14105988 Loss_f: 0.14107762 rho -2.326524e-09 lambda: 10.0\n",
      "    Train error: 19.14805\n",
      "-- Epoch: 30  Batch: 30 --\n",
      "    Loss_i: 0.14178677 Loss_f: 0.14228657 rho -5.4432843e-05 lambda: 0.01\n",
      "    Loss_i: 0.14178677 Loss_f: 0.1425621 rho -1.1249489e-05 lambda: 0.099999994\n",
      "    Loss_i: 0.14178677 Loss_f: 0.14210944 rho -4.8425454e-07 lambda: 0.99999994\n",
      "    Loss_i: 0.14178677 Loss_f: 0.14177321 rho 2.0420763e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14177321 Loss_f: 0.1418179 rho -6.7300316e-09 lambda: 9.999999\n",
      "    Loss_i: 0.14177321 Loss_f: 0.1418179 rho -6.7300303e-09 lambda: 10.0\n",
      "    Train error: 19.051794\n",
      "\n",
      "*** Epoch: 30 Train error: 19.227490933736167  Test error: 18.871044  Time: 708.223589600002 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history, opt_name = MNIST_AE_test(use_SF = True, start_from_previous_run = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error versus training step. For reference, the previous best training error obtained by a first order method was 1.0 \\([Sutskever, _et al_., 2013](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\\), and by the SF method was 0.57 \\([Dauphin, _et al_., 2014](https://arxiv.org/abs/1406.2572)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Steps')\n",
    "#plt.xscale('log')\n",
    "plt.title(opt_name + ' Optimizer')\n",
    "plt.legend()\n",
    "plt.savefig('1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
